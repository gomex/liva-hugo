[{"categories":["portugues","pipeline","iac"],"contents":"Deploy de Infraestrutura como código Introdução Explicamos aqui o que é um deploy, mas nesse artigo falaremos sobre um tipo especial de deploy. Esse que lida com a infraestrutura.\nInfraestrutura como código (IaC) é um grande domínio de conhecimento. Nesse artigo falaremos especificamente de infraestrutura de servidores, onde é criado e/ou mantido uma plataforma que receberá código posteriormente.\nExistem algumas formas de manter uma infraestrutura mantida de forma automatizada. Quando se fala sobre \u0026ldquo;deploy de infraestrutura como código (Deploy de IaC)\u0026rdquo; estamos falando de criar e/ou fazer modificação em uma infraestrutura com base em um código infra as code (IaC) de um repositório.\nPara facilitar o entendimento, ignore a existência de container (docker, podman ou afins), pois isso seria uma outra camada, que explicaremos em uma outra oportunidade.\nExemplo Imagine a necessidade de criar e manter um servidor web para servir um código PHP.\nNesse servidor precisa ser instalado alguns softwares principais:\n FPM: que é responsável pela tradução do código PHP. NGINX: que é responsável por fazer o proxy da requisição do usuário para o FPM.  Esses softwares precisam ser devidamente instalados e configurados de acordo com definições do seu uso.\nQuando se fala em fazer deploy de um servidor web para PHP, estamos falando da existência de um código IaC que será responsável por todo processo de instalar e configurar tudo que precisa para que exista a infraestrutura pronta para receber o código PHP e servir o site que você deseja.\nDeploy de infra é sobre infra Muitas pessoas têm dificuldade para entender deploy de infraestrutura, pois há uma confusão sobre deploy do produto em si e de sua infra.\nEm todo momento que falarmos sobre deploy neste capítulo será apenas sobre infraestrutura. Que a depender da situação, pode ser chamado também de plataforma.\nEm alguns casos o código pode ser \u0026ldquo;deployado\u0026rdquo; ao mesmo tempo, mas isso não é uma regra.\nNo exemplo citado anteriormente de um serviço PHP, não estamos falando do código PHP, apenas os softwares que deixaremos pronto para receber o deploy do código posteriormente.\nTipos de deploy de IaC Quando se fala de deploy de plataformas, especificamente aquelas que envolvem servidores, temos dois caminhos possíveis:\n Gerência de configuração Infraestrutura imutável  Gerência de configuração Deploy de IaC por gerência de configuração é quando você usa uma ferramenta que conecta em um servidor existente e faz intervenções nele. Nesse modelo o mesmo servidor sofre alterações com o passar do tempo.\nSempre que for preciso modificar o comportamento da plataforma, você precisará rodar a ferramenta de gerência de configuração escolhida no mesmo servidor, e ela será responsável por modificar o servidor com base na mudança que você informou no código. Seja por mudança do código de IaC ou suas variáveis.\nEssa comunicação entre a ferramenta de gerência de configuração e o servidor normalmente acontece via SSH (Para servidores GNU/Linux) e winrm (Para servidores Microsoft Windows). Esse software de IaC conecta na máquina e faz intervenções baseado no que está no código e suas variáveis.\nÉ importante salientar que as ferramentas de gerência de configuração farão de tudo para garantir que o que foi definido no código, pois estes softwares garantem mudanças de estado atual para o estado desejado definido como IaC, desde que este processo não falhe por algum motivo especifico.\nExemplo:\nSe o IaC define que um determinado serviço deveria estar em execução, como podemos ver no código abaixo:\n- name: Iniciar o nginx ansible.builtin.service: name: nginx state: started No código acima temos as seguintes instruções detalhadas:\n- name: Iniciar o nginx  Descreve o nome da task em questão.\n ansible.builtin.service:  Descreve o nome do módulo ansible, que nesse caso é o service que é responsável por gerenciar serviços.\n name: nginx state: started  Descreve o nome do serviço que deseja gerenciar e qual estado esperado dele, ou seja, caso o serviço esteja desligado, ele vai iniciar. Caso esteja iniciado, não faz nada.\n  Nesse exemplo, o código garante que o serviço iniciará caso esteja parado, mas se o serviço cair logo após o ansible inicia-lo, o ansible não saberá disso, a menos que exista outra task que faça essa checagem posteriormente.\n Infraestrutura imutável Antes de falar o que é Infraestrutura imutável, vamos explicar o que é Infraestrutura mutável.\nA infraestrutura mutável é exatamente o que é feito na gerência de configuração. Uma máquina é criada com uma determinada configuração e a mesma máquina sofre alterações para que um novo estado do serviço seja alcançado. No exemplo demonstrado anteriormente usando o ansible, ele se conecta a um servidor existente. Tudo é feito na mesma instância, a não ser que alguém solicite a recriação dessa máquina.\nExemplo\nAo instalar o serviço nginx pela primeira vez, é escolhida a máquina que tem o nome de bakunin e o ansible conecta em bakunin por ssh e faz a instalação do nginx.\nSe for necessário modificar a versão do nginx ou instalar um outro software, o ansible conectará em bakunin via ssh novamente e aplicará o que de novo foi adicionado no código ansible.\nInfraestrutura mutável é aquela que pode mudar dentro da mesma instância. Ela \u0026ldquo;nasce\u0026rdquo; em um estado de configuração e um processo externo pode fazer esse estado mudar.\nE a Infraestrutura imutável? Infraestrutura imutável é aquela que não muda, certo? Sim, mas pra explicar isso melhor vamos ampliar a visão sobre essa questão.\nQuando falamos sobre infra que muda ou não, falamos do servidor em específico, no exemplo apresentado anteriormente, uma vez que o servidor bakunin fez boot pela primeira vez ele não seria modificado até o dia que fosse desligado.\nO serviço que bakunin hospeda poderá continuar em outra máquina, mas a instância bakunin não mudará no modelo de Infraestrutura imutável.\nNão há nenhuma conexão ssh, ou outro método, para instalar ou configurar nada. A instância \u0026ldquo;nasce\u0026rdquo; com tudo que precisa para hospedar o serviço a que foi designada.\nNa infraestrutura imutável é necessário criar uma imagem, uma espécie de fotografia da instância em um determinado momento, para que quando necessário criar a instância, tudo seja iniciado sem precisar de gerência de configuração após ligar a máquina.\n Fotografia ou Snapshot, é o termo utilizado para referenciar um recurso que não pode ser modificado, igualmente a fotografia, podemos escrever(desenhar) por cima mas seu conteúdo base é impossível de ser modificado por este processo.\n Exemplo\nUsando o mesmo exemplo de uma máquina com nginx, ao invés de iniciar uma instância genérica, sem nginx, e após seu boot conectar via ssh e completar sua configuração, é criada uma máquina temporária, onde a instalação e configuração do nginx é feita através de uma ferramenta de configuração, e o processo final resulta em uma imagem (snapshot da instância)\nA imagem será usada quando for necessário fazer o deploy da infraestrutura imutável, pois nesse caso iniciaremos a máquina com essa imagem. Dessa forma a instância inicia já com nginx e nenhuma configuração posterior será necessária. E toda vez que for necessário trocar algo na instância, ao invés de conectar na mesma, é criada uma máquina nova a partir de uma imagem nova. Você pode inclusive manter as duas instâncias imutáveis executando ao mesmo tempo e desligar a antiga apenas quando tiver certeza que a nova está funcionando como esperado.\nImportante notar que nem todos os serviços suportam uma infraestrutura imutável, mas falaremos disso melhor em outra oportunidade.\nAgradecimentos Obrigado a Somatório que, como sempre, revisou esse material antes dele sair.\nMeu agradecimento a Caio que revisou o texto e me ajudou a deixar ele melhor ainda.\nObrigado também a Saulo que leu o artigo e me deu a sua opinião sobre ele.\nEscrevi esse artigo ouvindo:\n Black Pumas Fresno Incubus Froid Ana Tijoux  ","permalink":"https://demo.gethugothemes.com/liva/examplesite/blog/deploy_de_iac/","tags":["portugues","pipeline as code","pipeline","devops","qa"],"title":"Deploy de infraestrutura como código (IaC)"},{"categories":["portugues","ansible","QA"],"contents":"Introdução O Ansible é uma ferramenta com muitas possibilidades e grande versatilidade, e justamente por isso pode ser um problema para quem usa suas funcionalidades sem seguir algumas boas práticas.\nEsse artigo tem como objetivo apresentar as formas que eu aconselho na utilização de variáveis do Ansible. Eu vou tentar pontuar, da forma mais clara possível, os motivos para cada sugestão apresentada, mas percebam que boa parte delas partem de uma premissa de organização, e talvez não seja a sua forma de organizar.\nComo assim usar variáveis no Ansible? Normalmente você utilizará variáveis em uma role Ansible por dois motivos:\n Mudar comportamento da role Evitar repetição de valores dentro da role  Mudar comportamento da role Se você não sabe o que é uma role no Ansible, vale muito a pena ler esse artigo.\nQuando se constrói uma role Ansible normalmente se espera que ela assegure um conjunto de comportamentos, que normalmente são agrupados para fazer algum sentido.\nVamos usar como exemplo uma role para controlar o nginx. Essa role seria responsável por garantir que o pacote do nginx esteja instalado e um determinado conjunto de domínios seja configurado. Esses são os comportamentos esperados dessa role.\nComo mudar o comportamento citado acima? Eu posso especificar qual versão do nginx será instalado, qual porta padrão será utilizada, quais plugins quero instalar no processo, localização da pasta de log e afins.\nA possibilidade de modificar o comportamento de uma role sempre estará limitado a quantidade de variáveis que essa role tem, pois caso contrário você precisa mudar o código da role para que ela se comporte de uma outra forma.\nImagine a execução de um playbook como esse:\n--- - hosts: all roles: - nginx vars: nginx_version: 1.19 Nessa role terá alguma task responsável por instalar o pacote e ele seria assim para uma plataforma que usa apt-get como instalador de pacotes:\n- name: Instalando pacote apt: name: nginx={{ nginx_version }} Com base nisso podemos concluir que o valor passado no nginx_version será usado no momento da instalação, isso quer dizer que se você esquecer de passar esse valor o Ansible quebrará.\nNesse momento você deve ser fazer uma pergunta muito importante ao criar variáveis para mudança de comportamento na role: \u0026ldquo;Essa variável precisa mesmo ser informada pelo usuário? Existe algum valor que eu possa sugerir sem que isso afete a lógica da role?\u0026rdquo;\nSe você pode passar um valor padrão, SEMPRE passe, pois uma role que precisa de um lista enorme de valores para funcionar não é algo muito funcional, né?\nCaso você possa passar um valor default, sempre faça isso na pasta default. Segue abaixo um exemplo:\n--- nginx_enable: true nginx_install: true nginx_start: true nginx_version: 1.18 Com esses valores dentro do main.yml da pasta default, indica que caso você não informe nginx_version o valor usado pela role para informar qual versão do nginx que será instalada é 1.18.\nEvite usar o filtro default Existe um filtro no Ansible para manipular valores de variáveis dentro da task.\nEsse filtro é usado dentro da definição de variável na task:\n- name: Instalando pacote ansible apt: name: nginx={{ nginx_version | default('1.18') }} Com essa definição, caso você não especifique a versão usando a variável nginx_version essa task instalará o nginx na versão 1.18.\nQual problema há nesse tipo de uso? Falta de padrão e desorganização da sua role. Imagine uma role com diversos arquivos definindo tasks e os valores padrão espalhados por todos os arquivos. Se você precisar modificar o valor padrão? Como saber quais arquivos devem ser modificados?\nSe você utilizar os arquivos da pasta default ficará mais claro, afinal esses arquivos tem um propósito único. O que não é o caso dos arquivos de tasks.\nTratando variáveis sem valores padrões Um exemplo de uma variável que precisa realmente ser informada pelo usuário: Uma role que é responsável por gerenciar entradas DNS em um serviço SaaS. Essa role pode garantir que determinadas entradas não estejam lá. Se eu quiser usar esse comportamento eu necessariamente precisarei informar qual a entrada deve ser removida. Existe alguma forma de sugerir um valor padrão na role nessa situação? Não. Sendo assim você não informará nenhum valor na pasta default.\nNesses casos usaremos o assert do Ansible, para validar se a variável existe antes de continuar o uso dessa role:\n- name: Checar se 'nome_variavel' está definida assert: that: - nome_variavel is defined msg: \u0026quot;'nome_variavel' precisa ser definida\u0026quot; Com o código acima executado antes de qualquer outra tarefa poderemos garantir que a role só será usada caso você informe a variável necessária. Você evita que a sua role seja executada pela metade e pare de funcionar antes de terminar todo processo. Deixando seu servidor com metade do que deveria ser assegurado na máquina.\nEvitar repetição de valores dentro da role Dentro de uma role podem existir valores que você pode precisar repetir, mas você quer criar uma role bem parametrizada e evitar que uma mudança no nome do produto não demande que você busque em todo código o nome para que ele seja trocado um a um.\n- name: Instalando pacote apt: name: {{ nome_produto }} ={{ versao_produto }} - name: Garantir que o serviço está executando ansible.builtin.service: name: {{ nome_produto }} state: started O valor de nome_produto deve ser armazenado na pasta vars dentro do arquivo main.yml:\n--- nome_produto: nginx Lembre-se que o valor de versao_produto deve ser armazenado dentro da pasta default, pois esse é o comportamento que se espera que seja modificado pelo usuário. O valor da pasta vars é apenas para variáveis que não devem ser sobrescritas pelo usuário da role.\nÉ importante entender a precedência das variáveis quando elas são sobrescritas.\nConclusão Variáveis são formas interessantes de tornar sua role Ansible mais configurável e assim garantir uma boa manutenção desse código.\nSempre imagine que sua role vai crescer o suficiente para que seja necessário o uso de boas práticas, por isso aconselho sempre que começar já faça do jeito ideal, afinal depois de pronta, a refatoração pode ser mais trabalhosa.\nFontes  https://docs.ansible.com/ansible/latest/user_guide/playbooks_variables.html https://docs.ansible.com/ansible/latest/user_guide/playbooks_best_practices.html  Agradecimentos Obrigado a somatorio que sempre revisa tudo que escrevo.\nObrigado também a lista de pessoas abaixo que também revisaram o texto:\n Kleber Cabral Prof. Dr. Vicente Marçal Francílio Araújo  ","permalink":"https://demo.gethugothemes.com/liva/examplesite/blog/variaveis_no_ansible/","tags":["portugues","devops","ansible","QA","variavel"],"title":"Usando variáveis no Ansible"},{"categories":["portugues","ansible","QA"],"contents":"Introdução A infra como código ainda é um assunto em desenvolvimento, e ao contrário da engenharia de software no desenvolvimento convencional de código, que está bastante estabelecida, na automação de infra as melhores práticas ainda estão em amplo debate e pouco adotadas pela maioria das equipes que fazem esse tipo de implementação.\nÉ muito comum em infra as code (IaC) não existir muita preocupação com a qualidade do código escrito, seja porque, normalmente, eles são feitos por pessoas com pouca experiência em desenvolvimento em geral, ou talvez por existir poucos materiais falando sobre isso.\nEnviar o código sem uma mínima validação para que o pipeline \u0026ldquo;tente\u0026rdquo; aplicar seu código de infra com pouco indicativo que ele será aplicado como esperado, ou executar esse código manualmente em um servidor real com o mesmo grau de incerteza do exemplo anterior é bastante corriqueiro. Mas o que leva a pessoa que trabalha com infra a cometer esse tipo de atitude? Será que existe outra possibilidade?\nNesse artigo mostraremos alguns caminhos do que pode ser feito para evitar esse tipo de situação, afinal não é uma boa prática essa atitude de \u0026ldquo;tentativa e erro\u0026rdquo;. Pois como estamos falando de infraestrutura, uma tentativa muito equivocada pode causar danos com grandes chances de lhe oferecer novos desafios por algumas horas ou até mesmo dias.\nO que seria QA para IaC? Quality Assurance (QA) é a disciplina que trata sobre prevenir erros ou falhas na entrega de software. É o estudo e prática do que deve ser feito para oferecer alguma garantia antes do código ser compartilhado com todo time, pois todo mundo erra, mas o mais importante é que esse erro seja visualizado o mais rápido possível, evitando assim que ele cause problemas para outros membros da sua equipe ou até mesmo para a pessoa que usará seu software após entregue em produção.\nQA pode e deve ser usado para códigos de infra, mas temos pouca literatura sugerindo esse tipo de prática, infelizmente.\nEsse texto terá um enfoque especial na ferramenta de gerência de configuração chamado Ansible, pois é um dos softwares de IaC mais usado na comunidade.\nSerá apresentado formas de como validar o seu código antes dele ser compartilhado com outras pessoas do seu time e como garantir que o pipeline identifique rapidamente o erro no código que você ou outra pessoa do seu time compartilhou sem perceber que tinha problemas.\nTestando seu código antes do commit Pensando no Ansible, o teste não se reduz ao comando ansible-playbook executar com sucesso. A execução correta do ansible-playbook é um dos passos necessários, mas não o único.\nA primeira checagem que seu código precisa passar é o que chamamos de lint.\nLint é uma checagem simples e estática do seu código, que é um binário que acessa todos seus arquivos IaC e procura por falhas na escrita do código, como uma indentação incorreta, um espaço em branco desnecessário, uma linha que não precisa ou um ponto e vírgula necessário. Ele busca erros simples e executa muito rápido. Esse é talvez o teste mais rápido que você poderá executar no seu código IaC.\nNa checagem estática o código não precisa ser implementado em nenhum servidor, pois ele só olha o conteúdo do código e não seu resultado.\nPara o Ansible, temos o ansible-lint que pode ser instalado com o comando abaixo:\npip3 install \u0026quot;ansible-lint[yamllint]\u0026quot; Obs: Eu aproveitei para instalar o yamllint, que usaremos para analisar o arquivo yaml que é o padrão usado no Ansible também.\nPara testar usando essas ferramentas é muito simples:\nansible-lint yamllint . Eu aconselho muito a criação dos arquivos .ansible-lint e .yamllint na raiz do seu repositório para configurar o uso correto do lint.\nTestes automatizados para Ansible Agora que você já sabe validar estaticamente o conteúdo do seu código Ansible, podemos passar para validação automatizada do comportamento do seu código, que é a escrita de um código que tem como objetivo validar se o produto final entregue pelo seu IaC de fato faz o que você espera que ele faça.\nUsando um exemplo mais prático, imagine que você tem uma role Ansible que tem como objetivo instalar e configurar um servidor web. Considere que, alguém precisou alterar o código para adicionar a feature de configurar certificados para oferecer uma conexão HTTPS. Porém, a alteração de código quebrou o arquivo de configuração e, seu servidor web, uma vez configurado com essa role, não se sustentará executando no servidor. Pois, o mesmo está com erro no arquivo de configuração.\nA execução do Ansible, nesse caso, não falhará, muito menos o lint apontará qualquer problema. Você só perceberá qualquer problema após aplicar o código Ansible em um servidor e então acessar o servidor manualmente tanto na porta HTTP como HTTPS.\nPara evitar situações como essas, deve ser usado testes automatizados Uma boa ferramenta para testar códigos de IaC é o Testinfra.\nPara o utilizar o Testinfra, primeiro você deve executar o seguinte comando para instalar:\npip install pytest-testinfra Após instalado, você deve criar um arquivo de teste.\nEsse arquivo deve ser python, e o nome desse arquivo deve começar com \u0026ldquo;test_\u0026rdquo;, ou seja, seu arquivo pode ser \u0026ldquo;test_http.py\u0026rdquo; por exemplo.\nPara o exemplo citado acima, podemos fazer o seguinte teste:\ndef test_http_is_listening(host): http = host.socket(\u0026quot;tcp://0.0.0.0:80\u0026quot;) assert http.is_listening No arquivo acima, estou fazendo a seguinte pergunta: \u0026ldquo;Servidor, você tem o endereço socket \u0026ldquo;tcp://0.0.0.0:80\u0026rdquo; escutando? Se a resposta for sim, isso indica que meu código não afetou esse comportamento, pois esse teste só será executado após a aplicação do Ansible.\nA valor de host é inserido pelo Testinfra e por padrão ele usará a máquina onde está rodando o código.\nComplicações ao utilizar o Testinfra Na forma como foi apresentado até aqui, o Testinfra tem uma série de obstáculos para seu uso, que tornam o processo de teste um pouco complexo.\nNo código apresentado anteriormente, o pacote pytest-testinfra e o arquivo test_http.py devem estar no servidor destino, o servidor que será configurado com o Ansible. Isso quer dizer que você deve, de alguma forma, fazer isso na máquina destino e ela, possivelmente, ficará com esse \u0026ldquo;lixo\u0026rdquo; depois de testada, a não ser que você retire após executar o teste.\nÉ possível utilizar o Testinfra de forma remota, usando o conector ssh ou Ansible, mas isso tornará o processo ainda mais complicado.\nIsso, sem falar, na própria necessidade de ter um servidor destino para testar seu código Ansible, pois imagina toda complicação de ter que destruir o servidor e reinstalar toda vez que seu código Ansible inviabilize o uso daquela máquina?\nQualquer atividade de QA deve ser automatizada ao máximo, ou terá grandes chances de não ser executada pelo time.\nMolecule Para resolver esses problemas, eu te apresento o Molecule, que é uma ferramenta de apoio para testes automatizados no Ansible.\nO Molecule visa facilitar a execução de uma rotina completa de QA, pois ela também pode aplicar o lint, realizar os testes, fazer verificação de idempotência e se o código consegue lidar com efeitos colaterais.\nPara instalar é muito simples:\npip install \u0026quot;molecule[ansible]\u0026quot; Para criar uma role já seguindo os padrões do Molecule é muito fácil também:\nmolecule init role sua-role-nova --driver-name docker Com o comando acima, será criada uma role com toda estrutura esperada pelo Ansible, mas também com uma pasta chamada molecule e dentro dela todo padrão que você pode utilizar para seus testes.\nCaso sua role já exista, poderá utilizar o seguinte comando:\ncd sua-role molecule init scenario default --role-name sua-role-nova --driver-name docker Como eu utilizo Ubuntu e Testinfra como verificador, meu arquivo molecule/default/molecule.yml ficaria da seguinte maneira:\n--- dependency: name: galaxy options: ignore-certs: True ignore-errors: True role-file: requirements.yml driver: name: docker platforms: - name: ubuntu-18.04 image: \u0026quot;geerlingguy/docker-${MOLECULE_DISTRO:-ubuntu1804}-ansible:latest\u0026quot; command: ${MOLECULE_DOCKER_COMMAND:-\u0026quot;\u0026quot;} published_ports: - 80:80/tcp volumes: - /sys/fs/cgroup:/sys/fs/cgroup:ro privileged: true pre_build_image: true provisioner: name: ansible playbooks: prepare: prepare.yml converge: converge.yml verifier: name: testinfra env: PYTHONWARNINGS: \u0026quot;ignore:.*U.*mode is deprecated:DeprecationWarning\u0026quot; Vamos explicar parte a parte.\n--- dependency: name: galaxy options: role-file: requirements.yml Essa parte é responsável por informar qual gerenciador de dependência será usado e, até o momento, não conheço algo melhor do que o galaxy. Nas opções, informamos que o arquivo requirements.yml conterá a lista das roles que precisa ser baixada para que esteja disponível para ser usada. Perceba que aqui não diz que ela será usada, apenas que fará o download das roles mencionadas.\ndriver: name: docker platforms: - name: ubuntu-18.04 image: \u0026quot;geerlingguy/docker-${MOLECULE_DISTRO:-ubuntu1804}-ansible:latest\u0026quot; command: ${MOLECULE_DOCKER_COMMAND:-\u0026quot;\u0026quot;} published_ports: - 80:80/tcp volumes: - /sys/fs/cgroup:/sys/fs/cgroup:ro privileged: true pre_build_image: true Nas seções informadas acima, definimos que usaremos docker para emular a máquina, e a plataforma usada nessa \u0026ldquo;máquina\u0026rdquo; será o Ubuntu 18.04. Usaremos uma imagem do geerlingguy nesse exemplo, pois ela tem tudo que precisamos. O command é padrão e raramente precisará ser modificado. O published_ports define qual porta esse container vai expor e você só precisa colocar caso você queria testar da sua máquina, pois todo teste que falaremos aqui será executado de dentro da \u0026ldquo;máquina\u0026rdquo; e não de onde é executado o molecule. Quanto aos valores de volumes, privileged e pre_build_image você também, possivelmente, não precisará mudar.\nprovisioner: name: ansible playbooks: converge: converge.yml Na seção acima, é informado que usaremos o Ansible como ferramenta de IaC, definimos também o playbook que será usado na etapa do converge do Molecule. Para explicação de cada etapa do Molecule, acesse essa parte da documentação. É importante salientar que o Molecule aceita apenas o Ansible como provisioner.\nverifier: name: testinfra Nessa última seção, configuramos que será usado o Testinfra como verificador automatizado de código.\nO seu teste deve ser colocado dentro da pasta molecule/default/tests, com seu nome seguindo o mesmo padrão apresentado anteriormente (começando com \u0026ldquo;test_\u0026quot;), porém o conteúdo do arquivo deve seguir a estrutura:\nimport os import testinfra.utils.ansible_runner testinfra_hosts = testinfra.utils.ansible_runner.AnsibleRunner( os.environ['MOLECULE_INVENTORY_FILE']).get_hosts('all') def test_http_is_listening(host): http = host.socket(\u0026quot;tcp://0.0.0.0:80\u0026quot;) assert http.is_listening A seção nova que aparece nesse arquivo é responsável por configurar o Testinfra para utilizar a \u0026ldquo;máquina\u0026rdquo; entregue pelo Molecule.\nApós tudo configurado, só nos resta executar o teste:\nmolecule test Esse comando será responsável por:\n dependency lint cleanup destroy syntax create prepare converge idempotence side_effect verify cleanup destroy  Os nomes de cada etapa são auto-explicativos, mas se tiver dúvidas, basta ler a documentação oficial.\nCaso você queria testar seu código à medida que o cria, você pode apenas executar os seguintes comandos:\nmolecule create molecule converge O create será responsável por criar o container que atuará como \u0026ldquo;máquina\u0026rdquo; e o converge aplicará o código Ansible nesse \u0026ldquo;servidor\u0026rdquo; temporário, que na verdade é um container docker.\nApós aplicar com sucesso seu código Ansible no converge, você deve executar o comando abaixo:\nmolecule verify Ele será responsável por executar o Testinfra nessa \u0026ldquo;máquina\u0026rdquo; temporária e assim validar o comportamento após execução do converge.\nMeu conselho, antes de fazer commit e push do seu código IaC é que você execute o molecule test. Ele executa todos os passos em um servidor recém-criado. Além disso, ele também executa o idempotence que é responsável por avaliar se alguma tarefa que você definiu no Ansible está idempotente. Em outras palavras, significa que a segunda aplicação do Ansible na mesma máquina, sem modificar nenhum código Ansible, deve ter um resultado sem modificação alguma do servidor.\nA idempotência é uma melhor prática que deve ser buscada sempre, pois não faz sentido algum, uma mesma task sendo aplicada no mesmo servidor duas vezes fazer modificações no destino. Isso indica que seu código tem problemas e pode estar atuando na máquina de forma equivocada, o que pode ocasionar em duplicação de arquivos ou até mesmo configuração com problemas em arquivos.\nAgradecimentos Obrigado a minha companheira que revisou esse texto.\nObrigado a somatorio que sempre revisa tudo que escrevo.\nObrigado também a lista de pessoas abaixo que também revisaram o texto:\n Fábio Costa Alynne Ferreira Francílio Araújo Willian César Prof. Dr. Vicente Marçal  ","permalink":"https://demo.gethugothemes.com/liva/examplesite/blog/qa_iac_ansible/","tags":["portugues","devops","ansible","QA","molecule"],"title":"Garantindo a qualidade para sua Infra como código"},{"categories":["portugues","mentoria"],"contents":"O que é IaC? IaC é a sigla para Infrastructure as Code (Infraestrutura como Código) que é o processo de manipular a infraestrutura através de código.\nNesse processo você deve criar um arquivo de definição, um código mesmo, e esse arquivo será lido por um software que executa as ações baseada no que foi escrito neste código.\nNo código IaC você declara o que deseja que seja aplicado na infraestrutura e o software IaC é responsável por executar todos os comandos necessários para que a infra saia do estado atual e seja modificada para ficar da forma como foi declarado no arquivo IaC.\nSim, Gomex, mas que papo é esse de mentoria em IaC? Eu tenho percebido que uma galera está interessada em aprender mais sobre Infrastructure as Code (IaC) e não tinha oportunidade de uma experiência mais prática, com apoio de outras pessoas.\nEssa experiência é aquela que a maioria dos empregos requer para contratar uma pessoa, mas poucas empresas oferecem oportunidade para quem não tem nenhuma. É uma conta que não bate, mas vamos deixar o debate sobre essa incoerência pra depois.\nA mentoria acontece dentro de projetos de software livre, pois entendo que essa colaboração com um projeto maior seja muito potente e tenha um grande poder de transformação para quem está colaborando. Mais ainda, essa proposta fortalece a comunidade técnica. A pessoa deixa de ser apenas consumidora e passa a também ser produtora, ajudando a roda da comunidade a continuar a girar.\nComo funciona? Eu decidi iniciar um projeto de mentoria que esteja conectado com um projeto de software livre, que possua demanda suficiente de IaC para que as pessoas da mentoria possam ter oportunidade de passar pelo fluxo completo de uma entrega desse tipo. Um outro ponto importante para a escolha do projeto de software livre é seu impacto social, ou seja, sempre que possível escolherei projetos que tenham esse viés.\nComo projeto de Software Livre atual escolhi o Dados Abertos de Feira, que tem como objetivo oferecer transparência e dados abertos do município de Feira de Santana (BA).\nEsse projeto tem bastante demanda de IaC, pois ele está saindo do Heroku para a Absam, que é um provedor local na cidade de Feira de Santana.\nDepois de escolher o projeto, passamos a entender a demanda em relação a IaC e, colaborando com as pessoas que estão na mentoria, entregaremos as demandas seguindo as melhores práticas do mercado.\nO foco do processo de mentoria é o aprendizado das pessoas que estão sendo mentoradas, porém a entrega do produto para o projeto de software é muito importante também. Ou seja, não temos como esperar muito para que cada tarefa seja entregue. Não há cobrança, afinal é trabalho voluntário de ambos os lados, mas não poderemos bloquear a tarefa por tempo indeterminado para uma pessoa em específico.\nO modelo principal da mentoria é a colaboração, seja com seu mentor (eu nesse caso), ou com as outras pessoas que participam da mentoria também, pois todos sempre tem algo a acrescentar e ajudar. Sempre lembrando que no ato de ajudar outra pessoa, sempre aprendemos também.\nA conversa acontece no chat do projeto de software livre em questão. No momento estamos no Discord do projeto Dados Abertos de Feira, colaborando com atividades de IaC para o app mariaquiteria.\nComo posso participar? Você tem interesse em desenvolver/aprender suas habilidades de IaC? Tem tempo para estudar (ler e praticar) sobre o assunto? Tem disposição para interagir com as pessoas nos chats e participar das reuniões?\nSe todas as respostas foram sim, basta entrar no servidor do discord do atual projeto: https://discord.gg/Vsua55ZxMp\nQuando já estiver no servidor discord do Mentoria de IaC, entra no canal #geral e se apresenta.\nO que eu preciso saber previamente para começar? Conhecimento básico em Linux, com uso de linha de comando, assim como conhecimento básico em rede de computadores.\nNão precisa saber ansible ou qualquer outra ferramenta de IaC, mas prepare-se para aprender pois usaremos elas no projeto. A ideia é que você mesmo faça as tarefas, mas sempre com apoio e revisão de outras pessoas.\nPara quem não sabe nada de ansible ou qualquer outra ferramenta de IaC, sempre é criado um grupo à parte pra aprender do zero mesmo, a partir de algum material como referência para aprender, ou seja, se você não sabe nada, pode chegar, mas se prepare pois a ideia é que isso se resolva logo.\nEu sou mulher, LGBT, negro ou tenho deficiência. Esse grupo é inclusivo pra mim? A resposta rápida é sim, mas isso é um processo em constante construção/desconstrução e para tal eu preciso muito da sua ajuda, para me dar feedbacks para que o espaço seja realmente inclusivo e não seja apenas um interesse escrito.\nO projeto atual possui um código de conduta, e não atuaremos com projetos que não tem um claro código de conduta.\nConclusão Esse é um processo em desenvolvimento, por conta disso precisamos de todo feedback e colaboração de todas as pessoas para torná-lo cada vez melhor.\nO objetivo principal dessa iniciativa é nos apoiarmos enquanto pessoas que trabalham com TI, pois no momento sinistro que vivemos no Brasil o que nos resta é apoio mútuo.\nAgradecimentos Obrigado a minha companheira que revisou esse texto.\nObrigado a somatorio que sempre revisa tudo que escrevo.\nObrigado também a lista de pessoas abaixo que também revisaram o texto:\n Ana Paula Gomes Nana da Silva  ","permalink":"https://demo.gethugothemes.com/liva/examplesite/blog/mentoria/","tags":["portugues","devops","ansible","mentoria"],"title":"Como funciona a Mentoria de IaC em projetos de Software Livre"},{"categories":["portugues","mentoria"],"contents":"Mentoria de IaC? Como funciona? Veja qual objetivo nesse artigo.\nO resumo é: Eu conheço um pouco de IaC e quero ajudar outras pessoas a conhecerem a IaC para que elas possam conseguir melhores empregos. Trabalhador ajudando trabalhador.\nEu preciso pagar alguma coisa? NÃO! Se alguém lhe oferecer esse tipo de serviço em nosso canais, me avise por favor.\nEu preciso já saber Ansible, Terraform ou CI/CD? Não, mas quem nunca usou nenhum dessas ferramentas terá um tempo maior até colaborar com envio de código sem ajuda de outras pessoas.\nQuem nunca usou terá dois caminhos:\n Entrar em um canal espécifico para estudar a ferramanta, nos usaremos sempre algum material para ajudar a você aprender ao menos o básico; Parear com pessoas que tem algum conhecimento na ferramenta, a pessoa lhe ajudará a entender o básico. Use esse tempo pra perguntar tudo que precisa pra entender cada passo. Não deixe que a pessoa apenas faça sem sua interação o tempo todo, pois a ideia não é essa, ok?  Quanto tempo eu preciso dedicar para participar? Isso cabe apenas a você. Não há cobrança de participação, mas quem dedica mais tempo acaba aproveitando mais o processo.\nSe você tiver problemas pra conseguir tempo, compartilhe comigo, quem sabe podemos pensar em alguma coisa pra ajudar ;) Não concordo com esse papo de meritocracia e sei que nem todos tem a mesma possibilidade. Vamos nos ajudar.\nE se eu começar a fazer alguma tarefa e não terminar? Sem problemas! Duas coisas podem acontecer aqui:\n Se você nos atualizar do que foi feito, outra pessoa pode continuar de onde você parou; Se você não atualizar, outra pessoa pode começar do zero, mas não tem problemas, pois o foco da mentoria é o processo e não necessariamente a entrega.  Precisa saber inglês? Não, mas quem sabe inglês acaba aproveitando melhor. Acredito que o processo pode ajudar você a melhorar seu conhecimento em inglês também.\n","permalink":"https://demo.gethugothemes.com/liva/examplesite/blog/mentoria-faq/","tags":["portugues","devops","ansible","mentoria"],"title":"Perguntas frequentes sobre a mentoria de infraestrutura como código"},{"categories":["portugues","devops","ansible"],"contents":"Contexto O ansible é por definição um gerenciador de configuração, que resumidamente é a ferramenta responsável por aplicar definições de infraestrutura como código nos ativos.\nExemplo:\nVocê quer instalar e configurar um servidor nginx, o gerenciador de configuração permite que você escreva um arquivo com tudo que precisa, e toda vez que você precisar instalar e configurar um novo nginx, você executa o software apontando para o arquivo de definição que você escreveu previamente, que o gerenciador de configuração se encarrega de instalar e configurar tudo exatamente da forma como foi definido anteriormente.\nO problema de arquivos de definição de gerenciadores de configuração é a organização e separação das responsabilidades, pois à medida que a necessidade de infraestrutura como código aumenta, é comum que tudo fique em um único arquivo, o que dificulta a gestão, ou muitas vezes é separado de forma equivocada.\nEsse artigo falará especificamente sobre ansible, mas com alguma adaptação, eu acho que ele pode ser usado para outro gerenciador de configuração também.\nIntrodução O ansible tem o conceito de playbook e roles. Ambos têm como objetivo organizar as definições que serão aplicadas no ativo. Para facilitar o entendimento, trataremos esse ativo como servidor, uma máquina, que receberá comandos para estar de acordo com nosso desejo. O ansible pode ser usado para configurar SaaS, PaaS e afins, mas nesse artigo trataremos especificamente de máquinas, para facilitar o entendimento.\nPlaybook Esse é o arquivo mais elementar do ansible, ao começar a usar ansible, normalmente, se aprende criando um playbook e algumas pessoas acabam usando apenas isso para o resto da vida, o que é bem ruim, mas trataremos disso depois.\nO playbook é um arquivo YAML que tem a definição inicial do ansible, toda execução do ansible deve partir de um playbook. Segue abaixo um exemplo comum:\n--- - name: update web servers hosts: webservers tasks: - name: ensure nginx is at the latest version yum: name: nginx state: latest No primeiro bloco temos:\n- name: update web servers Essa linha é o título desse playbook. Tem como objetivo dizer claramente qual objetivo desse playbook, o que ele irá fazer.\nEm seguida a especificação sobre quais hosts ou grupos de hosts podem usar esse playbook:\n hosts: webservers Isso quer dizer que se você executar esse playbook ele será aplicado apenas para os hosts que estiverem no grupo webservers. Segue um exemplo de inventário:\n[databases] mysql01.exemplo.com.br mysql02.exemplo.com.br [webservers] web01.exemplo.com.br web02.exemplo.com.br Com base no exemplo de inventário acima, o playbook em questão será executado apenas para o grupo webservers, mesmo que seja especificado os hosts do databases na execução do ansible, pois nesse caso ele dirá que não tem servidor disponível para executar esse playbook.\nNo próximo bloco de código é onde começa as tarefas do ansible:\ntasks: - name: ensure apache is at the latest version yum: name: nginx state: latest O bloco tasks é usado para iniciar as tarefas que serão executadas no servidor destino, no caso exemplo apresentado acima, ele usará o módulo yum para instalar o pacote httpd na máquina alvo.\nÉ muito comum usar o bloco tasks para colocar todo tipo de tarefa e assim o playbook vira um arquivo único que tem todas as tarefas que você precisa. Quando esse arquivo tem apenas 2 ou 3 tasks talvez não demonstre o problema desse uso, mas ao aumentar o número de tasks se percebe o problema de gerência de um arquivo único.\nPensando nisso foi criado o conceito role do ansible.\nRole Essa é a melhor forma para organizar as plays do seu ansible, onde cada pasta tem suas função específica para facilitar tanto no uso como na implementação do que se deseja aplicar com aquela role.\nNormalmente dentro de uma role temos as seguintes pastas:\n tasks/ files/ templates/ handlers/ meta/ defaults/ vars/  Quando uma pasta com um determinado nome, tem esse conjunto de subpastas dentro, com todos seus arquivos yaml dentro de cada uma delas, podemos dizer que temos uma role.\nAo utilizar uma role o ansible automaticamente procurará o arquivo main.yml dentro da pasta tasks para iniciar as tasks, uma vez que uma task tenha a cópia de um arquivo, usando o módulo copy por exemplo, essa task vai buscar o arquivo na pasta files. Segue abaixo um exemplo:\n- name: Copiar o arquivo tests.sh copy: src: tests.sh dest: /opt/tests/tests.sh Perceba que a pasta onde está o arquivo tests.sh não foi sequer mencionada, isso sendo uma role, ele vai buscar dentro da pasta files e não no local onde foi executado o binário do ansible, como é o comportamento padrão para execução de playbook puro.\nO mesmo acontece ao utilizar templates:\n- name: Copy o arquivo tests.sh template: src: tests.sh dest: /opt/tests/tests.sh Na pasta handlers será usada a configuração dos handlers que ao ser chamado em uma task, será executado conforme sua configuração. Segue exemplo:\n--- - name: Copy o arquivo tests.sh template: src: tests.sh dest: /opt/tests/tests.sh notify: restart test Dentro da pasta handlers teríamos algo assim:\n--- - name: restart test command: \u0026quot;docker restart test\u0026quot; Percebam que nem no main.yml do tasks ou do handlers será necessário começar com os blocos handlers: ou tasks:, pois o fato do main.yml está dentro da pasta correta, já indica o propósito e execução daquela informação. Isso quer dizer que se você errar e colocar uma linha que deveria ser do handlers e colocar ela dentro do main.yml da pasta tasks ele será executado como uma task comum e não como um handler.\nA pasta meta/ é onde os metadados dessa role são armazenados. Não há muito a dizer além do que temos na documentação oficial.\nA pasta defaults/ é onde estará o valor padrão de todas as variáveis que não seja realmente necessária ser setada por quem está consumindo a role. Normalmente são valores que funcionam sem qualquer dano para quem está executando pela primeira vez, ou seja, o valor default de uma task que possa apagar algo na máquina da pessoa que consome essa role jamais deve ser definido, por exemplo.\n--- integration_test: true No exemplo acima a variável integration_test, que será usada pelas tasks dessa role, será usada com valor true toda vez que não for passado um valor de outra forma.\nAtenção, para quem consome a role, a pasta default não deve ser editada para que um novo valor de variável seja aplicado.\nA pasta vars/ é usada para configurar variáveis que serão usadas para organização do que foi implementado na role e evitar repetição dentro do código, o conteúdo dessa pasta não deve ser alterado pelo usuário da role ao baixar a role para uso via playbook. Os valores das variáveis normalmente são aplicados no playbook, seja diretamente ou através de includes ou imports de arquivos de arquivos externos.\nVamos pensar juntos no processo de uso da role, você vai baixar a role de algum lugar, essa role segue um outro versionamento, você vai querer mesmo abrir a role, acessar a pasta vars dela só pra colocar um valor que você pode perfeitamente colocar em um arquivo que, esse sim, é rastreado pelo seu controle de versão?\nMais detalhes sobre a precedência no uso de variáveis nesse artigo\nOrganização das roles e playbooks Já foi dito que não deve usar o playbook para armazenar tudo que precisa para uma execução, como por exemplo colocar diversas tasks e handlers dentro do mesmo playbook.yml, que ao final do trabalho ficará enorme e insustentável do ponto de vista da organização e gestão de atualização.\nJá foi dito também que os valores das variáveis não devem ser aplicadas dentro da role, que apenas os valores defaults devem ser aplicados lá e qualquer alteração por parte de quem consome a role, deveria ser aplicada em outro local, pois a pasta vars foi também apontada como não ideal para o caso.\nQual a forma de se utilizar as roles e playbooks?\nSegue abaixo um ótimo exemplo para um playbook:\n--- - hosts: all roles: - dokku_bot.ansible_dokku - iac-role-basica vars: dokku_version: 0.22.3 sshcommand_version: 0.12.0 dokku_hostname: mariaquiteria.example.org Veja o propósito do playbook. Ele deve ser usado apenas como inicializador do processo, é onde deve iniciar o processo de configuração.\nNele deve estar qual limite de grupos devem ser usados para aplicar esse playbook em questão:\n - hosts: all Nesse exemplo indica que esse playbook pode ser aplicado para qualquer grupo do seu inventário.\nUma coisa muito importante a pensar ao criar playbooks é que não deve estar nesse playbook nada de execução de tarefas, ou seja, se existir o bloco tasks nesse playbook ele não deve fazer nenhuma ação complexa.\nPense que seu playbook é uma forma de dar vida a sua role, se você colocar tasks no seu playbook toda a ideia de roles perdem o seu benefício.\nPara usar roles é simples:\nroles: - dokku_bot.ansible_dokku - iac-role-basica Essas roles precisam estar na máquina que fará o deploy do código ansible, ou seja, você deve usar um arquivo chamado requirements.yml e dentro dele colocar as roles que deseja baixar:\n# Do galaxy - src: dokku_bot.ansible_dokku # Do repositório github que tem a role básica - src: git@github.com:DadosAbertosDeFeira/iac-role-basica.git scm: git version: \u0026quot;0.1\u0026quot; O fato dessas roles estarem configuradas para baixar dentro do requirements.yml não indica que o ansible-playbook baixará as roles automaticamente ao executar. Isso precisa ser feito antes com o comando:\nansible-galaxy install -r requirements.yml Dentro das roles será colocado todo código necessário para aplicar o estado que deseja na máquina de destino.\nA organização ideal, em minha humilde opinião, segue no desenho abaixo:\nTeremos um repositório inicial, de ponto de partida, onde nele será adicionado o playbook.yml, que será usado para iniciar o deploy e o requirements.yml que terá a configuração para baixar as roles que serão usadas no playbook.yml.\nAs roles produzidas e mantidas pelo seu time devem estar pelo menos em um repositório de controle de versão git, que pode ser usado para baixar a role a partir do requirements.yml, como já foi apresentado anteriormente.\nÉ importante salientar que ao usar o repositório git as pastas apresentadas neste artigo devem estar na raiz do repositório git. Não crie uma pasta chamada roles e dentro dele coloque outros arquivos e pastas. A forma ideal você pode encontrar nesse repositório por exemplo.\nSegue abaixo um comando que pode ser usado para criar uma role:\nansible-galaxy init nome_da_sua_role O resultado disso será uma pasta com o título nome_da_sua_role, essa será a pasta que você deve executar o git init para criar um repositório git e depois enviar para seu servidor git.\nAgradecimentos Obrigado a somatorio que sempre revisa tudo que escrevo.\nObrigado a Matheus Lao que me lembrou que a pasta vars/ pode ser usado dentro da role.\nObrigado também a lista de pessoas abaixo que também revisaram o texto:\n Kleber Cabral Fábio Costa Juan Maia Willian César  ","permalink":"https://demo.gethugothemes.com/liva/examplesite/blog/ansible-roles-playbook/","tags":["portugues","devops","ansible"],"title":"Como organizar as roles e playbooks do ansible"},{"categories":["portugues","pipeline"],"contents":"Contextualização Esse artigo segue a série sobre “Deploy em produção para desenvolvedores?”, que tem como objetivo apresentar as melhores práticas para entregar em produção os produtos.\nNesse artigo falaremos sobre o que é pipeline, porque e como normalmente se utiliza essa ferramenta tão importante para a entrega automatizada de produtos.\nIntrodução O pipeline usado para entregar software segue o mesmo conceito usado normalmente nas indústrias, que é uma esteira metálica que faz o produto \u0026ldquo;se mover\u0026rdquo; por dentro da fábrica, e os robôs, que estão parados, montam o produto à medida que ele passa.\nUsando o exemplo da fabricação de carros, primeiro a estrutura metálica do carro (chassi) é colocado no pipeline e ela é movimentada pela fábrica, o primeiro robô, que está parafusado no chão, é responsável por pintar o chassi completamente. Sendo assim a cada chassi colocado no pipeline o primeiro robô pintará ele automaticamente.\nQuando o chassi chega no primeiro robô, o pipeline para, pois o robô precisa de um tempo no processo de pintura, e somente após terminar o pipeline se movimenta novamente, para que o chassi seguinte seja movido para passar pela etapa de pintura e assim sucessivamente.\nO segundo robô nesse pipeline é responsável por colocar as rodas e nesse processo o pipeline também espera ele terminar, e somente quando ele acaba o pipeline se move novamente.\nNo pipeline de software é bem parecido, sendo que ao invés do chassi, nesse modelo temos o código como \u0026ldquo;objeto\u0026rdquo; a ser movido pela esteira.\nA grande diferença entre os modelos está na natureza do \u0026ldquo;objeto\u0026rdquo; usado para construir o produto. No caso do pipeline de carros, o chassi tenderá a sempre ser o mesmo, mas no caso do código é o exato oposto. O código tenderá a ser sempre diferente. Cada execução do pipeline, o \u0026ldquo;objeto\u0026rdquo; normalmente será diferente.\nComo inicia um pipeline? Na entrega de software o código vem do controle de versão, que é o local onde ficará armazenado todo código produzido pelo time de desenvolvimento.\nO repositório de código armazenado no controle de versão normalmente tem uma ligação com a ferramenta responsável pelo pipeline de software, sendo assim é correto dizer que, em casos como esses, quando o código é modificado essa ação automaticamente ativa a execução do pipeline, ou seja, uma pessoa adicionou uma linha nova dentro do código? O pipeline será iniciado automaticamente com base nesse fonte atualizado.\nEtapas do pipeline de software O código é colocado na \u0026ldquo;esteira\u0026rdquo; e ela caminha de forma parecida com o que foi usado no exemplo da montagem do carro, ou seja, o código chega no primeiro \u0026ldquo;robô\u0026rdquo; e ele executa uma função específica e repetitiva. Esse momento é normalmente chamado de \u0026ldquo;step\u0026rdquo;. A tradução para português seria \u0026ldquo;etapa\u0026rdquo;, mas usaremos o termo em inglês, pois além de introduzir e fixar um termo tão importante nesse idioma, é esse nome que é usado em boa parte das ferramentas de mercado.\nÉ correto dizer que a execução completa de um pipeline é a \u0026ldquo;movimentação\u0026rdquo; do código por múltiplos steps, ou seja, o código é depositado na esteira e transferido para o primeiro step que fará a primeira intervenção no código, que pode ser uma validação estática do código. Esse mesmo código, que foi validado na primeira step passa para uma nova, que pode ser a responsável por transformar esse código em um executável, e na posterior esse artefato é armazenado em repositório.\nPerceba que no exemplo demonstrado na imagem o mesmo código passou por três steps diferentes na mesma execução do pipeline.\nCada step do seu pipeline é um comando, que é executado em uma console. Essa execução normalmente é executada dentro da pasta que tem os arquivos atualizados que foram recém baixados do controle de versão.\nNo exemplo anterior, a primeira step seria um comando para avaliar estaticamente o código fonte que está na sua pasta local, a segunda seria o comando para fazer *build e o terceiro um comando para fazer upload do binário construído na step anterior para um repositório de artefatos.\nSeparação por \u0026ldquo;job\u0026rdquo; Como já sabemos que no pipeline existem várias steps, que são responsáveis por executar ações no código a medida que elas avançam na esteira de entrega de software, é importante salientar que existe um outro nível de abstração chamada de job. A tradução para português seria \u0026ldquo;trabalho\u0026rdquo;, mas usaremos o termo em inglês para facilitar seu uso no futuro, pois esse é o nome que muitas vezes é usado pelas ferramentas de mercado.\nO job é um conjunto de steps, onde essas steps normalmente são executadas sequencialmente por padrão, ou seja, a segunda step só será executada após terminar a primeira.\nSe seu pipeline tiver vários jobs geralmente eles serão executados em paralelo, ou seja, se você tiver um job que executa seu código em uma plataforma específica, e um outro job que executa em outra. Os dois jobs serão iniciados ao mesmo tempo e não haverá nenhuma hierarquia entre eles, a não ser que seja explicitamente descrita.\nObs: Algumas ferramentas de pipeline não tem esse conceito de jobs ou utilizam outro nome. Lembre-se que essa explicação tem como objetivo oferecer uma ideia dos fundamentos.\nOnde é executada essa \u0026ldquo;step\u0026rdquo;? Como cada step tem seu comando, é importante saber onde esse comando é executado, pois quando o pipeline é iniciado, o código é copiado para uma pasta local e os comandos são executados no mesmo lugar.\nNormalmente é usado um outro servidor para clonar o código e executar todos os comandos de cada step. Essa máquina é comumente chamada de agente.\nEsse agente é o sistema que será usado para executar os comandos. Ele conecta no servidor de pipeline para buscar as informações necessárias para executar os steps e jobs da forma que foi configurada.\nEsse agente pode ser de fato uma máquina, mas também pode ser um container. Esse modelo é inclusive a melhor forma de utilização de agentes, pois não há necessidade de manutenção de servidores, que consome um tempo de gerência absurdo do time responsável pela infraestrutura.\nImagina ter de instalar uma máquina, especificar todos os pacotes, bibliotecas e afins que é necessário para diferente pipeline que tem dentro de uma organização? E se o time de produto quiser usar uma linguagem nova? Abre um ticket pro time de infra e espera ele criar um agente novo? Não precisa. Usando o docker você mesmo pode criar sua imagem ou utilizar as milhares que existem prontas nos repositório públicos de imagens que temos hoje na internet.\nVale salientar que mesmo utilizando containers como agente, ainda é aconselhável ter uma outra máquina, pois é nela que será instalado o software responsável pela gerência dos containers e onde será iniciado os containers, ou seja, basicamente os jobs serão executadas nesse agente, mas não no sistema operacional padrão e sim naquela que foi iniciado dentro do container.\nSe você ainda não sabe como funciona containers, aconselhamos a leitura do livro Docker Para Desenvolvedores, mas acredito que o conhecimento de containers não vai afetar a sua capacidade de assimilar esse conceito sobre pipelines. Basta saber que o container é um processo rodando em um sistema operacional isolado, mas que todos os containers do mesmo host rodam na mesma máquina.\nQuebrando o pipeline Usualmente as ferramentas de pipeline só permitem que a segunda step seja executada se a primeira for finalizada com sucesso. Isso é um comportamento extremamente esperado, porque a ideia do pipeline é justamente garantir que as ações sejam executadas em sequência, pois elas em geral são configuradas de forma gradual, ou seja, as primeiras steps fazem as primeiras validações, e as construções e outras intervenções mais críticas e demoradas acontecem depois. Isso quer dizer que se uma validação inicial falhar, e essa validação por via de regra é mais rápida, poupa o tempo de esperar a falha da etapa de construção de artefato, que costumeiramente é mais demorada.\nÉ comum encontrar pessoas afirmando que o objetivo de um pipeline é \u0026ldquo;quebrar\u0026rdquo;, pois é nesse processo que se percebe se o código enviado para esteira de fato está preparado para ser entregue ou não.\nA automatização das validações e construções são parte central de um processo de entrega de software moderno.\nConclusão O pipeline é uma abstração, onde temos jobs e steps compondo as etapas para construção de um produto a ser entregue no final da esteira. Tendo isso em mente, podemos pensar que muito mais do que apenas a ferramenta, o pipeline serve também para criar um fluxo rápido de feedback, onde em caso de quebra podemos entender que aquele código precisa de cuidados até que o problema seja resolvido.\nAgradecimentos Obrigado a Somatório que, como sempre, revisou esse material antes dele sair. Obrigado também a Braier que também revisou esse artigo antes dele sair.\nEscrevi esse artigo ouvindo:\n Dark Moor Fresno Makalister Beethoven Outras músicas do meu Daily Mix do Spotify  ","permalink":"https://demo.gethugothemes.com/liva/examplesite/blog/o_que_e_pipeline/","tags":["portugues","pipeline as code","pipeline","devops","qa"],"title":"O que é pipeline"},{"categories":["portugues","pipeline"],"contents":"Contextualização Esse artigo segue na série sobre \u0026ldquo;Deploy em produção para desenvolvedores?\u0026quot;, que tem como objetivo apresentar as melhores práticas para entregar em produção os produtos.\nNesse artigo falaremos sobre o que é deploy e quais ambientes normalmente estão envolvidos nesse processo.\nO que é deploy? Muito se fala sobre deploy e a maioria das pessoas que já estão, há algum tempo, na área de Tecnologia da Informação (TI) , provavelmente já tenham algum entendimento sobre o que é isso, e, quem iniciou na área há pouco, possivelmente já captou \u0026ldquo;alguma coisa\u0026rdquo; pelo contexto.\nEssa é a oportunidade para se estabelecer aqui um entendimento sobre o que é deploy, como ele funciona e porque ele é tão importante para área de TI.\nPra começar, deploy é um verbo do idioma inglês, cuja tradução mais próxima talvez seria posicionar.\nQuando se fala em fazer deploy, imagine que isso significa uma forma de posicionar algo, ou seja, é basicamente pegar algo que está em uma posição/localização e colocar em outra.\nIsso quer dizer que quando alguém falar que vai \u0026ldquo;deployar\u0026rdquo; algo, é basicamente o jeitinho brasileiro de usar uma palavra em inglês e verbalizar ela em português, o que não é de todo ruim, uma vez que quem escuta entende perfeitamente, ou seja, a comunicação funciona e não há problema algum com isso. Chato mesmo é alguém que complica a comunicação para forçar palavras não usuais e assim tentar demonstrar uma certa superioridade linguística. Aconselha-se a leitura desse livro sobre preconceito linguístico.\nComo funciona o deploy de um produto de software O que aqui se chama de \u0026ldquo;produto de software\u0026rdquo; é qualquer conjunto de arquivos que tenha como objetivo entregar uma funcionalidade como produto final. Um exemplo seria um site, que pelo conjunto de arquivos html, css e javascript, entrega uma exibição de informações que é traduzida pelo seu navegador e assim você pode ter acesso a informações navegando na internet.\nO deploy é o ato de pegar esse conjunto de arquivos e levar até um determinado lugar. Esse lugar é normalmente um servidor, que hospedará esse software e exibirá para o usuário sempre que solicitado.\nDestinos possíveis de um deploy Normalmente um software passa por alguns destinos antes de chegar no seu habitat final, que é o ambiente de produção. O que se chama de produção, dessa forma, é basicamente o lugar oficial de onde os usuários finais deste produto poderão acessá-lo.\nAs boas práticas apontam que antes de chegar no ambiente de produção esse software passe por outros ambientes, que normalmente são os seguintes, e, muitas vezes seguem também nessa ordem:\n Desenvolvimento Teste/Staging Produção  Desenvolvimento É o local no qual a pessoa que desenvolve tem acesso direto, é onde se executa rapidamente o código, a fim de verificar se o que está sendo escrito atenderá as expectativas da funcionalidade que está sendo desenvolvida\nNormalmente essa é a máquina da pessoa que desenvolve o software, e o verbo \u0026ldquo;deployar\u0026rdquo; faz pouco sentido aqui, porque não há uma movimentação de código, uma vez que este será usado na mesma máquina onde foi produzido.\nEm alguns casos, a infraestrutura necessária para simular o ambiente de produção é tão complexa que é preciso um ambiente de desenvolvimento fora da máquina local, neste caso, o verbo \u0026ldquo;deployar\u0026rdquo; volta a ter seu sentido completo, pois o código será transferido para um outro ambiente, no caso, um de desenvolvimento remoto.\nTeste/Staging Não existe um nome para esse ambiente que possa ser considerado unânime, mas esse é o ambiente no qual se espera que o software esteja mais maduro, isso quer dizer que aqui o código já passou por alguma análise e está pronto para ser validado por outras pessoas.\nO que aqui é chamado de análise será mais detalhado nos capítulos posteriores, mas, por hora, basta saber que esse é o processo usado para avaliar se há algum problema no código, normalmente de forma manual, e feito por uma outra pessoa, que analisa seu código a fim de encontrar possíveis erros.\nEsse é, via de regra, o último local que o código \u0026ldquo;visitará\u0026rdquo; antes de ser conduzido para o ambiente que será usado pelos usuários reais do serviço.Ou seja, é aqui o local no qual costumeiramente acontecem os testes mais \u0026ldquo;pesados\u0026rdquo;.\nEsses testes muitas vezes usam dados mais próximos do que os que seriam usados no ambiente real, de forma que validações bem mais elaboradas podem acontecer. Habitualmente, times de software simulam o uso do sistema, de forma automatizada ou não, a fim de encontrar possíveis erros. Esses tipos de testes serão tratados posteriormente. Por agora é suficiente saber que é nesse ambiente que isso habitualmente acontece.\nÉ importante salientar que se você copiar os dados de produção para ajudar na validação dos ambientes de \u0026ldquo;teste\u0026rdquo; e/ou desenvolvimento, deverá lembrar de apagar dados pessoais das pessoas que utilizam seu sistema. Imagine se fosse você a pessoa que utiliza um sistema, sabendo que seus dados pessoais estão acessíveis para qualquer membro da equipe de desenvolvimento?\nProdução Aqui é oficial, todo produto agora pode ser utilizado pelos clientes. Em geral, esse ambiente demanda um cuidado maior, tanto de quem pode fazer o deploy, como na disposição da quantidade de recursos. Muitas vezes o ambiente de produção tem, ao menos, duas máquinas, que carregam o mesmo conteúdo. Isso permite criar uma situação chamada de alta-disponibilidade, que ocorre quando o serviço permite que uma máquina seja perdida por falhas não esperadas, sem que isso afete a sua disponibilidade.\nUsando o exemplo anterior do site, basicamente, seria a hipótese de se ter duas máquinas hospedando os mesmos arquivos do site, e, caso aconteça uma falha elétrica, ou qualquer outro problema em uma das máquinas, a segunda pode assumir o serviço sozinha sem muitos prejuízos à disponibilidade do serviço ofertado, que, neste cenário, significa exibir o site para as pessoas que o acessam.\nNa prática, como funciona o deploy? Frequentemente, o ato de fazer deploy se resume às ações de copiar os arquivos de um lugar - que pode ser o repositório de código ou de artefato - e depositar ele no ambiente de destino.\nSeguindo o exemplo anterior do site, o ato de fazer o deploy corresponderia a copiar os arquivos html, css e javascript, que estão no repositório de código, e depositá-los no servidor que hospedará aquele ambiente.\nDeploy para testes do site usado como exemplo acima? O ato de fazer deploy seria resumido a copiar os arquivos do repositório de código e depositá-los no servidor que foi designado para ser ambiente de teste.\nConclusão O processo de deploy pode parecer simples, mas entender de fato como ele funciona é essencial para acompanhar tudo que é apresentado por aqui.\nAgradecimentos Obrigado a Somatório que, como sempre, revisou esse material antes dele sair.\nUm agradecimento especial a minha companheira (que ainda não tem twitter) Ana Carla, que fez uma revisão profissional do texto e me ajudou muito aqui. Obrigado meu amor.\nMeu agradecimento a Morvana e Guto Carvalho que revisaram esse artigo antes dele sair.\nEscrevi esse artigo ouvindo:\n Yung Buda My Chemical Romance Projota Beatles Outras músicas do meu Daily Mix do Spotify  ","permalink":"https://demo.gethugothemes.com/liva/examplesite/blog/o_que_e_deploy/","tags":["portugues","deploy","devops"],"title":"O que é Deploy"},{"categories":["portugues","pipeline"],"contents":"Contextualização Essa é a terceira parte da série \u0026ldquo;O que deve ter no seu pipeline?\u0026quot;, que tem como objetivo apresentar as melhores práticas para construção de um pipeline, baseada em minha experiência, seja em projetos ou em leitura.\nNesse artigo falaremos sobre o processo de revisão de código no Github baseado em pull request, desde a sua motivação, melhores práticas e como configurar seguindo as melhores práticas.\nO que é Pull Request? Antes de entender o que é um Pull Request é necessário entender os conceito de branchs , fork e o básico de repositórios git. Esse site pode lhe ajudar nisso.\nUma vez que você já sabe que a branch é uma ramificação do código, eu acrescento que o fork é uma cópia inteira do seu repositório. Essa cópia mantém uma ligação simbólica entre o fork e o repositório origem. De forma prática, essa ligação não tem grande efeito no uso do dia a dia, ou seja, se alguém fizer fork do seu repositório e fizer mudanças nesse fork o seu repositório não será afetado automaticamente.\nO seu repositório original só poderá ser alterado com commits diretos ou através de um pull request.\nPara evitar qualquer confusão, vamos dar nomes aos repositórios. Temos dois repositórios aqui:\nrepositório original, que é o primeiro repositório, aquele que foi a origem do fork. repositório fork, que é a cópia exata do repositório original no momento do fork.\nO pull request, é o pedido para que o repositório original, ou uma branch do repositório original, faça a ação de pull (puxar) as atualizações do repositório fork ou de um branch do próprio repositório. Confuso, né? Vamos para um exemplo.\nImagine que você tem um repositório que tem o código para um site, nesse site você recebe como entrada num campo o tempo de vida de um cachorro e você faz a conta para saber qual a \u0026ldquo;idade de cachorro\u0026rdquo; dele, depois de um tempo com esse projeto no ar uma pessoa muito interessada no seu projeto propõe colocar uma opção também para gatos.\nSó pra alinhar e facilitar o entendimento, o nome do repositório exemplo é gomex/idade-de-animal.\nEssa pessoa que pretende colaborar, ela faz um fork do seu repositório e agora ela tem um repositório chamado colaboradora/idade-de-animal (imaginando que o usuário dessa pessoa seja \u0026ldquo;colaboradora\u0026rdquo;, ok?). Esse repositório tem uma ligação simbólica com o gomex/idade-de-animal.\nTodas as mudanças feitas no \u0026ldquo;colaboradora/idade-de-animal\u0026rdquo; serão visíveis apenas nesse repositório e todas as mudanças feitas no \u0026ldquo;gomex/idade-de-animal\u0026rdquo; depois desse fork não serão automaticamente atualizadas no \u0026ldquo;colaboradora/idade-de-animal\u0026rdquo;, mas por definição não seria um problema, afinal a funcionalidade que a pessoa está trabalhando deve ser específica, ou seja, o que ela está trabalhando não deveria conflitar com as alterações que acontecem no repositório original, ou seja, não tem mais ninguém além dela trabalhando em \u0026ldquo;idade de gato\u0026rdquo;, né? Falaremos sobre resolução de conflitos depois.\nDepois que a colaboradora adiciona a funcionalidade de calcular a idade de gato o que ela faz? Ela faz um pedido para que o repositório \u0026ldquo;gomex/idade-de-animal\u0026rdquo; puxe (pull em português) tudo que \u0026ldquo;colaboradora/idade-de-animal\u0026rdquo; tem diferente do seu repositório e agora essa diferença faça parte do repositório oficial. Isso é o pull request. Um pedido para que o repositório original se atualize a partir de mudanças feitas no repositório novo criado a partir de um fork.\nVocê pode estar se perguntando \u0026ldquo;E se alguém nesse meio tempo adicionou uma funcionalidade nova tipo \u0026lsquo;idade de papagaio\u0026rsquo;, isso pode afetar o pull request do idade de gato?\u0026rdquo; A resposta é: depende.\nSe a funcionalidade for feita no mesmo local de código, mesmas linhas e afins, não terá problema, mas caso contrário teremos um conflito e trataremos disso em outro artigo.\nA minha sugestão é funcionalidades diferentes sejam tratadas de forma isolada, a fim de não causar conflito algum no processo.\nTodo esse processo que descrevi aqui, ele pode ser feito também baseado em branch, mas a pessoa que colabora precisa ser membro do repositório e não uma pessoa aleatória na internet, pois ela precisa ter permissão para criar branch no repositório. No fim é o mesmo propósito, mas ao invés de repositório inteiro, tudo que expliquei aqui acontece no nível de ramificações.\nComo usar Pull Request para o processo de revisão? A maioria das organizações utiliza o pull request como mecanismo padrão para revisão de código, pois ele é basicamente a \u0026ldquo;porta de entrada\u0026rdquo; para a base \u0026ldquo;oficial\u0026rdquo; de código, seja em relação ao repositório ou branch.\nNormalmente as branchs que serão usadas para construir o artefato final do repositório oficial são protegidas e não podem receber commits diretos, ou seja, tudo que entra nessas branchs devem entrar por um PR (pull request). Existe a possibilidade do administrador do repositório mandar o código direto, mas isso deve ser apenas uma exceção. Dito isso, eu reforço, mesmo os administradores do repositório, pessoas desenvolvedoras experientes, ou até mesmo a liderança técnica do time devem mandar suas mudanças por PR e elas devem ser avaliadas por outras pessoas.\nQuando começar a trabalhar em uma funcionalidade nova do repositório. Eu faço parte da organização? Tenho acesso a criar uma branch? Caso positivo, eu crio uma branch.\nExiste um Padrão para criação de branch? Eu gosto do modelo \u0026ldquo;feature/nome-da-funcionalidade\u0026rdquo; assim fica muito claro para todo mundo no que você está trabalhando. Se você usa algum sistema de ticket para gerenciar as tarefas você pode colocar o identificador do ticket também: \u0026ldquo;\u0026ldquo;feature/nome-da-funcionalidade#435\u0026rdquo;.\nLembre-se que sua branch precisa ser bem específica, ou seja, se \u0026ldquo;aparecer\u0026rdquo; outra demanda, o aconselhável é abrir outra branch a partir de branch \u0026ldquo;oficial\u0026rdquo; (que normalmente é a \u0026ldquo;master\u0026rdquo;).\nQuando você tiver muita confiança que seu código entrega tudo que a funcionalidade precisa para existir, você deve abrir um PR e na descrição desse PR você deve detalhar qual comportamento esperar dessa mudança que você está propondo.\nSegue abaixo um ótimo exemplo:\nO ideal é que o PR tenha o seguinte conteúdo:\n Descrição clara e objetiva do comportamento que será adicionado caso o PR seja aceito; Mínimo de detalhe sobre como a nova funcionalidade é usada, talvez um link para uma documentação externa seja uma boa, caso o detalhe seja muito grande; Passo a passo sobre como testar, da forma mais direta e clara possível e quais comportamentos esperados para os testes executados, ou seja, se for para clicar, diga onde clicar e o que deve acontecer se clicar no lugar informado, se possível diga também o que não deve acontecer.  Uma descrição seguindo esse modelo ajudará a pessoa que vai avaliar seu PR e ela talvez não precisará lhe perguntar nada, pois tudo que precisa saber sobre o trabalho e como avaliar ele está descrito lá.\nAcredite, cinco ou dez minutos investidos na criação de uma boa descrição de PR pode lhe \u0026ldquo;salvar\u0026rdquo; várias interrupções para explicação da sua mudança.\nA dificuldade em escrever na descrição do seu PR é um possível indicativo que você não está confiante e não tem uma real noção sobre o que foi entregue. Imagine que talvez esse seja o momento de você organizar mentalmente o que foi entregue.\nAlgumas pessoas criam uma PR draft (rascunho) para ir atualizando a medida que vão mexendo no código. Eu gosto desse modelo, pois assim nada se perde e você não precisa relembrar de tudo que foi feito em horas de trabalho naqueles últimos minutos de trabalho antes de entregar sua tarefa.\nComo revisar o Pull Request? A pessoa que vai olhar um PR ela precisa ter em mente alguns pontos:\n Qual o objetivo daquele PR? Ele está claro na descrição? As mudanças que estão sendo propostas no PR seguem o padrão que é usado nessa organização? A forma que a pessoa entregou à funcionalidade é a melhor? Existe maneira mais eficiente de fazer a mesma coisa? Os testes descritos no PR são o suficiente?  Qual o objetivo daquele PR? Ele está claro na descrição? É importante estar muito claro sobre o que se trata o PR em questão, pois a sua avaliação será com base nisso, sendo assim, a primeira coisa a analisar é a clareza no que está sendo entregue, se houver qualquer inconsistência nesse momento você deve pontuar e deixar claro baseado em que está fazendo a avaliação.\nUm exemplo:\nVocê abre o PR sobre idade de gatos, lê a descrição e não está claro pra ti se ideia é criar uma forma separada para calcular idade de outros animais ou apenas colocar uma opção na lógica atual feita para cachorro, sendo assim seu comentário poderia ser:\n\u0026ldquo;Não está claro pra mim se você colocou a lógica de calcular idade pra gato separado porque seja de fato a forma que você acha que seja ideal ou se fez isso apenas para não conflitar com o código original por agora e refatorar no futuro. Eu vou analisar seu código atual separado mesmo, mas adianto que mudar para que evite repetição de código seja uma boa no futuro\u0026rdquo;\nPronto, com isso você está dizendo que sua análise não levará em conta a quantidade de repetição de código que isso possa gerar e que você não concorda, mas que por agora não vê problemas nisso.\nAs mudanças que estão sendo propostas no PR seguem o padrão que é usado nessa organização? A maioria das organizações segue alguns padrões/estilos na escrita do código, seja em sua formatação (ex. quatro espaços, ponto e vírgula em cima ou embaixo) ou em como organizar funções, métodos e afins. Essas regras devem estar disponíveis em algum lugar para serem utilizadas por quem quiser contribuir com o projeto. O GitHub sugere que elas sejam publicadas em um arquivo chamado CONTRIBUTING.md, disponibilizado no próprio repositório do projeto.\nAssim, à pessoa que submete mudanças cabe ler e respeitar as regras (seria a \u0026ldquo;etiqueta\u0026rdquo;), mas sabemos que nem sempre isso é possível e, dessa forma, a colaboração pode não seguir os padrões. Caso o PR submetido para avaliação não respeite alguma das diretivas do projeto, você - que está avaliando - deve deixar bem claro para a pessoa qual regra ela está infringindo e em qual parte do código isso acontece. Para facilitar essa comunicação, o GitHub oferece possibilidade de adicionarmos comentários às linhas de código do PR.\nDepois que comentar todos os pontos relevantes, não se esqueça de finalizar sua revisão, pois caso não faça isso quem fez o PR não verá seu comentário.\nSe precisar que a pessoa atualize algo para que o PR seja aceito, escolha a opção \u0026ldquo;Request changes\u0026rdquo; (Solicitar mudanças); caso precise de mais tempo para decidir sobre aceitar ou não, comente sem aprovar; caso esteja tudo certo, aprove.\nA forma que a pessoa entregou à funcionalidade é a melhor? Existe maneira mais eficiente de fazer a mesma coisa? Esse ponto é um pouco abstrato, pois depende muito da experiência de quem está revisando, mas é talvez a parte mais importante desse processo de revisão. Está aqui a grande oportunidade de uma pessoa proporcionar para a outra que mandou o PR maneiras de deixar o código ainda melhor.\nATENÇÃO!!! Não faça uso desse espaço para diminuir ou ridicularizar a pessoa que mandou o PR, pois caso faça isso, além de perder uma grande oportunidade de melhorar a habilidade de outra que você \u0026ldquo;julga\u0026rdquo; inferior, você também perderá a oportunidade de ser uma pessoa melhor. Ajudar as pessoas que trabalham no mesmo projeto que você é a coisa mais básica de trabalhar em equipe. Caso tenha problemas em trabalhar dessa forma, aconselho criar um projeto onde você seja a única pessoa a enviar código.\nMais importante de que enviar as sugestões de mudança e apontar os erros do PR é validar se de fato isso é um erro ou uma abordagem diferente, da qual você discorda.\nSe sua sugestão melhorar a performance do que será entregue, tente mostrar algum elemento que embase sua sugestão.\nSe sua sugestão tem como objetivo seguir uma boa prática, aponte o link para onde a pessoa possa ler mais sobre ela, e se possível, aponte caminhos para que a pessoa possa aplicar aquela melhor prática de uma forma mais fácil. Essa é uma boa oportunidade para exercitar seu uso dessa boa prática também.\nOs testes descritos no PR são o suficiente? É importante avaliar se há testes o suficientes e não importa se os testes podem ser de exploração ou automatizados, você precisa praticar a avaliação disso. Não precisa ser uma pessoa especializada em QA (Quality Assurance) para fazer isso.\nTer uma pessoa QA no seu time é aconselhável, mas não ache que ela será a única a fazer essa análise. Nos primeiros PR você pode pedir a ajuda dela e fazer essa parte da avaliação juntas, mas aconselho que pratique o suficiente para internalizar esse tipo de revisão, pois a necessidade de entender qualidade de código, assim como segurança, \u0026ldquo;devops\u0026rdquo; ou afins deve ser de interesse de todos. Esses assuntos devem ser uma preocupação do time e não apenas de um cargo específico. A pessoa que está nesse cargo deve ser responsável por ajudar o time a evoluir nesse assunto, ajudando como uma espécie de consultor interno. Repito, essa pessoal não deve ser a única responsável sobre o assunto que é experiente.\nRecebi uma lista imensa de coisas a corrigir no meu PR, fico triste? Caso a pessoa que comentou no seu PR foi respeitosa e teve cuidado ao criar o review, não há motivos para tristeza. Encare essa longa lista de correções como uma boa experiência para melhorar sua habilidade de escrever código.\nA pessoa que mandou o PR não é necessariamente melhor do que você, ela apenas dedicou parte do seu tempo para escrever melhorias no seu trabalho. Ela teve a atenção e cuidado necessário para ajudar o time como um todo para entregar um código melhor para a organização. Ela provavelmente não tem nada contra você, e quanto mais detalhista ela foi, isso não é necessariamente uma coisa ruim, pois muitas vezes é a oportunidade que você terá para olhar nesse nível de detalhe o seu trabalho.\nNormalmente os prazos em empresas são apertados e uma longa lista de correções desanima, mas veja que o problema está no prazo curto que normalmente as empresas trabalham. Nesse caso, o que pode ser feito se divide em duas possibilidades:\nVocê pode calcular no futuro o prazo levando em consideração esse nível de exigência na revisão Negociar com a pessoa que revisou partes das críticas, tentando explicar sobre os prazos e afins.\nUma dica aqui é fazer com que seu PR seja o menor possível, pois quanto menor a possibilidade de retrabalho no retorno da revisão é menor.\nQuantas pessoas devem revisar meu código? Algumas empresas colocam no mínimo duas, outra colocam três, mas existem muitas que com apenas uma revisão já torna o PR disponível para ser de fato aceito. Isso depende da empresa.\nO ideal seriam duas pessoas, mas se isso atrasar demais o andamento do seu projeto, uma deve ser o suficiente, mas lembre que essa pessoa a revisar terá muito mais responsabilidade e normalmente não poderá ser uma pessoa com pouca experiência. Isso quer dizer que você estará usando ainda mais o tempo de pessoas mais experientes para avaliação de código do que de fato produzindo códigos.\nConclusão O PR é um método ideal, simples, com possibilidade de interação assíncrona através de comentários no código, possibilidade de debate, múltiplas opiniões e uma forma centralizada de entender como seu código avançou ao passar do tempo, quais os motivos que deixaram determinado comportamento entrar no código e quais foram as argumentações que embasaram as decisões.\nUma ferramenta ideal, que se usada sabiamente, pode ser muito poderosa!\nAgradecimentos Obrigado a Somatório que, como sempre, revisou esse material antes dele sair.\nMeu agradecimento a Morvana e Giu que revisaram esse artigo antes dele sair.\nUma honra esse artigo ter sido revisado pelo meu ídolo Guto Carvalho! Obrigado!\nEscrevi esse artigo ouvindo:\n Megadeth Git Scott Heron Cascadura Belchior Fantasmão Outras músicas do meu Daily Mix do Spotify  ","permalink":"https://demo.gethugothemes.com/liva/examplesite/blog/o_que_deve_ter_no_pipeline-pr/","tags":["portugues","pipeline as code","pipeline","devops","qa"],"title":"Precisamos falar sobre Pull Request"},{"categories":["portugues","herois"],"contents":"Você reconhece eles no seu dia a dia? Aquelas pessoas incríveis, que fazem coisas sensacionais e que você tem certeza que o mundo seria bem pior sem essas pessoas.\nNão espere seus heróis morrerem para reconhecer-los, eles estão ai ao seu redor, fazendo a vida de todos um lugar melhor, com todo peso de ser um herói, pois é um peso muito grande ser um herói.\nOs heróis choram? Eles caem do céu em seu voo matinal? Ninguém poderá saber. A vida que ele leva, não não é não. Esconde nas trevas o que lhe dar prazer.\nSem mocinho, eis nosso herói. Robson Veio, uma das pessoas mais incríveis que já apareceu em minha vida. Uma herói vivo, carne, osso e muita sabedoria.\nQuem o ver sorrir, não sente a dor e o peso da responsabilidade de ser o que ele é. Felizmente eu acho que ele é também a pessoa mais preparada para lidar com tudo isso.\nÉ um grande prazer ser seu amigo, nas horas boas e ruins e poucas pessoas sortudas sabem a delícia de ser próximo a ti ;)\nHoje é o aniversário dele e ele não merecia menos do que isso. Já que não posso dar um abraço, aqui vai meu abraço virtual.\nTe amo,\nOBS: Obrigado Cascadura por essa música sensacional. Ela me inspirou nesse texto.\n","permalink":"https://demo.gethugothemes.com/liva/examplesite/blog/veio/","tags":["portugues","herois","amigos","amor"],"title":"Seus heróis estão vivos?"},{"categories":["portugues","pipeline"],"contents":"Contextualização Essa é a continuação da série \u0026ldquo;O que deve ter no seu pipeline?\u0026quot;, que tem como objetivo apresentar as melhores práticas para construção de um pipeline, baseada em minha experiência, seja em projetos ou em leitura.\nGithub actions Lembra que falamos sobre \u0026ldquo;Qual software de pipeline (CI/CD) você deve usar?\u0026quot;, para demonstrar aqui na prática o que deve ter no seu pipeline eu usarei o Github actions como ferramenta de pipeline, pois para repositórios públicos ele funciona sem custos e precisa de quase nada para começar a usar.\nCrie a pasta .github/workflows na raiz do seu repositório. Vamos precisar posteriormente.\nSe você ainda tem dúvida de qual ferramenta usar e quer ler um pouco sobre isso. Leia de novo esse artigo.\nCódigo exemplo Usarei esse repositório como exemplo para demonstrar a implantação de todos os passos.\nO que deve ter no seu pipeline? Como já exaustivamente explicado em outros momentos dessa série, sabemos que o teste estático de código é uma das primeiras etapas de um pipeline, sendo assim vamos demonstrar aqui como executar o teste unitário de uma aplicação exemplo escrita em go.\nTeste unitário Para executar o teste é necessário uma série de passos e vamos detalhar eles um pouco mais nos tópicos a seguir.\nDentro da pasta .github/workflows crie um arquivo chamado validate.yml com o seguinte conteúdo:\nname: Go on: push: branches: [ master ] pull_request: branches: [ master ] jobs: test: name: test runs-on: ubuntu-20.04 steps: - name: Set up Go 1.14 uses: actions/setup-go@v2 with: go-version: 1.14 - name: Check out code uses: actions/checkout@v2 - name: Test run: go test -v -coverprofile=coverage.out Detalhando o arquivo descritivo do pipeline Vamos por partes, o começo do arquivo basicamente descrever o nome e a condição de execução do workflow do github actions em questão, pois é possível ter múltiplos workflows executando em paralelo a partir do mesmo commit.\nname: Go on: push: branches: [ master ] pull_request: branches: [ master ] A opção on informa que esse workflow apenas será executado se houver um push na branch master ou um Pull Request (PR) que tem como destino a master.\nSe houver um commit direto na master, esse workflow executará automaticamente e se alguém criar uma branch, fizer modificações e então quiser fazer com que seu código seja feito merge com oa branch master, ao criar um PR acionará automaticamente esse workflow.\nVale a pena reforçar que esse fluxo de etapas acontecerá com alguma frequência. Evite tarefas que não precisem necessariamente serem executadas a cada PR. Alguns exemplos de etapas que não fazem sentido de executar quando alguém abrir um PR:\n Build, tag e push de um artefato Deploy em staging ou produção  O que é PR (Pull request)? No git você tem a possibilidade criar várias branchs a partir de uma branch existente. A branch padrão normalmente é a master, dai normalmente o fluxo de trabalho para quem usa git é:\n Criar uma branch nova de trabalho a partir da branch master; Enviar suas modificações para esse branch de trabalho recém criada; Em algum momento seu trabalho estará pronto para ser compartilhado com o time; Você abre um pull request, que é o pedido para que o contéudo da sua branch seja copiada para a master; Outras pessoas do seu time avaliam o seu pedido e olham se o código enviado de fato atende da melhor forma possível; Essas pessoas aceitam o seu PR e seu código agora faz parte da branch master, onde o ciclo todo pode começar de novo com outra pessoa ou você novamente.  Obs: O pull request pode ser aberto de qualquer branch para uma outra a sua escolha. O exemplo acima foi apenas para apresentar o conceito de forma prática\nPor que determinadas tarefas não devem executar a cada PR? Imagine que o PR é uma ferramenta que deve ser usado por qualquer pessoa da sua empresa. Isso deve ser estimulado, pois a transparência e ampla colaboração tende a colaborar a qualidade do seu código.\nLevando com consideração que qualquer pessoa pode abrir um PR, você quer mesmo que seu pipeline gaste tempo gerando um artefato novo a cada PR? Se é um pedido, que deve ser revisado, existe motivo para que um artefato seja gerado, aplicado tag e enviado para um repositório centralizado? Repositório esse que normalmente tem seu custo associado a quantidade de uso, ou seja, quanto mais push de artefatos novos, mais caro será sua conta.\nUma consequência similar acontece no deploy, pois se você permite que o PR acione automaticamente um deploy em staging ou production, você não está fornecendo o tempo necessário que uma pessoa possa revisar devidamente o código antes dele ser aceito em staging ou production, ao abrir o PR o github action fará o deploy automaticamente.\nJobs Dando continuidade ao detalhamento do arquivo validate.yml desse pipeline, falaremos sobre configuração de jobs:\njobs: validate: name: validate runs-on: ubuntu-20.04 A definição de jobs nesse arquivo tem como proposito descrever quais etapas desse pipeline executarão a partir do mesmo sistema operacional e de for sequêncial, ou seja, se você precisar que seu build execute em multiplos sistemas operacionais, tal como windows e linux, será necessário informar um job para cada proposito.\nSempre que fizer um pipeline, inicie ele simples, coloque apenas um caminho a ser seguido, evite condicionais, faça com que ele cresça baseado nas demandas e não nas suas expectativas, isso quer dizer que deixar ele quebrar e apresentar alguns dados é uma boa maneira de deixar ele o mais enxuto possível, pois um pipeline complexo tem maior possibilidade de sofrer pouca manutenção por outros membros do time.\nLembre-se! O pipeline deve ser do time e não seu, pois se seu tempo for todo dedicado apenas para corrigir pipelines, voltaremos ao tempo em que as pessoas não tinham tempo porque estavam executando tarefas muitos simples e operacionais, que somente você poderia fazer.\nA descrição validate define o nome desse job e o runs-on onde as etapas desse pipeline serão executadas.\nPerceba que além de informar que precisa que seja GNU/Linux e distribuição ubuntu, é informado qual versão especifica do Ubuntu você deseja: ubuntu-20.04.\nSempre que possível informe as versões do que deseja para instalar, configurar ou afins, pois somente assim você terá controle que aquele pipeline se comportará de acordo com o que você espera dele. Imagine uma versão nova do Ubuntu que remova ou mude o comportamento de uma funcionalidade que você use para criar seu artefato. Seu pipeline irá falhar por um motivo completamente fora do seu escopo de trabalho.\nExiste uma ideia amplamente seguida por pessoas que usam o pipeline, que é a ideia que se o pipeline quebrou, o motivo deveria ser daquele commit que fez esse pipeline ser inciado. Isso quer dizer que qualquer fator externo que possa fazer ele quebrar deve ser isolado.\nPassos do seu pipeline Na descrição seguinte temos a descrição dos passos. Lembre-se que elas seguem a ordem do arquivo, ou seja, as primeiras etapas serão as primeiras a serem executadas e a posterior só acontece se o passo em questão não falhar.\nsteps: - name: Set up Go 1.14 uses: actions/setup-go@v2 with: go-version: 1.14 - name: Check out code uses: actions/checkout@v2 - name: Test run: go test -v -coverprofile=coverage.out Em cada passo você pode informar:\n name: O nome do passo. Ele serve para que você possa saber qual passo falhou e por qual motivo no dashboard. uses: Aqui é informado a ação a ser executada. Uma ação é um bloco de código que executa uma determina função. Ela pode ser de um repositório externo ou dentro do seu repositório. Por hora vamos usar apenas de outros repositórios. Se quiser mais detalhe pode ler aqui run: É uma alternativa do uses e deve ser usado sempre que você for apenas rodar um comando no sistema operacional escolhido na opção runs-on (ubuntu-20.04). with: Você pode especificar variáveis de ambiente, que normalmente são necessários para utilizar uma determinada ação no uses. Cada ação especifica em sua documentação individual quais variáveis de ambiente é necessário informar. Acesse a documentação do setup-go e veja isso na prática.  O primeiro passo desse fluxo é o Set up Go 1.14 ele é responsável por configurar o ambiente para se utilizar o go na versão que você deseja. Aqui é basicamente o momento que será instalado tudo que você precisa do go no ubuntu que foi especificado anteriormente nesse arquivo.\nUm segundo passo é o Check out code, que é responsável por baixar todo o seu código no diretório de trabalho do build. Esse passo é automatico na maioria das outras ferramentas de CI/CD, mas no Github actions é necessário especificar o passo, caso contrário tudo que precisar do código para executar não funcionará corretamente.\nO terceiro passo é o Test, aqui é onde acontece o teste unitário do Go. Veja que ele depende dos binários do go, por isso ele acontece depois do Set up Go 1.14 e também depende que o código esteja no diretório de trabalho, por isso ele será executado depois do passo Check out code.\nDentro do passo Test o comando acompanha a opção -coverprofile=coverage.out que será utilizada em um artigo posterior, onde avançaremos em mais detalhes sobre cobertura de testes e afins.\nConclusão É importante sempre ficar atento a ordem dos passos e como umas influenciam as outras. Cada ferramenta de CI/CD terá sua particularidade e isso muitas vezes afeta diretamente em como seu pipeline é configurado.\nAgradecimentos Meu agradecimento a Victor que revisou esse artigo antes dele sair.\nEscrevi esse artigo ouvindo:\n Foo Fighters Bob Dylan Arnaldo Baptista Marília Mendonça Beethoven Outras músicas do meu Daily Mix do Spotify  ","permalink":"https://demo.gethugothemes.com/liva/examplesite/blog/o_que_deve_ter_no_pipeline-testes/","tags":["portugues","pipeline as code","pipeline","devops","qa"],"title":"O que deve ter no seu pipeline? Testes!"},{"categories":["portugues","pipeline"],"contents":"Contextualização Esse artigo faz parte da série \u0026ldquo;O que deve ter no seu pipeline?\u0026quot;, que tem como objetivo apresentar as melhores práticas para construção de um pipeline, baseada em minha experiência, seja em projetos ou em leitura.\nQuando pensei em escrever sobre \u0026ldquo;O que deve ter no seu pipeline?\u0026rdquo; muitas pessoas pediram para eu fazer comparações entre ferramentas de CI/CD, e por conta disso vou escrever um artigo especificamente sobre isso.\nQual software de pipeline (CI/CD) você deve usar? Antes de você iniciar uma saga para encontrar a melhor ferramenta de CI/CD que você possa, tente elaborar um pouco quais são seus requisitos. Os meus são nessa ordem:\n Suporte a pipeline as code Suporte a builds com container docker Boa documentação Opensource Comunidade ativa  Suporte a pipeline as code Se a ferramenta não tiver suporte a definir meu pipeline como código, eu nem cogito a testar a ferramenta.\nO que é Pipeline as code? É a possibilidade de colocar as descrição de cada passo do seu pipeline em um arquivo definição, ao invés de ficar clicando em uma interface web e armazenando essas configurações em uma base de dados.\nSegue abaixo as vantagens de ter as definições em um arquivo:\n Os passos que sua aplicação segue até o deploy em produção estarão documentados em um arquivo dentro do seu repositório de código, ou seja, qualquer pessoa que ler essa informação estará rapidamente ciente de todo processo; O arquivo de definição será versionado junto com seu código, isso indica que você pode ver quando um determinado passo mudou e afetou o comportamento do seu pipeline; Uma vez versionado esse arquivo fica aberta as sugestões e qualquer pessoa pode mandar um PR (pull request) como proposta para mudar o comportamento do seu pipeline. É possível inclusive configurar seu pipeline para que esse PR seja executado com o arquivo proposto, dessa forma validando, ou não, a hipótese da pessoa que enviou o PR.  Segue um exemplo de um arquivo pipeline as code:\n--- kind: pipeline type: docker name: default steps: - name: teste image: \u0026lt;nome_da_imagem\u0026gt; commands: - \u0026lt;comandos para instalar pacotes que precisam no teste\u0026gt; - \u0026lt;comando para teste\u0026gt; - name: build image: \u0026lt;nome_da_imagem\u0026gt; environment: ENV: \u0026quot;production\u0026quot; commands: - \u0026lt;comandos para instalar pacotes que precisam no build\u0026gt; - \u0026lt;comando para build\u0026gt; Como vocês podem ver no exemplo resumido acima, é possível saber quem primeiro executa o teste, sem qualquer variável de ambiente. No passo seguinte, uma variável de ambiente é necessário para que o build execute de forma correta, e em seguida são demonstradas os comandos que precisam para executar o build.\nQualquer pessoa do time que abrir esse arquivo, seja uma pessoa nova ou mais antiga na equipe conseguirá entender como o processo de teste e build funcionam no pipeline.\nSuporte a builds com container docker A possibilidade de eu usar uma ferramenta de CI/CD que não permite executar builds com container docker é muito baixa.\nO que é na prática \u0026ldquo;builds em container docker\u0026rdquo;? Cada passo do seu pipeline é executado em algum lugar, ou seja, quando você diz na etapa do pipeline que você quer executar o comando de teste unitário, por exemplo, esse teste será executado em um agente e caso seu pipeline não tenha suporte a execução em docker, esse agente será uma máquina que foi previamente configurada.\nEssa máquina tem binários que foram instalados e configurados previamente. E se você precisar de bibliotecas novas ou uma versão mais atualizada do binário que é responsável pelo teste? A opção seria colocar o comando para instalar esses pacotes antes de executar o teste, correto? Errado!\nUm agente de build nesse modelo descrito acima é normalmente compartilhado. Isso quer dizer que um comando para atualizar o pacote, poderá afetar outro time, ou até mesmo seu próprio time na execução de pipelines antigos para simulação de comportamentos anteriores ao código atual.\nQuando seu pipeline tem suporte a execução das etapas em container docker, isso quer dizer que cada passo do seu pipeline iniciará um container docker a partir de uma imagem descrita por quem idealizou o pipeline. Isso implica que você pode informar a imagem que deseja usar, colocar todos os comandos para atualizar a biblioteca ou binário e então executar o teste sem culpa, pois ao terminar todos os comandos desse passo, o container será destruído e reconstruído do zero na próxima execução.\nSe você precisar executar um pipeline antigo, não terá problema, pois nesse pipeline estará a versão que funcionava, inclusive com os comandos específicos para instalação dos pacotes necessários. Aqui entra uma dica MUITO importante:\nSempre que utilizar uma imagem e precisar instalar um pacote dentro dela, informe de forma específica qual a versão que deseja instalar.\nVejam no exemplo abaixo:\n--- kind: pipeline type: docker name: default steps: - name: teste image: ruby:2.7.1-buster commands: - gem install bundler -v 2.1.3 - bundle exec rspec Se você não informar a versão específica de qual bundler você quer instalar, ele instalará a mais atual do momento e isso pode ser um problema, pois pacotes são atualizados o tempo todo e seu pipeline poderá quebrar porque você instalou de forma equivocada de algum binário a ser usado, pois o comportamento mudará.\nUma das coisas mais importantes de um pipeline é garantir que ele tenha o comportamento esperado. Quanto mais mecanismos você acrescentar para que cada execução de um pipeline sempre aconteça de maneira esperada, melhor ele será para você encontrar o problema de fato quando quebrar, pois você não terá dúvida que o motivo esteja de fato relacionado a modificação do código em questão.\nDesvantagem ao utilizar \u0026ldquo;builds em container docker\u0026rdquo;? Lembra que cada execução de um pipeline é único e o container é destruído após o ultimo comando daquela etapa? Isso implica que todos os arquivos gerados naquele passo serão destruídos por padrão.\nA ideia é que qualquer dado que precise ser usado em outra etapa que seja persistido de outra forma. Segue algumas dicas:\nSe for arquivo, a maioria das ferramentas oferece a possibilidade de montar um volume que pode ser compartilhado entre as etapas Se for uma imagem, biblioteca ou pacote, esse deve ir para um repositório e então no passo que precise ser usado, deve conter um comando para baixar a dependência e instalar.\nConclusão Não é necessário detalhes sobre \u0026ldquo;Boa documentação\u0026rdquo;, \u0026ldquo;OpenSource\u0026rdquo; e \u0026ldquo;Comunidade ativa\u0026rdquo;. Basta que cada item desse seja verdadeiro para ser considerado como um ponto positivo.\nNão há ferramenta perfeita, mas existem algumas funcionalidades que você não deveria abrir mão na hora de escolher a sua. Use algumas, veja qual seu time usaria melhor e proponha um teste, pois afinal de contas não importa qual ferramenta você usa e sim como você configura seu pipeline.\nAgradecimentos Obrigado a Somatório que, como sempre, revisou esse material antes dele sair. Obrigado também a Morvana, Giu e Victor que também revisaram esse artigo antes dele sair.\nEscrevi esse artigo ouvindo:\n Megadeth Bob Dylan Backstreet Boys Emicida Beethoven Outras músicas do meu Daily Mix do Spotify  ","permalink":"https://demo.gethugothemes.com/liva/examplesite/blog/o_que_deve_ter_no_pipeline-2/","tags":["portugues","pipeline as code","pipeline","devops","qa"],"title":"Qual software de pipeline (CI/CD) você deve usar?"},{"categories":["portugues","pipeline"],"contents":"Contextualização Ao longo de alguns anos de experiência tenho percebido que muitas pessoas tem dúvidas sobre quais os elementos que podem ser usados para compor um pipeline de entrega de produto.\nPretendo iniciar uma série de artigos para tentar compartilhar o pouco que sei sobre o assunto.\nNão tenho pretenção alguma de aqui fundar nenhum padrão ou ideia nova. O que apresento aqui é nada mais do que a soma de experiências, algumas minhas, mas muito mais de outras pessoas, então não tenho intenção alguma de tomar para mim todo crédito, afinal toda construção de novo conteúdo é assim, correto? 10% experiência própria e 90% de aprendizado prévio.\nNão vou me aprofundar sobre o que é pipeline, nem muito menos tentar lhe convencer a usá-lo, pois se chegou até aqui por esse título, acredito que você já tenha interesse. Prometo que se tiver demanda sobre um artigo específico sobre o que é pipeline, eu escrevo em outra oportunidade sobre esses assuntos.\nVou levar em consideração também que você já sabe o que pipeline as code significa, mas se não souber, não causará muito problema ao seu aprendizado.\nProblema Antes de \u0026ldquo;colocar a mão na massa\u0026rdquo; e iniciar o processo de construção do seu pipeline, você precisa entender qual problema você está tentando resolver, pois toda intervenção na computação tem (ou deveria ter) como objetivo resolver algum problema, correto? Mesmo que o problema seja otimização, por conta de performance, ou trabalho proativo para que não exista problema no futuro.\nQuando você inicia a construção de um pipeline, normalmente, seu objetivo é entregar um produto. Seja ele de software ou infraestrutura.\nSe a solução do problema aqui é entregar o produto de forma automatizada, você precisa entender quais são os passos que seu produto precisa seguir para ser colocado em produção.\nA ordem importa? Antes de apresentar os passos, precisamos primeiro entender que a ordem das etapas do pipeline importam e muito, sendo assim apresentarei as etapas aqui na ordem que elas devem estar no seu pipeline.\nPor que a ordem importa? Uma das vantagens de usar pipeline no processo de entregar de software é a ideia dele \u0026ldquo;economizar\u0026rdquo; tempo das pessoas que estão produzindo código, então o ideal é que as tarefas que demoram menos, entregam algum valor e não dependem de outros passos posteriores sejam as primeiras no seu pipeline.\nVamos usar um exemplo abstrato. No processo de entrega de um software hipotético, temos os seguintes passos:\n Build do artefato Teste unitário Provisionamento da infra pré-produção Teste de integração Teste de aceitação Provisionamento de infra produção Deploy de pré-produção Deploy de produção  Na sua opinião, qual seria o primeiro? Vamos analisar alguns dos candidatos:\nBuild do artefato depende de outro passo? Não. Ele entrega valor? Entrega sim, pois se o build quebrar, a pessoa que está desenvolvendo saberá que tem problemas para fazer build, mas esse processo de build costuma demorar demasiadamente e isso pode fazer com que o feedback seja demorado. Vamos imaginar juntos: A pessoa manda o commit para o repositório, o pipeline automaticamente é executado e depois de alguns longos minutos a pessoa que mandou o commit poderá descobrir que errou, pois a etapa de build vai executar a construção do artefato e assim pegará qualquer problema que apareça nesse processo. Muitas vezes um detalhe bobo pode levar a quebrar o pipeline nessa etapa.\nE se pensarmos no teste unitário? Depende de outro passo? Não. Ele entrega valor? Entrega sim, e aqui temos um detalhe diferente do build do artefato, pois o retorno é mais rápido, uma vez que, normalmente, nada precisa ser realmente construído. Por padrão os testes unitários demoram menos do que o build dos artefatos. Voltando ao processo de imaginação: A pessoa manda o commit para o repositório, o pipeline automaticamente é executado e depois de segundos ela já terá um feedback que um determinado teste não está passando. Tudo por culpa daquele \u0026ldquo;detalhe\u0026rdquo; bobo que falamos anteriormente.\nSeguindo essa lógica, o primeiro passo desse pipeline seria o teste unitário, pois não há nada que demore menos e ainda assim não dependa de outro passo. Vejam que são sempre múltiplos fatores para determinar a ordem e em minha opinião são normalmente esses:\n Dependência de outro passo Entrega de valor Tempo de feedback  Quando eu falo de entrega de valor, a preocupação é com o processo de desenvolvimento e não apenas com o produto finalizado. Para o produto finalizando o build talvez seja mais importante do que o teste unitário, pois levando em consideração o processo de desenvolvimento eles tem importâncias bem próximas.\nConsiderações finais Nesse artigo foi iniciado uma análise sobre como organizar seus passos dentro de um pipeline, nos próximos artigos tentarei ir mais a fundo em cada passo, mostrando inclusive alguns exemplos práticos de como fazer de fato.\nAgradecimentos Obrigado a Somatório que, como sempre, revisou esse material antes dele sair.\nEscrevi esse artigo ouvindo:\n Foo Fighters Yung Buda Racionais Sabotagem Beethoven Outras músicas do meu Daily Mix do Spotify  ","permalink":"https://demo.gethugothemes.com/liva/examplesite/blog/o_que_deve_ter_no_pipeline/","tags":["portugues","pipeline as code","pipeline","devops","qa"],"title":"O que deve ter no seu pipeline? Parte 1"},{"categories":["portugues","remoto","dicas"],"contents":"Contextualização Eu escrevi um artigo com dicas sobre como trabalhar remoto, mas boa parte delas se destinavam aos trabalhadores. Nesse texto foco agora em pessoas que tem como objetivo gerir/coordenar pessoas, e sendo assim precisam estar atentas a produtividade e foco dos seus profissionais.\nEu não sou gestor, mas está aqui algumas dicas que já vi ótimos gestores fazendo comigo e deram super certo. São quase 5 anos de remoto e já tive todo tipo de gestor. Acho que tenho alguma bagagem para falar sobre isso.\n1. Seu time VAI performar menos nos primeiros dias Se seu time NUNCA fez remoto e fará agora em regime de urgência, se prepare, pois a entrega será comprometida. As pessoas ainda precisarão se habituar com isso e esse processo leva tempo. Tenha paciência! Alinhe com o negócio que é uma medida de urgência e logo o time voltará a performar como esperado.\n2. Reuniões diárias Agende um horário para conversar com todo time e evitar ficar interrompendo eles a todo momento para saber o que estão fazendo. Esse momento cada pessoa deve falar sobre o que está fazendo e/ou qual atividade pretende fazer ao longo do dia.\nPerceba que nem todo mundo conseguirá ser assertivo aqui de primeira. Isso leva tempo.\nEssa reunião não é um momento de cobrança e sim de compartilhamento de status e possíveis bloqueios que venham a acontecer no futuro. Se antecipar a problemas no modelo remoto é o nível avançado desse tipo de trabalho.\nEsse ambiente precisa ser seguro! A pessoa precisa estar confortável para compartilhar nesse momento. Todos devem falar ao menos o status do que está fazendo e como pretende continuar a fazer.\n3. Esteja disponível para reunião individual Informe a seus funcionários que você está disponível pra conversar individualmente, mas solicite cautela nesse convite, pois todos do time precisa fazer isso. Se uma pessoa monopolizar seu tempo, outras não poderão ter a mesma oportunidade.\nUse serviços como esse, que oferecem slots de tempo na sua agenda para marcação de reuniões. Atente para deixar sua agenda atualizada, ou seja, se tiver ocupado naquele horário, coloque na sua agenda, caso contrário o serviço poderá agendar em um momento que você não tem disponível.\nEssa reunião individual serve para alinhamento de expectativas individuais, para compartilhar alguma coisa pessoal, ou até mesmo discutir sobre a carreira da pessoa. Esse papo, que em muitos lugares chamam de 1 a 1, é muito benéfico para todos.\n3. Sua visibilidade agora é outra Oriente seus funcionários a usarem o chat escolhido para compartilhar informações e evitar outros espaços não oficiais para troca de informação, pois assim todos podem estar cientes do que está acontecendo.\nEu sei que nesse momento você estará um pouco ansioso para saber se a pessoa está trabalhando ou não, mas tente dar o benefício da dúvida para todos. Espere o melhor das pessoas e tente captar o que está rolando através dos status no chat e nas reuniões diárias. Se perceber que algo está fora do esperado, chame a pessoa no privado e converse, evite constrangimento em público.\nEssa ideia de \u0026ldquo;dar exemplo\u0026rdquo; em público não funciona bem. Isso criará um ambiente hostil e as pessoas passarão a esconder as coisas de você.\n4. Pense antes de escrever algo, principalmente em grupos de várias pessoas A comunicação online é diferente da presencial. Você precisa deixar seu texto claro, e atentar para que ele não fique desnecessariamente rude e grosso, pois isso poderá causar um mal-estar desnecessário no seu time.\nTenha empatia! Assim como você, todos estão se adaptando também.\n5. O trabalho agora é assíncrono A pessoa não estará 100% do tempo olhando o chat, ela estará ocupada trabalhando, então se você mandar uma mensagem, pense que essa pessoa poderá demorar algum tempo para responder, ok?\nSe for MUITO urgente, ligue para pessoa.\n6. Evite ligar MUITO para seus trabalhadores A ligação ao telefone deve ser último recurso. Se você usar muito isso a pessoa poderá demorar a lhe atender e pode ser justamente em um momento que você precisava da urgência.\nLembre que cada ligação sua causará um grande impacto na atenção daquela pessoa, que já está sofrendo em trabalhar de cada pela primeira vez e tendo que lidar com toda distração que uma casa ainda não preparada para remoto oferece.\n7. Valide se cada pessoa tem tudo que precisa Verifique se a pessoa tem todos os acessos, VPN, equipamentos e instruções para trabalhar de casa.\nAs vezes algumas pessoas perdem muito tempo em casa esperando isso.\n8. Atualize seu status Algumas ferramentas de chat permitem que você adicione seu status, ou seja, se fizer uma pausa coloque que está em pausa. Normalmente algumas pessoa usam o termo AFK (Away from keyboard - Longe do teclado), sendo assim a pessoa que está do outro lado sabe que se pedir algo nesse momento demorará um pouco mais de ser respondido.\nÉ interessante alinhar com seu time os status e o que esperar de cada um deles, ok?\nConclusão O trabalho em casa é um pouco diferente do trabalho presencial, mas completamente possível, caso você mude um pouco sua postura. Ele agregará muito na sua produtividade.\nComente aqui se você tem dicas novas ou ponto de vista diferente das dicas que passei aqui.\n","permalink":"https://demo.gethugothemes.com/liva/examplesite/blog/dicas_remoto_gestor/","tags":["portugues","remoto","coronavirus","dicas"],"title":"Você tem uma equipe de TI e tem dúvidas com o trabalho remoto em tempos de coronavírus?"},{"categories":["portugues","remoto","dicas"],"contents":"Contextualização Em tempos de pandemia de coronavírus pelo mundo, a orientação mais adotada pelas empresas de TI tem sido o trabalho remoto, mas como sei que muitas pessoas ainda não tem experiência nisso, a ideia desse texto é apresentar algumas dicas, que funcionam pra mim.\nEu tenho aproximadamente 4 anos trabalhando remoto, tanto para empresas que eu visitava 1 vez por mês, como empresas que nunca encontrei nenhum dos meus colegas pessoalmente.\nAs dicas tem como objetivo deixar seu trabalho mais efetivo, com menos interrupção e saudável.\n1. Você precisa de um grande acordo em casa, com criança e tudo. Antes de qualquer coisa. Você precisa conversar com todas pessoas que moram em sua casa sobre seu novo modelo de trabalho. Você precisa estabelecer um acordo com todos eles. Deixar MUITO claro que você está trabalhando e não de férias.\nO meu acordo é o seguinte: Quando eu estiver no meu escritório, estou trabalhando, e quando estou com fone de ouvido, estou em reunião e não posso ser interrompido de forma alguma. Qualquer pedido que não seja urgente deve ser enviado por mensagem.\nAs pessoas vão lhe interromper bastante no começo, mas você precisa reforçar isso para que seu trabalho não seja afetado. A cada interrupção, reforce o acordo que você fez anteriormente e peça para que lhe envie uma mensagem. Você olhará essa mensagem em algum momento.\n2. Lembra do acordo? Se você tiver criança pequena será bem dificil A criança demanda atenção, e a ideia do acordo será dificil com ela, pois a criança sente saudade e quer bastante atenção. Dessa forma já dedique algumas pausas para estar 100% com ela, brincando mesmo, mas antes de sair fale claramente com ela que você precisa trabalhar, quando você parar ele não poderá brincar com você até a próxima pausa.\nSerá bem dificil, mas você precisa continuar tentando, sempre sendo honesto com ela e dizer que precisa trabalhar. Comigo funciona 70% das vezes. Quando ele insiste normalmente o motivo é a falta de pausa para brincar.\nCom todas essas pausas você acabará trabalhando um pouco mais depois, mas se seu trabalho permitir você organizar suas tarefas, isso não será um problema.\nPior mesmo é ter a criança gritando do seu lado implorando atenção quando você precisa executar aquela atividade importante no trabalho.\nUma dica de ouro: NUNCA grite com a criança. Eu sei que é tentador usar a autoridade e fazer a criança ficar com medo de você. Acredite, isso não é bom para relação de vocês. Use sempre um carinho e a palavra honesta, uma hora funciona.\nPS: Eu já perdi a paciência algumas vezes, mas sempre peço desculpas e tento não perder mais.\n3. Você precisa de um escritório Quando falo escritório, não precisa ser uma sala tradicional de escritório. A ideia é que seja um lugar onde as pessoas entendam que você está trabalhando e não faça barulho próximo e nem fiquem passando atrás de você enquanto tiver em reunião por videoconferência.\nO ideal é verificar se esse local tem um bom acesso a internet, eu prefiro rede cabeada, mas se não puder, verifique se seu sinal wifi funciona bem. Teste uma videoconferência com um colega para validar se a velocidade está boa.\n4. Antes de trabalhar garanta que tem tudo a mão É realmente muito ruim lembrar a cada momento que falta algo para você trabalhar (ex. celular para autenticação de multiplos fatores, mouse e afins)\nOrganize sua mesa. Não caia na tentação de deixar tudo bagunçado, pois isso normalmente será um problema quando precisar de algo.\n5. Fique atento a suas pausas para comer Organize alguns alarmes para não perder os momentos de comer, eu faço da seguinte forma:\n Acordo pela manhã e não começo o trabalho antes de tomar café Tenho uma pausa por volta das 10:00 para um lanche Paro por volta das 12:00 para almoçar Paro novamente as 15:00 para um novo lanche rápido Paro por fim por volta das 19:00 para jantar e normalmente não volto mais para o trabalho.  6. Equipamentos Se você puder investir em alguns equipamentos, faça, pois vale a pena.\nSegue a minha lista preferida:\n Um bom mouse (Eu gosto da marca Logitech) Um bom teclado (Se seu equipamento for da Apple, o teclado original é caro, mas vale a pena) Um equipamento wifi especifico para trabalho (O da sua casa normalmente não dará conta) Um bom fone de ouvido com microfone (O meu atual é Edifier, mas ja tive um USB da Microsoft que também era muito bom) Um monitor extra (Trabalhar com duas telas é muito bom) Uma boa cadeira (Isso aqui talvez será a compra mais cara da lista, mas vale a pena)  Meus equipamentos são:\nMouse: Logitech M170 Teclado: Magic Keyboard (Apple) Wifi: Intelbrás Twibi Giga Fone: Edifier com microfone (Antigo, nem acho mais a referência) Monitor: AOC LED 27\u0026quot; Cadeira: NORDI cadeira executiva alta\n7. Foco O trabalho em casa exige um foco diferente do escritório da empresa, pois existem diversas distrações dentro da sua casa que podem drenar sua produtividade, sendo assim tente estabelecer alguns acordos mentais consigo para evitar isso.\nFaça uma lista do que pretende entregar naquele dia e sempre que ficar \u0026ldquo;sem nada pra fazer\u0026rdquo; visite a lista para pegar uma nova tarefa. Essa lista pode sobrar tarefas para outro dia, mas evite colocar uma lista grande demais, para não causar angustia.\n8. Comunicação O trabalho em casa exige uma comunicação diferente do escritório da empresa, pois as pessoas não estão vendo o que está fazendo, sendo assim você precisa deixar MUITO claro os passos do seu trabalho. Assim que iniciar uma tarefa nova, comunique ao time de alguma forma, e faça o mesmo ao encontrar algum bloqueio, demonstre tudo que fez pra tentar ultrapassar o bloqueio e fale de forma clara e direta o que você acredita que deve ser feito para retirar o bloqueio.\nA comunicação online é diferente. Evite ironia quando o papo é sério. Evite também rodeios para pedir algo. Seja claro e objetivo, sem ser rude, claro.\nLembre-se que a pessoa do outro lado não está esperando você pedir algo, ela também tem o ritmo próprio dela, sendo assim se organize para pedir algo as pessoas com alguma antecedência, para dar o tempo da pessoa responder. Evite ficar cobrando a cada x minutos, e se precisar cobrar, faça com alguma empatia e cuidado.\nUma coisa muito importante a salientar é que na comunicação virtual, a tendência é a pessoa do outro lado entender da forma mais rude o que foi escrito, sendo assim use os emoticons da forma correta, coloque alguns quando quiser demonstrar um determinado sentimento. Para deixar claro pra outra pessoa, ou se quiser, pode escrever mesmo. Deixa eu passar um exemplo:\nVocê precisa que uma pessoa lhe adicione em um determinado repositório git para que possa continuar o trabalho.\n\u0026ldquo;Você: Olá, tudo bem? Como vai você? (espere um pouco e veja se ela responde rápido, caso contrário, você pode engatar com o seu pedido) Você: Eu acabei de perceber que a atividade que preciso executar demanda de um acesso ao repositório X. Eu perguntei a algumas pessoas e me falaram que você poderia me ajudar. É isso mesmo? Desculpa o incomodo, mas não sabia que precisaria desse acesso antes. Qual o procedimento para liberação desse acesso?\u0026rdquo;\nNesse momento, estou bloqueado com relação a isso, o que faço? Pego uma outra tarefa caso a pessoa demore de responder. Depois de 1-2h de espera, eu posso interagir novamente.\n\u0026ldquo;Você: Olá, tudo bem de novo? Desculpa incomodar mais uma vez, mas acabei de ver aqui e de fato parece que você é a única pessoa que pode me ajudar nisso. Tem algo que eu possa fazer pra lhe ajudar e conseguir um tempo pra me adicionar nesse repositório? Novamente, desculpa qualquer coisa\u0026rdquo;\nFique atento que se houver um site com todos procedimentos de solicitação, você precisa ler o site e ver se o procedimento não está lá, ok? Pior do que ser requisitado quando está muito ocupado, é ser chamado para algo que a pessoa do outro lado bastava ler um documento.\nNormalmente esse tipo de interação funciona bem. E quando não funcionar, você pode falar com seu líder ou algo do tipo, mas nunca apontando o dedo. Sempre dando o benefício da dúvida, afinal a pessoa que você solicitou de fato pode está muito ocupada.\n9. Atualize seu status Algumas ferramentas de chat permitem que você adicione seu status, ou seja, se fizer uma pausa coloque que está em pausa. Normalmente algumas pessoa usam o termo AFK (Away from keyboard - Longe do teclado), sendo assim a pessoa que está do outro lado sabe que se pedir algo nesse momento demorará um pouco mais de ser respondido.\nÉ interessante alinhar com seu time os status e o que esperar de cada um deles, ok?\nConclusão O trabalho em casa é um pouco diferente do trabalho presencial, mas completamente possível, caso você mude um pouco sua postura. Ele agregará muito na sua produtividade.\nComente aqui se você tem dicas novas ou ponto de vista diferente das dicas que passei aqui.\n","permalink":"https://demo.gethugothemes.com/liva/examplesite/blog/dicas_remoto/","tags":["portugues","remoto","coronavirus","dicas"],"title":"Em casa por conta do Coronavírus? Segue algumas dicas para trabalhar remoto"},{"categories":["devops","mac","english"],"contents":"Starting the year, I decided to do a backup and format my Macbook, restart from scratch. I am an ops guy. My setup is based on tools to create and manage automated infrastructure, SaaS services, and containers.\nI started a twitter thread (Portuguese only) to get some good options.\nI created this article to document the setup to myself and share it with the community to receive more feedback too. Please share your opinions.\n .1. Using brew .2. Applications outside brew  .2.1. Amphetamine .2.2. Oh My ZSH   .3. Generic apps (Using Brew)  .3.1. Firefox and Chrome .3.2. Iterm2 .3.3. Slack .3.4. Spectacle .3.5. Telegram .3.6. Whatsapp .3.7. Transmission .3.8. Spotify .3.9. Flycut   .4. Automation apps (With Brew)  .4.1. Visual Studio Code .4.2. JQ .4.3. Nmap .4.4. Watch .4.5. Docker .4.6. Docker Compose .4.7. Kubernetes-cli .4.8. Popeye .4.9. Stern .4.10. Kubectx + Kubens .4.11. Insomnia .4.12. Dash   .5. Thanks  .1. Using brew Everybody that uses Mac should use brew to install your packages, if you don\u0026rsquo;t use it, please consider this as my first recommendation of this article.\nThe brew is a tool to install packages on your Macbook using a CLI command brew install package_name\nBrew has an outstanding feature to install all packages that you need using just one command and one file with the complete list of apps.\nTo use it you need to create a Brewfile with the list following this convention:\ntap \u0026quot;homebrew/bundle\u0026quot; tap \u0026quot;homebrew/cask\u0026quot; tap \u0026quot;homebrew/core\u0026quot; cask \u0026quot;alfred\u0026quot; brew \u0026quot;jq\u0026quot; To understand the Brewfile: tap is the repository to get the app, cask is to install this app using cask and brew using brew. Please read this document (https://apple.stackexchange.com/questions/125468/what-is-the-difference-between-brew-and-brew-cask) to understand the difference between brew and brew cask.\nWhen you have this file, you need to run this command:\nbrew bundle install Here is my Brewfile.\n.2. Applications outside brew You can\u0026rsquo;t find some apps at brew, and because of it, I will explain first the apps that you should install using other methods.\n.2.1. Amphetamine  Website: https://apps.apple.com/us/app/amphetamine/id937984704?mt=12 Price: Free  The simple app used to keep your Mac awake on a specific time frame. This is really useful on presentations, can you imagine your Mac hibernate in the middle of your talk because you didn\u0026rsquo;t use your power supply? Amphetamine can save you.\n.2.2. Oh My ZSH  Website: https://ohmyz.sh/ Price: Opensource  Do you use a zsh shell? If yes, you should consider this community-driven framework for managing your zsh configuration.\n.3. Generic apps (Using Brew) I will separate the details of apps (using brew) in two categories:\nGeneric: Apps that everyone could install. These are not related to my role. Automation: Apps related to my role (Infrastructure Automation).\n.3.1. Firefox and Chrome  Website: https://www.google.com/intl/pt-BR/chrome/ e https://www.mozilla.org/pt-BR/firefox/new/ Price: Opensource (Firefox) and Free (Chrome)  I don\u0026rsquo;t need to explain these. IMHO it would be best if you had both because you need to troubleshoot problems of web apps in a particular browser.\n.3.2. Iterm2  Website: https://iterm2.com/ Price: Opensource  This terminal is impressive. So many features and it is more beautiful than the regular Mac terminal too.\n.3.3. Slack  Website: https://slack.com/ Price: Free  This is used by all companies that I already worked, and we can\u0026rsquo;t avoid to install it. This app has relevant problems to consume CPU and RAM memory, but we need to have it.\n.3.4. Spectacle  Website: https://www.spectacleapp.com/ Price: Free  You can find useful shortcuts to manage your app windows. Because of it, you need to install another tool to give you options. The Spectacle app is a good option. This app is no longer being actively maintained, but they are effortless. I am still using it.\nMy best shortcut is control + CMD + F to force my apps\u0026rsquo;s windows to use all the space available on the screen and don\u0026rsquo;t enter in \u0026ldquo;full screen mode\u0026rdquo;.\nIf you wanna try another option, you may install Rectangle.\n.3.5. Telegram  Website: https://telegram.org/ Price: Opensource  IMHO the best chat app ever, simple, clean, and enjoyable features (poor security, I know). This tool is really used for the Brazillian IT community (i.e., Dockerbr group has ~5k members).\n.3.6. Whatsapp  Website: https://www.whatsapp.com/ Price: Free  This app is viral in Brazil, most people use this, and we can\u0026rsquo;t avoid it to talk with families and friends outside the \u0026ldquo;tech world\u0026rdquo;.\n.3.7. Transmission  Website: https://transmissionbt.com/ Price: Opensource  IMHO the most straightforward BitTorrent client for Mac.\n.3.8. Spotify  Website: https://www.spotify.com/br/ Price: Free with limitations  \u0026ldquo;Spotify is a digital music service that gives you access to millions of songs.\u0026rdquo; I love songs, if you like it too, you may install and pay for it.\n.3.9. Flycut  Website: https://apps.apple.com/br/app/flycut-clipboard-manager/id442160987?mt=12 Price: Free  \u0026ldquo;Flycut is a clean and simple clipboard manager for developers. Every time you copy code pieces Flycut store it in history. Later you can past it using Shift-Command-V even if you have something different in your current clipboard. You can change hotkey and other settings in preferences.\u0026rdquo;\n.4. Automation apps (With Brew) .4.1. Visual Studio Code  Website: https://code.visualstudio.com/ Price: Opensource  \u0026ldquo;Visual Studio Code is a code editor redefined and optimized for building and debugging modern web and cloud applications.\u0026rdquo; Microsoft won this \u0026ldquo;race\u0026rdquo;. IMHO VScode is the better code editor.\n.4.2. JQ  Website: https://stedolan.github.io/jq/ Price: Opensource  If you need to handle data on JSON format, this tool may help you. JQ is considered the sed of JSON documents. You can learn more about this tool here.\n.4.3. Nmap  Website: https://nmap.org/ Price: Opensource  If you need to check open ports and other network discoveries, you should consider to install it. You can install plugins to extend the tool, and they have a vast community with a lot of documents and new ideas of usage too.\n.4.4. Watch  Website: https://en.wikipedia.org/wiki/Watch_(Unix) Price: Opensource  If you need to run the same command continuously to refresh the data (.i.e., ps to check process), you should use the watch to don\u0026rsquo;t add too many entries on your shell history and keep it updating until you are free to check something else.\n.4.5. Docker  Website: https://docs.docker.com/docker-for-mac/ Price: Opensource  If you use containers these days, probably you should install this app. This app is Docker for Mac and provides you a virtual machine to build/use your Linux docker images on your Mac.\nI use docker to run other binaries and avoid the necessity to install it. For example:\ndocker run -it -v $PWD:/app -w /app --entrypoint=\u0026quot;\u0026quot; terraform:light sh I can use the terraform, and I didn\u0026rsquo;t install it on my Mac, and I can specify the version of terraform too.\n.4.6. Docker Compose  Website: https://docs.docker.com/compose/ Price: Opensource  This tool is handy to set up a sophisticated container set up locally. If you work with containers, you should install it.\n.4.7. Kubernetes-cli  Website: https://kubernetes.io/docs/tasks/tools/install-kubectl/#install-kubectl-on-macos Price: Opensource  If you work with Kubernetes, you need this tool. This is the CLI binary to interact with your Kubernetes cluster.\n.4.8. Popeye  Website: https://github.com/derailed/popeye Price: Opensource  If you use Kubernetes, this tool can help you a lot. You can use it to scan your Kubernetes cluster, and popeye will give you a report with possible problems on your resources and configuration.\n.4.9. Stern  Website: https://github.com/wercker/stern Price: Opensource  Do you need to troubleshoot Kubernetes pods? This tool can help you to \u0026ldquo;tail\u0026rdquo; multiple pod\u0026rsquo;s logs of a Kubernetes cluster.\n.4.10. Kubectx + Kubens  Website: https://github.com/ahmetb/kubectx Price: Opensource  Do you use multiple clusters/namespaces Kubernetes? This tool can help you to switch cluster and namespace smoothly.\n.4.11. Insomnia  Website: https://insomnia.rest/ Price: Opensource  Do you need to test REST or GraphQL API? This straightforward tool can help with that. You may use postman, this is an alternate option.\n.4.12. Dash  Website: https://kapeli.com/dash Price: Free with limitations  Dash is an API Documentation Browser and Code Snippet Manager. The best thing is that you can search offline too.\n.5. Thanks  Everyone here: thread. @somatorio @jjunior0x2A @badtux_ @@jabezerra @malaquiasdev  ","permalink":"https://demo.gethugothemes.com/liva/examplesite/blog/o_que_instalar_mac_infra_en/","tags":["english","devops","mac","brew","saas"],"title":"What I should install on my Mac?"},{"categories":["devops","mac"],"contents":"Pra começar bem o ano, resolvi fazer um backup e formatar meu Mac, reiniciar do zero, pois o meu notebook tem apenas 128GB de espaço em disco e ultimamente tenho brigado por cada último byte livre no HD.\nAssim que acabei de formatar, lembrei o motivo de tanto retardo: A necessidade de reinstalar tudo que preciso do zero.\nFiz uma postagem no Twitter solicitando ajuda e prometi criar um artigo, e aqui está.\n .1. Usando o Brew .2. Aplicativos fora do Brew  .2.1. Dash .2.2. Amphetamine .2.3. Oh My ZSH   .3. Aplicativos genéricos para se usar no Mac (Com Brew)  .3.1. Alfred .3.2. Firefox e Chrome .3.3. Grammarly .3.4. Iterm2 .3.5. Slack .3.6. Spectacle .3.7. Telegram .3.8. Vanilla .3.9. Whatsapp .3.10. GPG Suite .3.11. Keybase .3.12. Transmission .3.13. Spotify .3.14. Flycut   .4. Aplicativos para desenvolvedores (Com Brew)  .4.1. Visual Studio Code .4.2. Kubernetes-cli .4.3. JQ .4.4. Nmap .4.5. Openssl .4.6. Watch .4.7. Docker .4.8. Popeye .4.9. Stern .4.10. Kubectx + Kubens .4.11. Insomnia .4.12. Docker Compose   .5. Agradecimentos  .1. Usando o Brew Todo mundo que tem Mac e usa ele para trabalhar com programação/devops/automação/afins já deve conhecer o brew. A instalação a é simples e pode ser vista aqui.\nO que poucas pessoas sabem é sobre a existência do brew bundle (Aqui um link para saber mais) basicamente ele faz o mesmo que o bundle do ruby faz. Reuni tudo que vc precisa instalar em um arquivo, que aqui é chamado de Brewfile onde você informa linha a linha, dizendo se é brew on cask para instalar os pacotes.\nAo final do arquivo pronto, basta um simples comando:\nbrew bundle install Ele instala tudo pra você. Segue meu Brewfile\n.2. Aplicativos fora do Brew Vou começar com os aplicativos que não estão no Brew, pois esses você precisará entrar no site e instalar manualmente mesmo. Seguindo a documentação de cada um deles.\n.2.1. Dash  Site: https://kapeli.com/dash Valor: Grátis com limitações  É uma ferramenta muito interessante de documentação, tudo a partir de um campo de busca centralizado. A melhor parte dele é a possibilidade de pesquisar offline.\n.2.2. Amphetamine  Site: https://apps.apple.com/us/app/amphetamine/id937984704?mt=12 Valor: Grátis  Um aplicativo simples que pode ser usado para manter seu Mac ligado por um espaço específico de tempo. É ótimo para quem faz apresentações sem carregador e não quer que o Mac desligue no meio de apresentação, mas quer quer que esse comportamento volte depois da palestra.\n.2.3. Oh My ZSH  Site: https://ohmyz.sh/ Valor: Opensource  É um framework para gerenciamento de seu console Zsh, onde deixa ele mais bonito e bem mais produtivo, com vários atalhos e funcionalidades novas.\n.3. Aplicativos genéricos para se usar no Mac (Com Brew) Vou passar rapidamente aplicativo por aplicativo para defender porque escolhi ele:\n.3.1. Alfred  Site: https://www.alfredapp.com/ Valor: Grátis, com exceção do Powerpack  Esse é uma aposta, pois todo mundo fala, mas até então não tinha testado ainda. Tenho pouco a falar dele, que tem como objetivo substituir o spotlight (Aquela busca do CMD+Barra de espaço). Dizem que ele tem mais funcionalidades, é mais rápido e afins.\nEu ja tive muitos problemas com o spotlight, dai estou apostando no Alfred.\n.3.2. Firefox e Chrome  Site: https://www.google.com/intl/pt-BR/chrome/ e https://www.mozilla.org/pt-BR/firefox/new/ Valor: Opensource (Firefox) e Grátis (Chrome)  Outra opção meio padrão para maioria das pessoas, pois o uso do navegador Safari é bem baixa ainda. Eu tenho maior uso do Chrome, pois ele tem uma funcionalidade de múltiplos perfis de conta por janela de navegador. Dai eu consigo gerenciar meu perfil pessoal e profissional alternando as janelas.\n.3.3. Grammarly  Site: https://www.grammarly.com/ Valor: Grátis com limites (Versão paga no plano anual sai USD$ 11,66 por mês)  Se você não é tão bom escrita de inglês quanto eu, e precisar escrever coisas com algumas constância, você deveria considerar um app como esse. Eu adoro o Grammarly, pois ele não verificar apenas erro de ortografia, ele verifica concordância e outros aspectos que um corretor comum não dá conta. Ele faz sugestões quando você repete demais a palavra e afins. Funciona bem. Eu aconselho.\n.3.4. Iterm2  Site: https://iterm2.com/ Valor: Opensource  Aqui temos outra unanimidade, pois a maioria esmagadora das pessoas que conhecem o Iterm2, acabam usando ele como padrão e esquecem o terminal padrão do Mac por completo.\n.3.5. Slack  Site: https://slack.com/intl/pt-br/ Valor: Grátis  A maioria esmagadora das empresas que já trabalhei usam o Slack como ferramente de comunicação interna, sendo assim fica difícil não instalar esse app. Ele consome MUITA memória, mas necessidade não se discute, né?\n.3.6. Spectacle  Site: https://www.spectacleapp.com/ Valor: Grátis  O Mac não tem teclas de atalho para gerenciamento da posição das janelas. As vezes você quer colocar a janela ocupando toda tela, mas não quer tela cheia, pois o Mac tem um comportamento diferente e deixa todas as outras telas em preto, ou seja, não da pra ficar no Mac sem um software que ofereça opções para isso. O Spectacle é uma boa opção. Com o atalho control + CMD + F você faz com que a janela do software atual ocupe toda tela.\nAo fazer esse post, descobri que ele foi descontinuado e sugere a utilização do Rectangle\nVale a pena olhar esse novo. Vou continuar com o Spectacle, pois ele me atende tranquilamente.\n.3.7. Telegram  Site: https://telegram.org/ Valor: Opensource  Uma ótima ferramente de troca de mensagens. No Brasil a maioria das comunidades técnicas estão nesse software de bate papo. A prova disso é esse extensa lista de grupos no telegram: https://listatelegram.github.io/\nEle é praticamente igual ao Whatsapp, só que com mais funcionalidades, e sem necessidade de celular ligado para acessar versão desktop/web, por exemplo.\n.3.8. Vanilla  Site: https://matthewpalmer.net/vanilla/ Valor: Grátis com limites  Se você instala uma grande lista de apps e isso lota sua barra de notificação (Aquela que fica ao lado do relógio), esse app é pra você, pois ele oculta e mostra os apps com apenas um clique.\nSimples e direto. Gostei.\n.3.9. Whatsapp  Site: https://www.whatsapp.com/ Valor: Grátis  É praticamente um padrão de comunicação entre as pessoas no Brasil. Por mais que não curto esse mensageiro, sou obrigado a usá-lo por conta disso.\n.3.10. GPG Suite  Site: https://gpgtools.org/ Valor: Opensource  Se você encripta seus emails, arquivos e afins, acredito que esse app é essencial pra você.\n.3.11. Keybase  Site: https://keybase.io/ Valor: Opensource  Esse é uma outra aposta. Muita gente fala bem dele, mas confesso que ainda não usei de verdade. A ideia dele é oferecer uma plataforma segura para comunicação, troca de arquivos e afins.\n.3.12. Transmission  Site: https://transmissionbt.com/ Valor: Opensource  Um cliente para baixar arquivos via BitTorrent. Muito leve e bom.\n.3.13. Spotify  Site: https://www.spotify.com/br/ Valor: Grátis com limites  Um cliente para uma plataforma de músicas online, que também oferece opção para Podcasts. Muito usado por boa parte das pessoas.\n.3.14. Flycut  Site: https://apps.apple.com/br/app/flycut-clipboard-manager/id442160987?mt=12 Valor: Grátis  Outra aposta. O objetivo dessa ferramenta é oferecer uma opção mais potente de gerenciamento de área de transferência (O que você pega no CMD+C ).\n.4. Aplicativos para desenvolvedores (Com Brew) .4.1. Visual Studio Code  Site: https://code.visualstudio.com/ Valor: Opensource  Esse editor de código tem crescido cada vez mais entre as pessoas que trabalham com infra/devops/automação. Acredito que por conta dos plugins/extensões que aumentam bastante a sua produtividade.\nPrometo que depois faço um artigo apenas sobre as extensões do meu Visual Studio Code.\n.4.2. Kubernetes-cli  Site: https://kubernetes.io/docs/tasks/tools/install-kubectl/#install-kubectl-on-macos Valor: Opensource  Se você trabalha com kubernetes de alguma forma, essa ferramenta é necessária. Ela é usada para se comunicar com o cluster kubernetes.\n.4.3. JQ  Site: https://stedolan.github.io/jq/ Valor: Opensource  Essa ferramenta é considerada o sed para formato JSON. É muito potente na coleta e manipulação de dados que contenham o formato JSON.\nSe quiser aprender um pouco como usar, veja esse link\n.4.4. Nmap  Site: https://nmap.org/ Valor: Opensource  Ferramenta potente e extremamente extensível para descoberta e auditoria de segurança de rede. Muito bom para descobrir quais portas estão abertas no seu servidor e afins.\n.4.5. Openssl  Site: https://www.openssl.org/ Valor: Opensource  Ferramenta necessária para gerenciamento e criação de certificados SSL/TLS.\n.4.6. Watch  Site: https://en.wikipedia.org/wiki/Watch_(Unix) Valor: Opensource  Ferramente que é usada para mostrar repetidamente a mesma tela, atualizando automaticamente periodicamente com base em um espaço de tempo que você estipulou.\nExemplo:\nVocê quer saber se seu processo do PHP subiu corretamente, ou quantos processo tem aparecido em um espaço de tempo. Você pode usar esse comando: watch \u0026quot;ps -e | grep php\u0026quot;\nPor padrão o tempo de atualização é de 2 segundos, sendo assim ele executará o comando ps -e | grep php e mostrará na sua tela a saída do mesmo. Sendo assim você não precisa ficar dando o mesmo comando várias vezes, \u0026ldquo;sujando\u0026rdquo; seu histórico de comandos com vários comandos repetidos sucessivamente.\n.4.7. Docker  Site: https://docs.docker.com/docker-for-mac/ Valor: Opensource  Esse eu preciso de pouco pra defender, correto? Ele é basicamente hoje a base da maioria das pessoas que trabalham com automação de infraestrutura. Eu mesmo uso ele para praticamente tudo. Eu não instalo o aws-cli, terraform, packer e afins, pois eu uso um container com esses binários dentro a partir do docker. Por exemplo:\ndocker run -it -v $PWD:/app -w /app --entrypoint=\u0026quot;\u0026quot; terraform:light sh Com esse comando eu tenho uma cli com terraform mais atualizado instalado com sucesso.\n.4.8. Popeye  Site: https://github.com/derailed/popeye Valor: Opensource  Ferramenta interessante para scan de cluster kubernetes, que reporta possíveis problemas em recursos \u0026ldquo;deployados\u0026rdquo; e configurações. É uma aposta, dica do @badtux_.\n.4.9. Stern  Site: https://github.com/wercker/stern Valor: Opensource  Mais uma ferramente sugerida pelo @badtux_ para ajudar na gestão de logs de múltiplos pods em um cluster kubernetes.\n.4.10. Kubectx + Kubens  Site: https://github.com/ahmetb/kubectx Valor: Opensource  De novo o @badtux_ com sugestões massas para ajudar na gestão do kubernetes. Essas oferecem funcionalidades de rápida troca de namespace e cluster kubernetes . Outra aposta. Valeu @badtux_.\n.4.11. Insomnia  Site: https://insomnia.rest/ Valor: Opensource  Esse foi amplamente pedido por várias pessoas em diversos canais (Obrigado @@jabezerra e @malaquiasdev por serem os primeiros). Resolvi testar e gostei. Está na lista, como uma boa aposta.\n.4.12. Docker Compose  Site: https://docs.docker.com/compose/ Valor: Opensource  Não menos importante, mas eu sempre esqueço dele e só vou instalar depois que preciso. Essa ferramente é responsável por gerenciar a execução de múltiplos containers com apenas um comando e um arquivo de configuração. É uma ótima ferramenta para iniciar infraestrutura complexa localmente na máquina de uma pessoa que desenvolve o produto.\n.5. Agradecimentos  Obrigado a todo mundo que ajudou nessa thread. Obrigado a @somatorio que sempre revisa meus trabalhos ;) \u0026lt;3 Obrigado a @jjunior0x2A que deu uns pitacos também. Obrigado a @badtux_ que colocou bastante coisa de Kubernetes na lista. Obrigado a @@jabezerra e @malaquiasdev pelo lembrete do Insomnia.  ","permalink":"https://demo.gethugothemes.com/liva/examplesite/blog/o_que_instalar_mac_infra/","tags":["devops","mac","brew","saas"],"title":"O que instalar em um Mac de alguém de infra"},{"categories":["devops"],"contents":"Fui surpreendido essa semana com uma matéria, de uma revista super conceituada, sobre \u0026ldquo;9 segredos obscuros sobre DevOps que você precisa saber\u0026rdquo;.\nMe marcaram no twitter, pedindo minha opinião, e como minha opinião era bem longa, e precisava de um contexto, resolvi escrever um artigo aqui.\nEu não quero de forma alguma desrespeitar quem escreveu a materia, e nem criar nenhum constrangimento para a revista em sí, mas eu preciso me posicionar com relação a isso, pois o contéudo da matéria propaga uma senso comum que é bem ruim para quem trabalha com isso, e tem que lidar com informações desencontradas no mercado.\n1. DevOps não programa Aqui começa o desserviço desse artigo, pois a todo momento a comunidade da cultura DevOps tem tentado convencer as pessoas que migram para cargos dessa cultura DevOps, sobre a necessidade do conhecimento sobre desenvolvimento, ou seja, sobre programar de fato.\nNão caia nessa armadilha. É muito cômodo se fechar em uma posição confortável e apontar que seu cargo não envolve desenvolvimento, pois não é desenvolvedor, mas lhe digo que a linha entre as responsabilidades e possibilidades são bem diferentes.\nUm \u0026ldquo;Sysadmin\u0026rdquo; que entende de fato o processo de desenvolvimento, consegue olhar para um código e apontar um problema, que pode impactar na infraestrutura agrega muito mais para o negócio do que aquela pessoa que antigamente era considerada apenas como custo operacional, para suportar a aplicação no ar, onde na maioria esmagadora dos casos a medida do sucesso era apenas SLA (Service Level Agreement).\nNão estou aqui defendendo que você seja responsável por enviar códigos e nem ser revisor de fato de código alheio, mas a possibilidade de fazer isso lhe coloca em outro patamar enquanto profissional.\n2. Supervisão de outros programadores Nesse \u0026ldquo;item\u0026rdquo; ele inicia citando a falta de necessidade de o \u0026ldquo;DevOps\u0026rdquo; em não programar, mas mesmo assim era o seu papel supervisionar o trabalho de envio de código. Isso é uma contradição, pois como alguém seria responsável por dizer o que entra ou não no \u0026ldquo;container\u0026rdquo; (como se isso fosse uma caixa, e o \u0026ldquo;DevOps\u0026rdquo; aqui seria o segurança portão que permite entrada ou não).\nO fato é: Ninguém sozinho deveria ser responsável por dizer o que entra ou não \u0026ldquo;no container\u0026rdquo;, mas o ideal é que o time entenda as demandas, na escrita do código, faça uso do melhor algorítimo para escrita da lógica ideal, afim de atender a demanda do negócio para aquele código que se prentende ser colocado em produção. Falo pretende, porquê o esperado é que exista um processo de revisão de código, que pode ser feito por outros desenvolvedores, QA (Quality Assurance) ou até mesmo a pessoa responsável pela automação de infra (\u0026ldquo;DevOps\u0026rdquo;?).\nNão queira ser o gargalo do time, não almeje esse poder, pois isso é também uma prisão, onde você raramente poderá tirar férias, descansar no final de semana, estar com seu filho em algumas noites no meio da semana.\nPermita que seu time sobreviva sem você, pois isso pode significar sua hora extra na empresa. Isso não é apenas garantia de trabalho (não ser demitido), isso é quase a certeza que qualquer movimento do time, independentemente se envolver mudanças na infra ou não, você precisará ser envolvido.\n3. DevOps está assumindo o controle Eu acho falsa a premissa que os desenvolvedores tinha o \u0026ldquo;controle\u0026rdquo; antes do microsserviço. Essa inclusive é uma das dores primordiais que estimulou a criação da cultura DevOps. \u0026ldquo;O controle\u0026rdquo; na maioria das empresas esteva na mão dos Sysadmin, que era o time responsável pelo que é colocado em produção.\nA cultura DevOps não estimula que esse \u0026ldquo;controle\u0026rdquo; mude de lado e agora um outro time assuma essa responsabilidade. A premissa é colaboração, ou seja, o \u0026ldquo;controle\u0026rdquo; foi distribuído em partes menores e agora todos são corresponsáveis.\nSe você pretende trabalhar com cultura DevOps, toda vez que seus movimentos tiverem como objetivo concentrar em você o \u0026ldquo;controle\u0026rdquo;, você além de estar desvirtuando a idéia inicial da cultura, você estará arrumando um grande problema pra ti, e terá pouco tempo pra fazer as coisas que as empresas normalmente esperarão de você (Ex. Monitoramento de negócio, melhorias na arquitetura e afins).\n4. Redução de custos Isso até poderia ser algo que não mereceria nenhuma crítica a matéria, ao menos que no mesmo texto não fosse mencionado o baixo envolvimento do \u0026ldquo;devops\u0026rdquo; com o código, ou seja, não é possível ser efetivo na redução de custos quando seu envolvimento com código é baixo. Para reduzir custos, de verdade, é necessário um envolvimento que alguém que está distante do código raramente terá.\nÉ possível reduzir custos olhando só pra infraestrutura? Claro que sim, mas a real redução demanda de um envolvimento com o código. Não tem pra onde fugir.\n5. Aumento de desempenho Estaria aqui a possível redenção do artigo, mas infelizmente falha no alvo. Poderia entrar aqui o papel desse profissional \u0026ldquo;devops\u0026rdquo; no auxilio ao time de desenvolvedores sobre a perspectiva de infra no uso dos recursos de cloud, que tem bastante possibilidades de composições que impactam muito no custo e desempenho.\nO profissional \u0026ldquo;devops\u0026rdquo; aqui atuaria como consultor interno, junto ao time de desenvolvimento, para que a melhor solução fosse adotada, analisando a demanda na perspectiva de infraestrutura, desenvolvimento, qualidade, negócio, produtividade, segurança e afins.\n6. Demolição O \u0026ldquo;encobrindo\u0026rdquo; citado na matéria só se torna um problema em ambientes que atuam no paradigma anterior, onde o monitoramento lida apenas com máquinas, onde o log raramente tem tags ou é centralizado, tudo precisa ser analisando em console.\nSe um container reiniciou, qual problema com isso? Não tem log? Monitoramento? A ideia não é exatamente essa? Que o container reinicie em caso de problemas?\nA minha impressão é que esse tipo de indagação sobre \u0026ldquo;encobrimento\u0026rdquo; é típico de quem usa container como máquinas, infelizmente.\n7. Bancos de dados Felizmente banco de dados não é mais \u0026ldquo;a única fonte da verdade\u0026rdquo;, temos outras opções, que podem armazenar as interações com dados e até mesmo enviar novamente eventos numa fila (Ex. Kafka).\nA verdade é que Banco de Dados ainda é um grande calo na maioria dos times, de fato.\n8. Como o código está sendo executado Aqui fica evidente o problema citado em outros pontos da matéria. A separação funcional entre os dois times sempre foi um dos maiores problemas que a cultura DevOps pretende resolver, ou seja, não deveria existir isso de \u0026ldquo;Essa questão é deixada para os programadores\u0026rdquo;. Entender os números é uma tarefa do time! Claro que cada um com sua especialidade, mas todos trabalhando juntos para entender e atuar.\nNão deixe que seja criado um muro imaginável entre os times. É uma demanda de todos. Não sabe ler o log, senta com alguém que sabe e aprende, pois isso pode ser importante na perspectiva de infra e você nem se atentou a isso. O inverso também pode acontecer.\n9. Um pouco de mistério Eu entendo o quanto é tentador colocar os incidentes na conta do azar, mas acredite, tudo tem fatores contribuidores para seus problemas. O que pode acontecer é que talvez agora não tenhamos as ferramentas e conhecimentos necessários para descobrir eles, mas que eles existem, não há nenhuma dúvidas sobre isso.\nNão deixe \u0026ldquo;o mistério acontecer\u0026rdquo;, tenha sede pelo entendimento das tecnologias, não apenas nos seus parâmetros, mas no funcionamento mais elemental das mesmas. Evite esse papo de \u0026ldquo;muitas vezes é mais simples seguir em frente\u0026rdquo;, pare um pouco, aprenda com seus incidentes (e dos outros também), pois eles tem muito para lhe ensinar e evitar que novos problemas aconteçam.\n","permalink":"https://demo.gethugothemes.com/liva/examplesite/blog/9mitos_que_voce_deveria_esquecer/","tags":["devops"],"title":"9 mitos que você deveria esquecer"},{"categories":["devops","devopsdays"],"contents":"Esse ultimo fim de semana foi muito intenso pra mim, eu fui aceito para fazer minha primeira palestra internacional, e foi no aniversário de 10 anos do DevOpsDays. A palestra foi em Ghent, onde tudo começou, com as pessoas que começaram a cultura DevOps ali na plateia.\nA minha palestra era sobre o crescimento da comunidade DevOpsDays no Brasil, como saímos de 2 para 15 eventos por ano. Sobre as nossas dificuldades e o que fizemos para lidar com os problemas.\nFalei sobre como nosso o Brasil é grande e como temos diferenças econômicas e sociais entre diferentes regiões. Falei também como é importante focarmos esforços em locais com comunidades menores. Por fim, falei como esses eventos de fato mudam a vida das pessoas.\nPessoas que tinham pouca ou nenhuma perspectivas, que através de exemplos, ajuda com materiais e um pouco de esforço mudaram suas vidas. Não quero aqui fazer nenhuma propagada a meritocracia. Não é isso que quero falar. Quero dizer que DevOps é uma moda, oferece melhores empregos, mas agora felizmente temos eventos, BONS eventos, com conteúdo de alto nível acontecendo perto das pessoas. Elas não precisam pagar absurdos para viajar, hospedagem para ter acesso a esse conteúdo, networking e afins.\nE por isso que contínuo a dizer. A comunidade brasileira, que forma O DevOpsDays Brasil, está sim mudando a vida das pessoas e o melhor, não porque isso é interessante para uma empresa A ou B. É importante para as pessoas. Da comunidade para comunidade.\nAinda no DevOpsDays Ghent, no final da palestra fiz questão de chamar todas as pessoas que organizam DevOpsDays no Brasil, e que estavam em Ghent, a subirem no palco comigo, pois os aplausos não era apenas pra mim, eu não mereço isso sozinho.\nAquilo foi simbólico, mas muito poderoso. Muitas pessoas de outros eventos vieram nos procurar, conversar conosco, aumentar o intercâmbio entre os eventos de diferentes países.\nNunca imaginei que meu nome e do Brasil seriam citados tantas vezes em outras palestras, de pessoas que considero tanto. Eu quase chorei em várias palestras. Foi muito impactante e forte.\nUm filme passou pela minha cabeça. Desde 2015, quando tive a ideia de reviver o DevOpsDays no Brasil, pois ele tinha sido feito pela ultima vez em 2010. Quantas reuniões eu tive, quantas tentativas que se frustaram pelos mais diferentes motivos, mas em 2016 conseguimos fazer em Porto Alegre, onde a comunidade local abraçou vigorosamente o evento e nos deu força para continuar.\nEm 2016 lá estavam Guto Carvalho e Miguel Di Ciurcio no primeiro ano. Patrocinando o evento e até mesmo ajudando na gravação das palestras do DevOpsDays Porto Alegre 2016. Eu digo e repito, Guto Carvalho sempre foi e sempre será o nome por trás do DevOps no Brasil.\nNão posso deixar de agradecer a Guilherme Elias, Diogo Lucas e Somatório, que estavam lá organizando junto comigo esse evento e me apoiando até mesmo em questões pessoais.\nMuita coisa aconteceu comigo naquele ano, nossa, como aconteceu. Eu fiquei devastado, mas continuar era preciso. No ano seguinte indo pra Salvador realizar o evento na minha cidade natal. Nossa, lembro como hoje a alegria de ver tanta gente naquela sala cheia, onde muitos insistiam a dizer que não daria certo (“Ninguém vai a eventos em Salvador, Rafael, tira essa ideia da cabeça, foca em São Paulo”).\nQuem apostaria em uma cidade que não estava comercialmente atrativa em 2017? A CodeOps e LinuxTips de Mateus Prado e Jeferson Noronha, e a K21, que bancou a ida de Wellington pra lá. Vale salientar que Mateus Prado bancou do bolso a ida a tantos DevOpsDays que eu nem sei. Ele é uma pessoa incrível, isso sem falar no profissional absurdo que é. Obrigado de verdade. O DevOpsDays é o que hoje por conta de pessoas como Mateus Prado. Eu nunca vou conseguir retribuir isso.\nTantas coisas aconteceram nesses DevOpsDays, muitos momentos felizes, muitos reencontros, inspirações e momentos de tristeza também. Lembro de eu quase ter chorado na minha palestra em Fortaleza, falando sobre carreira DevOps, chorei mais ainda no ano seguinte ter pessoas me dizendo que minhas palavras ajudaram a conseguir empregos novos e melhores. Nossa! Foram apenas palavras… eu fico grato, de verdade.\nLembro como hoje de estar profundamente cansado em um DevOpsDays, ali no canto da plateia, pois tinha passado por umas semanas difíceis, mas não poderia deixar de estar presente para prestigiar, e meu grande amigo, que já considero quase como anjo, Fernando Ike, chegou em mim e perguntou “Tá tudo bem?” “Tudo vai passar cara, fica tranquilo” Foram só palavras… mas que palavras, hein?\nNão posso deixar de citar o que rolou lá em Ghent, antes da minha palestra, as pessoas percebendo meu nervosismo, se voluntariam para revisar minha palestra (Obrigado Michelle), pra conversamos sobre ela e dizendo que tudo acabaria bem. Foram só palavras… mas que palavras, hein? Valeu também Gleydson\nFoi muito esforço coletivo, muita gente mesmo, muitos laços bacanas e em Ghent foi assim, Brasil no topo, e todo mundo tava lá pra levantar a taça.\nPra coroar tudo que fizemos em Ghent, Somatório foi convidado para fazer parte do Core Team. Ele ficou tão feliz, que sua felicidade me contagiou profudamente. Ele é realmente alguém comprometido com o sucesso do evento no Brasil. Eu diria que ele é como se fosse a ideia do DevOpsDays em pessoa, carne e osso. Comunidade para comunidade. Somatório é uma daquelas pessoas que você tem sorte de conhecer na vida, sabe? Daquelas que nascem a cada 50 anos.\nEu nunca imaginaria que chegaria a esse ponto. Pode parecer que estou fingindo, que tudo isso não passa de uma grande mentira, mas é verdade. Eu nunca, nem mesmo nos meus sonhos mais profundos, imaginei que eu chegaria tão longe, ainda mais da forma que está sendo.\nNão é como se eu não desejasse isso, eu sempre desejei ser aceito, recompensado ou coisa do tipo, mas nunca imaginei dessa forma, de uma forma tão coletiva. De uma forma tão nossa. Não é sobre eu estar sendo reconhecido apenas, é sobre o Brasil está no mapa, está nas falas das pessoas.\nNão é sobre carreira apenas, é sobre construir algo em comunidade, não é sobre dinheiro apenas, mas sim sobre reconhecimento coletivo, sobre se ajudar, é definitivamente sobre cuidar uns dos outros.\nTivemos alguns problemas? Claro que sim, mas estamos aqui pra aprender com nossos erros e melhorar nossas atitudes e posicionamentos.\nPor hora, ainda sigo em êxtase sobre o que aconteceu, pois alguns de vocês estavam lá e isso foi mais importante do que qualquer coisa.\nObrigado a todos, por tudo que fizemos juntos até agora.\n","permalink":"https://demo.gethugothemes.com/liva/examplesite/blog/devopsdays10anos/","tags":["devops","devopsdays","brasil"],"title":"10 anos DevOpsDays e quem recebe o presente sou eu"},{"categories":["english","devops","docker","ruby","node"],"contents":"TL;DR We needed to deploy a ruby+node application as a docker image on Heroku, but I didn\u0026rsquo;t use Heroku cli to build it. This document is about how we did the development, test, build, tag and deployment of a docker image on a Heroku application according to the best practices.\nI will split this article into some parts. In this \u0026ldquo;Part 1\u0026rdquo; I will explain in details how I delivered the first version of a Dockerfile, without Multistage build, to create the docker image and Docker compose file to bring up the whole development environment.\nAttention If you copy and paste some codes shown here without reading the context, it may not work properly on your environment.\nI will present the code, tell the story behind this phase and show how I improved it after seeing the problem.\nSetup To reproduce this article, you need to install these tools:\n Docker Docker compose Make  The environments We have three environments:\n Development: It is only on the developer\u0026rsquo;s machine. Staging: It is for QA analysis on Heroku app Production: It is the real app running on Heroku app  Dockerization for developers first I started this job providing docker environment to developers and getting feedbacks.\nHere is my Dockerfile to developers:\nFROMruby:2.5.1 as builderRUN curl -sL https://deb.nodesource.com/setup_8.x | bash - \u0026amp;\u0026amp;\\  curl -sS https://dl.yarnpkg.com/debian/pubkey.gpg | apt-key add - \u0026amp;\u0026amp;\\  echo \u0026#34;deb https://dl.yarnpkg.com/debian/ stable main\u0026#34; | tee /etc/apt/sources.list.d/yarn.listRUN apt-get update \\  \u0026amp;\u0026amp; apt-get install -y locales \\  graphviz \\  imagemagick \\  postgresql-client-9.6 \\  yarn \\  nodejs \\  \u0026amp;\u0026amp; echo \u0026#34;en_US.UTF-8 UTF-8\u0026#34; \u0026gt; /etc/locale.gen \u0026amp;\u0026amp; /usr/sbin/locale-gen \u0026amp;\u0026amp;\\  rm -rf /var/lib/apt/lists/*ENV GEM_HOME /gems/vendorENV GEM_SPEC_CACHE /gems/specsENV BUNDLE_APP_CONFIG /gems/vendorENV BUNDLE_PATH /gems/vendorENV BUNDLE_BIN /gems/vendor/binENV PATH /app/bin:/gems/vendor/bin:$PATHARG RAILS_ENVENV RAILS_ENV=$RAILS_ENVENV APP_ROOT /appWORKDIR$APP_ROOT/RUN mkdir -p /gemsRUN groupadd -r app \\  \u0026amp;\u0026amp; groupmod -g 1000 app \\  \u0026amp;\u0026amp; useradd -g app -ms /bin/bash app \\  \u0026amp;\u0026amp; chown app $APP_ROOT \\  \u0026amp;\u0026amp; chown app /gemsUSERappCOPY Gemfile.lock $APP_ROOT/COPY Gemfile $APP_ROOT/COPY package-lock.json $APP_ROOT/COPY package.json $APP_ROOT/RUN bundle installRUN yarn installEXPOSE3000CMD [\u0026#34;bundle\u0026#34;,\u0026#34;exec\u0026#34;,\u0026#34;rails\u0026#34;,\u0026#34;server\u0026#34;,\u0026#34;-b\u0026#34;,\u0026#34;0.0.0.0\u0026#34;]This Dockerfile is responsible for setting up everything that we needed to start the development job.\nWe didn\u0026rsquo;t need to copy the code to docker context on the docker image creation phase. We mounted the source code inside the container on runtime:\ndocker build -t ruby_node_app:0.1 . docker run -it --build-arg RAILS_ENV=development -v $PWD:/app ruby_node_app:0.1 The RAILS_ENV variable was used to change the gem installation behavior — some gems we didn\u0026rsquo;t need to install in production.\nDockefile, explained We used this first section to set up the basis of this dev environment:\nFROMruby:2.5.1 as builderRUN curl -sL https://deb.nodesource.com/setup_8.x | bash - \u0026amp;\u0026amp;\\  curl -sS https://dl.yarnpkg.com/debian/pubkey.gpg | apt-key add - \u0026amp;\u0026amp;\\  echo \u0026#34;deb https://dl.yarnpkg.com/debian/ stable main\u0026#34; | tee /etc/apt/sources.list.d/yarn.listRUN apt-get update \\  \u0026amp;\u0026amp; apt-get install -y locales \\  graphviz \\  imagemagick \\  postgresql-client-9.6 \\  yarn \\  nodejs \\  \u0026amp;\u0026amp; echo \u0026#34;en_US.UTF-8 UTF-8\u0026#34; \u0026gt; /etc/locale.gen \u0026amp;\u0026amp; /usr/sbin/locale-gen \u0026amp;\u0026amp;\\  rm -rf /var/lib/apt/lists/*We needed to use a specific folder of gems. We mounted this folder on runtime. It was a dev requirement to troubleshoot gem usage:\nENV GEM_HOME /gems/vendorENV GEM_SPEC_CACHE /gems/specsENV BUNDLE_APP_CONFIG /gems/vendorENV BUNDLE_PATH /gems/vendorENV BUNDLE_BIN /gems/vendor/binWe needed to specify the app folder and create the app user to use root instead:\nENV APP_ROOT /appWORKDIR$APP_ROOT/RUN mkdir -p /gemsRUN groupadd -r app \\  \u0026amp;\u0026amp; groupmod -g 1000 app \\  \u0026amp;\u0026amp; useradd -g app -ms /bin/bash app \\  \u0026amp;\u0026amp; chown app $APP_ROOT \\  \u0026amp;\u0026amp; chown app /gemsUSERappWe needed to install all ruby and node requirements:\nCOPY Gemfile.lock $APP_ROOT/COPY Gemfile $APP_ROOT/COPY package-lock.json $APP_ROOT/COPY package.json $APP_ROOT/RUN bundle installRUN yarn installIn the end, we needed to specify the exposed port and default command:\nEXPOSE3000CMD [\u0026#34;bundle\u0026#34;,\u0026#34;exec\u0026#34;,\u0026#34;rails\u0026#34;,\u0026#34;server\u0026#34;,\u0026#34;-b\u0026#34;,\u0026#34;0.0.0.0\u0026#34;]Adding docker-compose We needed to use some external resources (i.e., DB), to that, Docker-compose was used on this setup.\nversion: \u0026#39;3.4\u0026#39; services: onboarding_app: working_dir: /app build: context: . args: RAILS_ENV: development env_file: - ./.env volumes: - .:/app - ./tmp/gems:/gems - onboarding_app_home:/home/app/ - .irbrc:/home/app/.irbrc ports: - 3010:3000 depends_on: - mailcatcher - postgres - redis mailcatcher: image: schickling/mailcatcher ports: - 1080:1080 postgres: image: postgres:9.6-alpine ports: - 5432:5432 volumes: - postgres:/var/lib/postgresql/data redis: image: redis:4.0.6-alpine volumes: - redis:/data volumes: gems_2_5_1: postgres: redis: onboarding_app_home: Run this command to bring up this environment:\ndocker-compose up --build Docker-compose file, explained We needed to send the RAILS_ENV argument to build process of docker image. This RAILS_ENV variable was used to install all the gems related to development environment:\nbuild: context: . args: RAILS_ENV: development We needed to inform the environment variable to be used on docker container execution:\nenv_file: - ./.env We mounted some folders to help coding and troubleshooting. I will explain one by one.\nFirst, we needed to mount the source code:\nvolumes: - .:/app We needed to mount the folder used inside the docker image to add gems. This mounted folder is necessary to troubleshoot gem usage:\n- ./tmp/gems:/gems We needed to persist some config of app user so that we mounted the home folder of app user and .irbrc file too:\n- onboarding_app_home:/home/app/ - .irbrc:/home/app/.irbrc To finish this service, we needed to publish the port used to connect on the application and inform which services it depends on:\nports: - 3010:3000 depends_on: - mailcatcher - postgres - redis These services are being used to provide external resources to app service:\nmailcatcher: image: schickling/mailcatcher ports: - 1080:1080 postgres: image: postgres:9.6-alpine ports: - 5432:5432 volumes: - postgres:/var/lib/postgresql/data redis: image: redis:4.0.6-alpine volumes: - redis:/data Here we added the volumes used inside this docker-compose file:\nvolumes: gems_2_5_1: postgres: redis: onboarding_app_home: To be continued\u0026hellip; Thanks My co-workers at Paycertify who helped me giving me feedbacks and options to fix the problems, I need to say a special thanks to Rafael Affonso who enabled me to correct some mistakes about my English.\n","permalink":"https://demo.gethugothemes.com/liva/examplesite/blog/deploy_ruby_heroku/","tags":["english","devops","docker","ruby","node"],"title":"How to deploy Ruby and Node app on Heroku using Docker - Part 1"},{"categories":["portugues","devops","docker","scratch"],"contents":"TL;DR Apresentarei um conceito novo de focar na aplicação e suas dependências na construção de imagens Docker, falando sobre problemas com imagens grandes, superfície de ataque e como usar Multi stage build e a imagem scratch para resolver esse problema.\nContextualização Antes da popularização dos containers, o modelo mais utilizado era baseado em máquinas virtuais. Que instâncias virtuais, criadas em um virtualizador de maquinas (hypervisor), que atuam de forma similar a máquinas físicas, com um sistema operacional instalado, bibliotecas compartilhadas, softwares de acesso remoto (Ex. SSH), agentes em geral (Monitoramento, geranciamento de log e afins) e por fim sua aplicação (A coisa mais importante desse setup inteiro).\nA equipe de desenvolvimento, normalmente composta por várias pessoas, são responsáveis pela manutenção pela aplicação instalada nessa máquina e todo restante fica a cargo do time de suporte, ou seja, aproximadamente 90% dos softwares instalados em cada maquina desse modelo estava a cargo de um time menor (suporte geralmente era menor na maioria das empresas).\nCom a chegada do Docker tivemos a grande oportunidade de minimizar a necessidade de uma infra completa para suportar nossa aplicação. Era uma oportunidade do controle da infraesturutra mais próxima da aplicação se deslocar para o time que estava lidando com o código, os desenvolvedores nesse caso, pois com o Docker você poderia apenas instalar as dependências para sua aplicação e iniciar o processo em uma arquitetura isolada a nível de sistema operacional. Infelizmente não foi isso que aconteceu, ao menos não na maioria dos lugares.\nProblema Em muitas situações o docker ainda é usado da mesma forma que se fazia com máquinas virtuais. Qual a consequência disso? Imagens enormes! E veja que o maior impacto não é apenas no custo com armazenamento de dados e sim em dois outros importantes pontos:\n Gerência: Imagens grandes normalmente tem diversos pacotes instalados, arquivos de configuração que precisam ser modificados para mudar comportamento da imagem e afins. Dessa formas as pessoas responsáveis por essas imagens tem mais trabalho toda vez que precisam alterar alguma coisa na imagem. Segurança: Normalmente quanto maior a sua imagem, maior o número de pacotes instalados, dessa forma mais softwares para atualizar, maior superfície de ataque, que no resumo, aumenta a possibilidade da sua aplicação ser comprometida.  Proposta A google apresentou um conceito interessante chamado Distroless, e isso me motivou a escrever sobre imagens menores, mas eu precisava acrescentar mais exemplos e usar recursos mais novos do Docker.\nO conceito distroless reside no fato de você pensar menos na distribuição (GNU/Linux) e focar na sua aplicação. É lembrar que o container Docker não é uma máquina mais leve e sim um processo isolado em execução. É reduzir ao máximo o que está sendo adicionado em sua imagem.\nExemplo Usaremos esse código Go como exemplo de uma aplicação:\npackage main import ( \u0026quot;fmt\u0026quot; \u0026quot;io/ioutil\u0026quot; \u0026quot;net/http\u0026quot; \u0026quot;os\u0026quot; ) func main() { resp, err := http.Get(\u0026quot;https://google.com\u0026quot;) check(err) body, err := ioutil.ReadAll(resp.Body) check(err) fmt.Println(len(body)) } func check(err error) { if err != nil { fmt.Println(err) os.Exit(1) } } Esse código basicamente baixa a página inicial do google e retorna a quantidade de linhas.\nComo é que normalmente as pessoas constroem uma imagem Docker pra esse código? Utiliza a imagem base golang certo?\nFROM golang:1.10.3 RUN mkdir /app ADD . /app/ WORKDIR /app RUN CGO_ENABLED=0 GOOS=linux go build -o main . CMD [\u0026quot;/app/main\u0026quot;] Imagem golang:1.10.3 em detalhes Vamos começar pelo seu consumo de espaço em disco:\ngolang 1.10.3 d0e7a411e3da 2 weeks ago 794MB A imagem com seu código terá 800MB, ou seja, seu código ocupa apenas 6MB e você carregará todo restante do peso contigo ao utilizar essa imagem.\nOutro detalhe importante é a quantidade de pacotes instalados, que é 189 softwares, ou seja, são 189 versões pra se preocupar e atualizar quando sair um pacote de atualização ou nova medida de segurança.\nPra finalizar temos 27098 arquivos nessa imagem.\nObs: Vale lembrar que essa imagem contém tudo que você precisa pra buildar seu código go, e normalmente imagens de build são relativamente grandes mesmo.\nUsando Multi Stage Build Essa feature foi adicionada no Docker na versão 17.05. Ela tem como objetivo possibilitar que você use multíplas imagens base, cada uma para seu propósito e no final possa usar uma imagem mais enxuta para executar seu serviço após as etapas de build.\nSeguindo exemplo do código go, o nosso Dockerfile seria escrito desse jeito:\nFROM golang:1.10.3 as builder RUN mkdir /app ADD . /app/ WORKDIR /app RUN CGO_ENABLED=0 GOOS=linux go build -o main . FROM alpine:3.8 COPY --from=builder /app/main /app/main RUN apk --no-cache add ca-certificates CMD [\u0026quot;/app/main\u0026quot;] Muita atenção para a instrução COPY \u0026ndash;from=builder /app/main /app/main esse parâmetro \u0026ndash;from informa de onde será obtido o arquivo, perceba que na primeira instrução FROM desse Dockerfile, temos um adento as builder que é responsável por fornecer um \u0026ldquo;apelido\u0026rdquo; para essa etapa do build. Quando você informa COPY \u0026ndash;from=builder /app/main /app/main você quer dizer que pegue o arquivo /app/main da etapa apelidada como builder e coloque em /app/main na atual.\nImagem alpine:3.8 em detalhes Vamos começar pelo seu consumo de espaço em disco:\nalpine 3.8 11cd0b38bc3c 4 weeks ago 4.41MB A imagem com seu código terá 10.5MB. É bem melhor do que os 800MB da imagem golang.\nCom relação quantidade de pacotes instalados, temos 14 softwares, ou seja, muito melhor que os 189 do golang, mas ainda temos 14 pacotes pra se preocupar e atualizar quando sair um pacote de atualização ou nova medida de segurança.\nE pra finalizar temos 478 arquivos nessa imagem.\nUsando a imagem scratch Essa imagem é basicamente vazia, isso mesmo, sem nenhuma camada de dados extra. Você deve estar se perguntando como uma imagem dessa funcionaria sem resolução de nomes (/etc/hosts e afins), dev, proc e sys? De acordo com a especificação usada pela Docker atualmente, existem algumas parte do seu sistema de arquivo que são montada automaticamente para todos os containers.\nSeguindo exemplo do código go, o nosso Dockerfile seria escrito desse jeito:\nFROM golang:1.10.3 as builder RUN mkdir /app ADD . /app/ WORKDIR /app RUN CGO_ENABLED=0 GOOS=linux go build -o main . FROM scratch COPY --from=builder /app/main . CMD [\u0026quot;/main\u0026quot;] Aqui encontraremos nosso primeiro problema ao utilizar scratch:\nGet https://google.com: x509: failed to load system roots and no roots provided Basicamente nosso container precisa de um arquivo que contém todos os certificados de autoridade certificadora da internet (CA). No exemplo usando alpine resolvemos isso com a linha abaixo:\nRUN apk --no-cache add ca-certificates No scratch não temos sistema de pacote, sendo assim a forma possível para resolver esse problema é usando Multi stage build novamente:\nFROM golang:1.10.3 as builder RUN mkdir /app ADD . /app/ WORKDIR /app RUN CGO_ENABLED=0 GOOS=linux go build -o main . FROM alpine:3.8 as certs RUN apk --update add ca-certificates FROM scratch COPY --from=builder /app/main . COPY --from=certs /etc/ssl/certs/ca-certificates.crt /etc/ssl/certs/ca-certificates.crt CMD [\u0026quot;/main\u0026quot;] O que fizemos aqui foi o uso do melhor de cada etapa do build, retirando os arquivos que interessam de cada uma delas e colocando na imagem final, que será a utilizada na inicialização do serviço em produção.\nA imagem é do tamanho da sua aplicação, nesse caso 4MB. A quantidade de arquivos mesma situação.\n0 pacotes para se preocupar com atualização e gerência de arquivos de apoio. Sua aplicação é sua única preocupação nesse caso.\nConsiderações finais É evidente que nem toda linguagem funcionará nesses moldes facilmente e talvez a quantidade de horas gastas nesse processo de desenvolimento da \u0026ldquo;melhor imagem\u0026rdquo; sejam tão alta que não justifique o processo. É muito claro que esse argumento nem sempre é uma desculpa de quem não quer seguir as melhores práticas, as vezes não vale a pena para o negócio de mudança, mas meu objetivo nesse artigo é apresentar um possível \u0026ldquo;norte\u0026rdquo; para onde sua aplicação poderia \u0026ldquo;mirar\u0026rdquo; toda vez que fosse refatorada.\nEu utilizei o archore para fazer as contagens de pacotes e arquivos nas imagems.\nAgradecimentos Pery Lemke que acordou mais cedo num dia de sábado com minha ligação pra perguntar sobre o problema dele com scratch com go.\nEscrevi esse artigo ouvindo:\n Megadeth AnaVitória Alice in Chains Tiê Dream Theather Flora Matos Sabotagem Outros artistas do meu Daily Mix do Spotify  Fontes  Distroless Building Minimal Docker Containers for Go Applications Inside Docker\u0026rsquo;s \u0026ldquo;FROM scratch\u0026rdquo;  ","permalink":"https://demo.gethugothemes.com/liva/examplesite/blog/distroless/","tags":["portugues","devops","docker","scratch","distroless"],"title":"Distroless! Pense mais em sua aplicação e menos na distribuição"},{"categories":["english","devops","docker","supervisord"],"contents":"TL;DR You need more than one process in a container, so you need to follow some best practices to your container does not become just a \u0026ldquo;lightweight machine\u0026rdquo;.\nProblem In my current job, I need to run many processes in the same container. This is a warning sign of problem in your service architecture and this will be a problem for a better usage of Cloud features, but if you can\u0026rsquo;t modify this structure, you shouldn\u0026rsquo;t introduce new problems on your project.\nPremise We need to point out that a service running in container model isn\u0026rsquo;t a machine. Container Docker is virtualization at the operating system level that is another level of abstraction. It is tempting to use the Docker container as a machine, but avoid bravely because this will cause indirect damage to resource management resource in the future.\nSupervisor If you will use more than one process per container, you need a process manager, since your container does not have this by default because it wasn\u0026rsquo;t made for this usage. This is usually done by \u0026ldquo;init\u0026rdquo; in a standard \u0026ldquo;Unix-like\u0026rdquo; operation system. Remember, we are already beyond the limits of best practices.\nMy advice is: (Supervisord)[http://supervisord.org/index.html]\nSupervisor is a software client/server type that allows you control multiple processes in the Unix-like family operating system. Each process managed by supervisor is started as a subprocess.\nHere are the advantages of using Supervisor:\n Convenience: It is responsible for starting subprocesses. You can configure for all multiple processes to start when you start the supervisor, for example. Management: When you have multiple distinct processes, different ways to ensuring that it is working or not, with distinct commands, shapes, and files. Supervisor is responsible for enforcing standard by ensuring a layer of process management abstraction. Grouping: You can group processes and execute commands to the group rather than executing manually one by one.  Most relevant components  supervisord - This is the process responsible for all subprocess management. It should be pid 1 in your container. supervisorctl - This is the client responsible for supervisord management through simple commands in your terminal. Communication between the supervisorctl and supervisord can be done by UNIX domain socket (local file) or TCP socket (network communication).  How to use We will use \u0026ldquo;Debian-like\u0026rdquo; base image.\nAdding in your Dockerfile Add the following instructions:\nRUN apt-get -qqy update \u0026amp;\u0026amp; \\ apt-get -qqy install python-pip supervisor \u0026amp;\u0026amp; \\ pip install supervisor-stdout \u0026amp;\u0026amp; \\ apt-get clean Add in the bottom:\nCMD [\u0026quot;/usr/bin/supervisord\u0026quot;,\u0026quot;-c\u0026quot;,\u0026quot;/etc/supervisor/supervisord.conf\u0026quot;] Setting up the supervisor Open the /etc/supervisor/supervisord.conf file and in the [supervisorord] session you must add the following line:\nnodaemon=true Now create a new session on /etc/supervisor/supervisord.conf file:\n[eventlistener:stdout] priority = 1 command = supervisor_stdout buffer_size = 100 events = PROCESS_LOG result_handler = supervisor_stdout:event_handler Eventlistener is a supervisor feature that aims to enable a process to listen to subprocess events and handle it properly.\nsupervisor_stdout is a script created for redirecting sub process logs to the standard and error output of the supervisor.\nThe advantage of eventlistener usage lies in the fact that logs treated in this way are displayed in the supervisor log with a prefix, making it easier to figure out which subprocess is that log line.\nSetting up the process We will use nginx as an example. It is important to note that this service can not be run as a daemon, it must be run in foreground mode. That means when you execute a process and it \u0026ldquo;lock\u0026rdquo; your terminal and you can\u0026rsquo;t run new commands until it is finished.\nHere is the example of nginx running in foreground:\n/usr/sbin/nginx -g \u0026quot;daemon off;\u0026quot; To set up a process in the supervisor you need to create a file inside the /etc/supervisor.d folder:\n/etc/supervisor.d/nginx.conf\n[program:nginx] autostart=true autorestart=false priority=100 command=/usr/sbin/nginx -g \u0026quot;daemon off;\u0026quot; stdout_events_enabled = true stderr_events_enabled = true Explaining line by line\n[program:nginx] Informs the name of subprocess.\nautostart=true Informs that once the supervisor starts, this subprocess will start automatically. This option is required for containers model, since you shouldn\u0026rsquo;t have to access the container to execute any commands.\nautorestart=false Informs that your subprocess won\u0026rsquo;t restart in case of failure. Remember that we aren\u0026rsquo;t using a machine, it means that if the process failed, it needs to be turned off and another resource should be responsible for. If you use Docker with Swarm HEALTHCHECK instruction can handle this, but if your Docker cluster is Kubernetes you should use liveness/readiness probes.\npriority=100 Informs the order that the process will start and shut down. A smaller number indicates that the process will start first and will be turned off last.\ncommand=/usr/sbin/nginx -g \u0026quot;daemon off;\u0026quot; Informs which command to use to run the process. Enter the absolute path of the command. You can use the which (# which nginx ) command to find it.\nstdout_events_enabled = true stderr_events_enabled = true Informs that the standard output or error will issue an event, which will be captured by the eventlisterner previously configured in that article. In this case the eventlisterner will sent to the stdout and stderr of the supervisor, with due prefix, based on the best practices of log in container. Remember that logs sent to these outputs are automatically managed by the container tool.\nLog management Usually processes in foreground send their logs to stdout and stderr by default, but that is not the case for all logs from nginx, for example, it needs to receive specific settings for it.\nIn the file /etc/nginx/nginx.conf you need to modify the lines below:\naccess_log /var/log/nginx/access.log; error_log /var/log/nginx/error.log; For the following lines:\naccess_log /dev/stdout; error_log /dev/stdout; By default, nginx will send its log to stdout, but that still doesn\u0026rsquo;t indicate that the logs for each site will also follow this pattern. You need to access the /etc/nginx/sites-available/site_name file and modify the same lines as previously reported:\naccess_log ...; error_log ...; For the following lines:\naccess_log /dev/stdout; error_log /dev/stdout; That is, you need to understand what process is working and verify how the log is managed and finally set it to be sent to stdout and stderr.\nManaging subprocesses Now that you have everything configured, you can access your container and execute the following command:\nsupervisorctl status This command will show all managed processes and their status, which can be the following:\n STOPPED: Subprocess terminated or never started STARTING: Subprocess has been requested to start and it is initializing RUNNING: Subprocess is running BACKOFF: Subprocess entered the STARTING state, but was terminated before entering the RUNNING state STOPPING: Subprocess has received a request to terminate and is terminating EXITED: Subprocess has exited the RUNNING state (Keep an eye on the exit code ) FATAL: Subprocess can\u0026rsquo;t be successfully initialized UNKNOWN: Subprocess is in an unknown state (Normally a supervisor error)  The most commonly used commands are:\nsupervisorctl start nginx supervisorctl stop nginx supervisorctl restart nginx If you modify any configuration within /etc/supervisor.d/nginx.conf the restart command will not be enough to apply your changes. You need to execute the command below:\nsupervisorctl update This command will apply all the changes and will perform the restart on processes that have changed.\nThanks My co-workers from Crossover](http://crossover.com) who collaborate in presenting options and especially to Evgeny Udalov who played a lot in this debate.\nSources  Supervisor Redirecting to stdout Multi-process Docker Service  ","permalink":"https://demo.gethugothemes.com/liva/examplesite/blog/supervisord-en/","tags":["english","devops","docker","supervisord"],"title":"Do you need to execute more than one process per container?"},{"categories":["portugues","devops","docker","supervisord"],"contents":"TL;DR Você precisa de mais do que um processo em um container, para tal você precisa seguir algumas melhores práticas para que seu container não se torne uma \u0026ldquo;máquina leve\u0026rdquo;.\nProblema No meu atual trabalho, preciso executar muitos processos em um container. Isso por sí já um sinal de problema na arquitetura do serviço em questão para um modelo de microserviço e utilizar melhores recursos de Cloud, mas caso você não possa modificar essa estrutura, você precisará ter alguns cuidados para não gerar mais problemas.\nPremissa Precisamos ressaltar que um serviço executando em modelo de container não é uma máquina. Container Docker é virtualização a nível de sistema operacional, ou seja, um outro nível de abstração. É tentador utilizar o container Docker como uma máquina, mas evite bravamente, pois isso causará danos indiretos na gerência desse recurso no futuro.\nSupervisor Se você usará mais do que um processo por container, você precisa de um gerenciador para tal, uma vez que seu container não tem isso por padrão, pois não foi feito pra esse função. Isso normalmente é feito pelo \u0026ldquo;init\u0026rdquo; em um sistema \u0026ldquo;Unix-like\u0026rdquo; padrão. Lembre-se, já estamos além dos limites das melhores práticas.\nMeu conselho é: Utilize (Supervisord)[http://supervisord.org/index.html]\nSupervisor é um software, do tipo cliente/servidor, que permite usuários controlarem múltiplos processos em sistema operacional da família \u0026ldquo;Unix-like\u0026rdquo;. Os processos controlados por ele são iniciados como sub-processo de sí.\nSegue abaixo as vantagens de se utilizar Supervisor:\n Conveniência: Ele é responsável por iniciar os sub-processos, ou seja, você pode configurar para todos os múltiplos processos iniciarem quando se iniciar o supervisor, por exemplo. Gerência: Imagine múltiplos processos distintos, cada um com sua forma de garantir que está funcionando ou não, com diferentes comandos, formas e arquivos. O que o supervisor faz é garantir um determinado padrão e oferecer uma camada extra de abstração no que tange a gerência dos sub-processos. Agrupamento: Você pode agrupar processos e executar comandos em vários ao mesmo tempo, ao invés de executar diversos comandos manualmente.  Componentes mais relevantes  supervisord - Esse é o processo responsável por toda gerência dos sub-processos. Ele deverá ser o pid 1 em seu container. supervisorctl - Esse é o client que permite gerência do supervisord atráves de comandos simples em seu terminal. A comunicação entre o supervisorctl e supervisord pode ser feita via UNIX domain socket (Arquivo local) ou TCP socket (comunicação de rede via endereço ip na rede).  Como usar Imaginando que estamos falando de um base image derivada de um \u0026ldquo;Debian-like\u0026rdquo;\nAdicionando em seu Dockerfile Adicione a seguinte instrução:\nRUN apt-get -qqy update \u0026amp;\u0026amp; \\ apt-get -qqy install python-pip supervisor \u0026amp;\u0026amp; \\ pip install supervisor-stdout \u0026amp;\u0026amp; \\ apt-get clean E no final adicione:\nCMD [\u0026quot;/usr/bin/supervisord\u0026quot;,\u0026quot;-c\u0026quot;,\u0026quot;/etc/supervisor/supervisord.conf\u0026quot;] Configurando o supervisor Abra o arquivo /etc/supervisor/supervisord.conf e na sessão [supervisord] você deve adicionar a seguinte linha:\nnodaemon=true Ainda no arquivo supervisord, agora deve criar uma sessão nova sessão:\n[eventlistener:stdout] priority = 1 command = supervisor_stdout buffer_size = 100 events = PROCESS_LOG result_handler = supervisor_stdout:event_handler Eventlistener é uma funcionalidade do supervisor que tem como objetivo possibilitar que um processo escute eventos de sub-processos e possa tratá-lo de forma devida.\nEm nosso uso o supervisor_stdout é um processo criado para redirecionamento de logs dos sub-processos para as saída padrão e de erro do supervisor.\nA vantagem do uso desse eventlistener reside no fato que os logs tratados dessa maneira são exibidos no log do supervisor com um prefixo, sendo mais fácil descobrir de qual subprocesso é aquela linha de log.\nConfigurando o serviço Imagine que você deseja iniciar um nginx. É importante salientar que esse serviço não pode ser executado como daemon, ou seja, ele precisa ser executado em modo foregroud. Em termos visuais é aquele processo que quando executado ele não libera o terminal, ou seja, após executado você não consegue dar novos comandos até que ele seja terminado.\nSegue abaixo o exemplo do nginx executado em foreground:\n/usr/sbin/nginx -g \u0026quot;daemon off;\u0026quot; Para configurar um processo no supervisord você precisa criar um arquivo dentro da pasta /etc/supervisor.d, que nesse caso vamos usar o exemplo do nginx:\n/etc/supervisor.d/nginx.conf\n[program:nginx] autostart=true autorestart=false priority=100 command=/usr/sbin/nginx -g \u0026quot;daemon off;\u0026quot; stdout_events_enabled = true stderr_events_enabled = true Abaixo vou explicar linha a linha\n[program:nginx] Define o nome do sub-processo, que nesse caso será \u0026ldquo;nginx\u0026rdquo;.\nautostart=true Informa que uma vez que o supervisor inicie, esse sub-processo será iniciado automaticamente. Essa opção é necessária para modelo de containers, uma vez que nessa abordagem usando docker você não deveria precisar acessar o container pra executar nenhum comando.\nautorestart=false Informa que o processo não será reiniciado em caso de falha, ou seja, caso ele falhe o supervisor não iniciará novamente. Lembre-se que não estamos atuando em uma máquina, isso quer dizer que se o processo falhou, ele precisa ficar desligado e outro recurso deverá ser responsável por verificar que não estar rodando e então desligar ou reiniciar o container inteiro. No caso do Docker com SWARM, isso é feito com a instrução HEALTHCHECK, com Kubernetes, isso pode ser feito com liveness/readiness probes.\npriority=100 Informa a ordem que o processo será iniciado e desligado. Um número menor indica que o processo começará primeiro e será desligado por último.\ncommand=/usr/sbin/nginx -g \u0026quot;daemon off;\u0026quot; Informa qual comando utilizando para executar o processo. Informe o caminho absoluto do comando. Você pode usar o comando which (# which nginx) pra achá-lo.\nstdout_events_enabled = true stderr_events_enabled = true Informa que a saída de padrão e de erro emitirá um evento, que será capturado pelo eventlistener configurado anteriormente nesse artigo. Nesse caso estamos o eventlisterner tratará enviado para o stdout e stderr do supervisor, com devido prefixo, baseado nas melhores práticas de log em container. Uma vez que logs que são enviado para essas saídas são automaticamente gerenciado pela ferramenta de container em questão.\nFalando em gerência de logs\u0026hellip; Normalmente processos em modo foreground enviam seus logs para stdout e stderr por padrão, mas não é o caso do nginx, por exemplo, ele precisa receber configurações específicas para tal.\nNo arquivo /etc/nginx/nginx.conf Modifique as linhas abaixo:\naccess_log /var/log/nginx/access.log; error_log /var/log/nginx/error.log; Para as seguintes linhas:\naccess_log /dev/stdout; error_log /dev/stdout; Com isso por padrão o nginx enviará seu log para stdout, mas isso ainda não indica que os logs de cada site também seguirão esse padrão. Você precisa accessar o arquivo /etc/nginx/sites-available/nome_do_site e modificar as mesmas linhas informadas anteriormente:\naccess_log ...; error_log ...; Para as seguintes linhas:\naccess_log /dev/stdout; error_log /dev/stdout; Ou seja, você precisa entender qual processo está trabalhando e verificar como o log é gerenciado e por fim configurar para que seja enviado para stdout e stderr.\nGerenciado processos Agora que você já tem tudo configurado, você pode acessar seu container e executar o seguinte comando:\nsupervisorctl status Esse comando mostrará todos os processo gerenciados e seu estado, que podem ser as opções abaixo:\n STOPPED: O processo foi finalizado ou nunca iniciado STARTING: O processo recebeu requisição para inicio e está inicializando RUNNING: O processo está em execução BACKOFF: O processo entrou no estado STARTING, mas foi terminou antes de entrar no estado RUNNING STOPPING: O processo recebeu requisição para término e está terminando EXITED: O processo saiu do estado RUNNING (Fique atento ao exit code) FATAL: O processo não pode ser inicializado com sucesso UNKNOWN: O processo está em estado desconhecido (Normalmente um erro do supervisor)  Os comandos mais usados são\nsupervisorctl start nginx supervisorctl stop nginx supervisorctl restart nginx Caso você modifique alguma configuração dentro de /etc/supervisor.d/nginx.conf o comando restart não será o suficiente para aplicar suas alterações. Você precisa executar o comando abaixo:\nsupervisorctl update Esse comando aplicará todas as mudanças e executará o restart nos processos que sofreram alteração.\nAgradecimentos Meus colegas de trabalho da Crossover que colaboram em apresentar opções e especialmente para Evgeny Udalov que atuou bastante nesse debate.\nFontes  Supervisor Redirecting to stdout Multi-process Docker Service  ","permalink":"https://demo.gethugothemes.com/liva/examplesite/blog/supervisord/","tags":["portugues","devops","docker","supervisord"],"title":"Precisa executar mais do que um processo por container?"},{"categories":["portugues","devops","carreira"],"contents":"TL;DR Um dia precisei fazer merge entre duas branchs, mas apenas dos dados de uma pasta específica. Nesse artigo irei demonstrar como usar o \u0026ldquo;git cherry-pick\u0026rdquo; para resolver esse problema.\nProblema Estava fazendo um trabalho extra e me deparei com um projeto usando incorretamente as branchs do repositório.\nNa verdade, eu estava usando uma branch, mas o pipeline foi modificado no meio do projeto, sem me informar, para usar uma outra branch, ou seja, eu tinha bastante trabalho não sendo aproveitado pelo pipeline e entregando a imagem errada para deploy.\nSolução rápida Entre as opções para solução eu tinha a possibilidade de copiar a única pasta que eu trabalhei nesse repositório e então enviar um grande commit na branch correta, mas com isso eu perderia todo log do git com relação as modificações, certo?\nSolução ideal A melhor opção era mover apenas os commits que eu enviei, certo? Como eu trabalhei em apenas uma pasta, isso ficou mais fácil.\nNa branch de origem (onde meu trabalho está hoje) executei o comando abaixo e assim foi possível obter a lista de commits da pasta em questão:\ngit log --oneline --decorate --color --graph pasta_escolhida/ Com posse dos identificadores de cada commit (Ex. f7d62e1) eu fiz uma lista e passei para a branch de destino (para onde meus commits devem ser migrados):\ngit checkout branch_destino Agora vamos a cereja do bolo :P\nUsando o comando abaixo é possível importar para a branch que você está no momento o commit com o identificador informado. Ele criará um outro commit, com outro identificador na sua branch de destino, mas com o mesmo conteúdo:\ngit cherry-pick f7d62e1 Execute o comando abaixo para validar se seu commit foi importado:\ngit log --oneline --decorate --color --graph pasta_escolhida/ Existe a possibilidade de algum conflito nessa importação, mas após resolvido basta apenas você dar o comando abaixo para continuar:\ngit cherry-pick --continue Lembre-se sempre de validar se o commit foi importado:\ngit log --oneline --decorate --color --graph pasta_escolhida/ Depois de migrar todos os commits, basta apenas fazer o push normalmente.\ngit push Pronto!\nAgradecimentos A todos do canal Hora Extra Salvador que colaboram em apresentar opções e especialmente para Massimo Rangoni que teve total paciência pra me ajudar nessa.\nFontes  Git cherry-pick  ","permalink":"https://demo.gethugothemes.com/liva/examplesite/blog/cherry-pick/","tags":["portugues","devops","carreira"],"title":"Fazendo merge de apenas uma pasta entre duas branches no git"},{"categories":["portugues","devops","carreira"],"contents":"TL;DR Quer entrar na carreira \u0026ldquo;DevOps\u0026rdquo; e não sabe por onde começar? Nesse texto falarei sobre possíveis caminhos para sua carreita técnica, com materiais para auxiliar no processo de aprendizado e dicas para acelerar seu processo.\nMotivação Muitas pessoas me perguntam por onde começar a trabalhar com \u0026ldquo;DevOps\u0026rdquo; e sempre prometo escrever um artigo com alguma síntese das minhas opiniões sobre como seria um início \u0026ldquo;ideal\u0026rdquo; de carreira. Aqui estou pagando uma promessa do ano passado.\nColaboração O texto não é 100% autoral, ou seja, imagine esse texto como uma colagem de muitas ideias e materiais produzidos por outras pessoas. Ele provavelmente sofrerá alteração ao longo do tempo. Isso que dizer que se você acha que algo deve ser colocado aqui, modifique esse arquivo e submeta um PR no github, vamos discutir nos comentários e aceitar a colaboração :)\nDevOps é carreira? Então, a resposta direta e resumida é: NÃO, mas por outro lado o mercado já usa esse termo para denominar pessoas e vagas de trabalho relacionadas a entrega contínua, infraestrutura ágil e afins, não vou entrar nesse embate no texto. Se quiser discutir isso, entre nesse canal do telegram que tem pessoas dispostas a essa conversa.\nO que é DevOps de verdade? É um mudança cultural, acredito que esse vídeo e tenha uma ideia básica do que se trata. Esse vídeo tem legenda em português, caso precise.\nRequisitos Vou presumir que você é uma pessoa que possui conhecimentos que permitam boa leitura e interpretação de textos em inglês e já está inserida no básico de tecnologia da informação, ou seja, que já é ao menos uma pessoa de nível técnico de TI.\nMetodologia Eu vou tentar organizar a ordem do conhecimento com base nos elementos mais básicos que você precisará para se qualificar para o mercado de trabalho e assunto teóricos mais básicos. Isso quer dizer que não precisa estudar tudo que eu falar para poder submeter para aquela vaga \u0026ldquo;DevOps\u0026rdquo;, lembre-se: \u0026ldquo;O não você já tem sem tentar\u0026rdquo; (Autor desconhecido).\nPor onde começar? Eu aconselharia os assuntos com maior demanda e mais próximo do que a maioria dos técnicos de TI já tem alguma familiaridade, sendo assim vou separar em dois sub-grupos de pessoas, que posteriormente se encontram em passos mais relacionados a esse novo paradigma.\nAlgumas pessoas gostam e tem mais experiência em escrever software e outras em manter a infraestrutura onde estarão hospedadas esses softwares, sendo assim acredito que os elementos básicos necessários para fazer a mudança na carreira são diferentes a depender da sua história.\nAssuntos básicos para pessoas inclinadas a desenvolvimento de software Estude sobre TCP/IP! Domine isso! Entenda como funciona endereço IP, mascara de rede, rotas de rede, portas TCP/UDP. O que é um socket e afins.\nMaterial sugerido:\n Livro Redes de Computadores - Andrew S Tanenbaum Redes de computadores e a internet uma nova abordagem - James Kurose e Keith Ross  Estude também sobre funcionamento do sistema operacional, o que são processos, como funciona o escolanamento de processamento e afins.\nMaterial sugerido:\n Livro Sistemas Operacionais Modernos - Andrew S Tanenbaum  Obs: Esses dois livros são enormes, então não precisa necessariamente ler todos. Apenas imagine que quanto mais você ler e absorver esses conhecimentos, melhor profissional você será.\nAssuntos básicos para pessoas inclinadas a infraestrutura de TI Estude sobre desenvolvimento de software! Não apenas sobre fazer scripts para tarefas simples. Você precisa entender como funciona o processo inteiro de desenvolvimento de software. Isso não quer dizer necessariamente que você se tornará uma pessoa desenvolvedora de software.\nA proposta aqui é ter o mesmo conhecimento que uma pessoa em início de carreira deveria ter, isso quer dizer que você precisa aprender a coisa mais básica nesse assunto: ALGORITMO!\nMaterial sugerido:\n Livro Algoritmos Teoria e Prática - Thomas H Cormen  Obs: A ideia aqui é ter uma base sólida sobre o que é Algoritmo, não precisa ler o livro inteiro\nUm outro conselho básico é a necessidade de dominar ao menos uma linguagem de programação a nível iniciante. Que você consiga usar os elementos mais básicos da linguagem para desenvolvimento de recursos simples, como um job para execução pontual, um serviço web e afins.\nMaterial sugerido:\n Curso online gratuito - Python Para Zumbis - Fernando Masanori Curso online pago - POO com Ruby - Jackson Pires Curso online pago - Curso de Go - Jeff Prestes  Se quiser validar seus conhecimentos, tem alguns sites que podem lhe ajudar nisso:\n Hacker Rank Project Euler  Conhecimentos comuns A partir daqui os conhecimentos são para qualquer tipo de pessoa, sendo assim aconselho a todos fazerem a medida que vejam necessidade dentro da sua carreira.\nPara as pessoas que não dominam desenvolvimento de software, perceberão que muitos dos assuntos são completamente novos e o caminho parece um pouco mais \u0026ldquo;tortuoso\u0026rdquo;, o que em parte é verdade, mas veja pelo lado bom, uma vez concluído esse caminho você dominará a área profissional mais requisitada no mercado e será capaz de resolver a maioria dos desafios, mesmo com pouca ajuda.\nConhecimento de controle de versão de código Esse é o \u0026ldquo;pontapé inicial\u0026rdquo; para o assunto \u0026ldquo;DevOps\u0026rdquo;, pois tudo que será feito daqui pra frente será baseado em código, e manter esse conjuntos de fontes em um repositório de controle de versão é requisito mínimo até mesmo para níveis mais iniciantes nessa carreira.\nNa minha opinião, o Git reina em absoluto nesse assunto. Há quem ainda use SVN, Mercurial e afins, mas a maioria dos lugares usam Git. Pode estudar sem culpa, pois aprender os outros será fácil depois do Git.\nMaterial sugerido para Git:\n Vídeo - Descomplicando o GIT - Parte 1 - LinuxTips Material oficial Git Material oficial Git (Português)  Conhecimento sobre virtualização Você precisa saber o mínimo de virtualização, pois a partir desse momento em diante você necessariamente estará trabalhando com algum nível de virtualização na maioria dos serviços que for interagir/manter.\nMaterial sugerido:\n Virtualization Getting Started - Oficial Red Hat  OBS: Não precisa ser especialista no assunto, mas antes de ver todo resto eu dedicaria um tempo entendendo esse assunto, pois os assuntos Cloud e containers serão mais fáceis pra você depois desse estudo.\nConhecimento de Cloud Aprenda sobre como utilizar algum fornecedor de Cloud, pois boa parte dos serviços são parecidos em seu funcionamento, ou seja, se você aprender um deles não encontrará muitos problemas em utilizar seus concorrentes. Na perspectiva de possibilidade de trabalho, eu aconselharia você começar pela AWS, que tem o maior números de clientes hoje.\nMaterial sugerido para AWS:\n Curso online pago - Cloud Guru - AWS Certified Solutions Architect Associate Canal Youtube Cloud Guru  Material sugerido para Google Cloud:\n Lab: Build a Continuous Deployment Pipeline with Jenkins and Kubernetes   Material sugerido para Azure:\n Portal de treinamentos gratuitos de Azure criado pela Microsoft Microsoft Virtual Academy - Academia virtual da Microsoft com treinamentos gratuitos de Azure Lista curada de aprendizado de Azure - Ricardo Martins  Conhecimento de gerência de configuração Você precisará automatizar a configuração dos seus ativos (servidores, switchs e afins), com isso algumas ferramentas precisam de alguma atenção no seu estudo. Você precisa de um nível acima do básico em ao menos uma delas. Eu aconselho experimentar todas descritas abaixo, mas escolha uma para especialização:\n Puppet Chef Ansible  Material sugerido para Puppet:\n Curso grátis - Oficial Puppet  Material sugerido para Chef:\n Curso grátis - Oficial Chef  Material sugerido para Ansible:\n Ansible - Getting started Webinars oficiais do Ansible  Conhecimento de containeres Você precisará saber sobre containers e duas ferramentas dominam esse assunto, Docker e Kubernetes. Você precisa saber além do básico em ambos produtos.\nMaterial sugerido para Docker:\n Livro Containers com Docker - Daniel Romero  Livro Aprendendo Docker - Wellington da Silva  Livro Descomplicando Docker - Jeferson Fernando Noronha Livro Docker para desenvolvedores - Livro Código Aberto - Rafael Gomes  Material sugerido para Kubernetes:\n Kubernetes Basics Treinamentos online grátis oficiais Kubernetes The Hard Way Tradução do Kubernetes The Hard Way  Conhecimento de CI/CD É importante conhecer sobre Continuous Integration e Continuous Delivery. Ambos são conceitos centrais dessa mudança de paradigma de desenvolvimento de software e fornecimento de infraestrutura automatizada.\nComo material sugerido, eu aconselho o que pra mim seria a bíblia do \u0026ldquo;DevOps\u0026rdquo;:\n Continuous Delivery - Jez Humble e David Farley   Esse livro é grande, mas uma leitura simples! Tem a versão traduzida se preferir:\n Entrega Contínua - Jez Humble e David Farley  O Jenkins \u0026ldquo;reina\u0026rdquo; com uma vantagem considerável entre as ferramentas mais comum sobre esses assuntos.\nMaterial sugerido para Jenkins:\n Curso online pago - Master Jenkins CI For DevOps and Developer Livro Jenkins - Automatize tudo sem complicações - Fernando Boaglio  Como ferramentas alternativas temos o GoCD, TeamCity e outras também relevantes, mas estudando o Jenkins você terá a base sólida sobre uso de ferramentas de CI/CD. Fique atento aos conceitos informados no livro \u0026ldquo;Entrega Contínua\u0026rdquo;.\nConclusão Importante salientar que os conhecimentos necessários para uma carreira \u0026ldquo;DevOps\u0026rdquo; não acabam aqui, mas acredito que esses sejam os mais básicos/intermediários para quem tinha interesse em ter um \u0026ldquo;norte\u0026rdquo; mais curto/médio prazo.\nLembre-se, esse é um texto em construção constante e feito através de colaboração de muitas pessoas. Se você tem alguma sugestão, mande um PR a partir desse arquivo.\nAgradecimentos A todos do canal DevOpsBR no Telegram que colaboram em enriquecer o texto.\nFontes  Guto Carvalho Awesome DevOps-BR  ","permalink":"https://demo.gethugothemes.com/liva/examplesite/blog/carreira-devops/","tags":["portugues","devops","carreira"],"title":"Os primeiros passos para uma carreira DevOps"},{"categories":["portugues","blog"],"contents":"TL;DR O texto será sobre o porque decidir novamente trocar de blog e experimentar um novo modelo mais simples, porém ainda usando site com conteúdo estático.\nPúblico alvo Esse texto é para qualquer pessoa interessada em ler sobre experiências de mudanças de blogs e escrita de contéudo técnico.\nIntrodução Eu já tenho um blog, mas ele está em um wordpress, uma plataforma CMS, que por mais que sua proposta tenha como foco a usabilidade, manter ela a nível de infra não é uma tarefa extremamente simples. Quando falo simples, estou querendo reduzir à de fato não ter trabalho praticamente nenhum para ter uma infraestrutura adequada para publicar apenas textos com imagens.\nVenho há algum tempo \u0026ldquo;namorando\u0026rdquo; a ideia de ter um site com conteúdo estático, com esses geradores mais famosos (hugo e jekyll)\nA idéia de utilizar conteúdo estático tem como objetivo não somente ser simples de manter, mas também utilizar menos recursos da máquina, uma vez que o site é modificado, ele é construído em um único momento e distribuído estáticamente para todos usuários, com isso não será necessário uso de banco de dados.\nUm pouco da história Depois de algumas conversas com somatorio resolvi usar o hugo. Criei um site estático bem rápido, a estrutura dele era uma landing page que serviria como hub para todos meus contatos e o blog era acessível através de um link, como vocês podem ver aqui, que ficará aqui apenas para título de histórico.\nFique satisfeito na época, mas algo ainda me incomodava, porém não conseguia explicar o porque.\nDepois de ver o site do Fike entendi o motivo. O fato do meu contéudo estar atrás de um link e não na página inicial, isso me fazia sentir que algo estava errado.\nPercebi que ele hospeda seu site em um repositório público, e assim resolvi cloná-lo e a partir dele fazer o meu próprio. Afinal é assim que nossa comunidade funciona, certo?\nEstou feliz com o blog atual? Ainda é cedo pra dizer, mas pretendo escrever em português e inglês e o modelo adotado por Fike no blog dele me agrada. Vamos aos testes. Se for pra falhar, que seja o quanto antes, certo?\n","permalink":"https://demo.gethugothemes.com/liva/examplesite/blog/comecando-novamente/","tags":["portugues","blog"],"title":"Começando novamente"}]